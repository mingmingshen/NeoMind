//! Tests for settings handlers.

use neomind_api::handlers::settings::*;
use neomind_api::handlers::llm_backends::{
    list_backends_handler, create_backend_handler, list_ollama_models_handler,
    CreateBackendRequest, OllamaModelsQuery,
};
use neomind_api::handlers::ServerState;
use axum::extract::{Query, State};
use axum::Json;

async fn create_test_server_state() -> ServerState {
    crate::common::create_test_server_state().await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_list_backends_handler_no_backends() {
        // This test will pass if no backends are configured
        let state = create_test_server_state().await;
        let result = list_backends_handler(State(state), Query(HashMap::new())).await;
        assert!(result.is_ok());
        let response = result.unwrap();
        // Should return empty array when no backends configured
        if let Some(data) = response.0.data {
            let backends = data.get("backends").and_then(|v| v.as_array());
            assert!(backends.is_some());
            if let Some(arr) = backends {
                assert_eq!(arr.len(), 0);
            }
        }
    }

    #[tokio::test]
    async fn test_create_backend_handler_invalid_type() {
        let state = create_test_server_state().await;
        let req = CreateBackendRequest {
            name: "test_backend".to_string(),
            backend_type: "invalid_type".to_string(),
            endpoint: Some("http://localhost:11434".to_string()),
            model: Some("test-model".to_string()),
            api_key: None,
            temperature: None,
            top_p: None,
            top_k: None,
            thinking_enabled: None,
            capabilities: None,
        };
        let result = create_backend_handler(State(state), Json(req)).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_list_ollama_models_handler_empty_params() {
        let params = OllamaModelsQuery {
            endpoint: None,
        };
        let result = list_ollama_models_handler(Query(params)).await;
        // This may fail if Ollama is not running, but should return a result
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_list_ollama_models_handler_with_endpoint() {
        let params = OllamaModelsQuery {
            endpoint: Some("http://localhost:11434".to_string()),
        };
        let result = list_ollama_models_handler(Query(params)).await;
        // May fail if Ollama is not running, but handler should not crash
        assert!(result.is_ok() || result.is_err());
    }

    #[tokio::test]
    async fn test_llm_generate_handler_no_config() {
        let state = create_test_server_state().await;
        let req = LlmGenerateRequest {
            prompt: "Hello".to_string(),
        };
        let result = llm_generate_handler(State(state), Json(req)).await;
        // Should fail because LLM is not configured
        assert!(result.is_err());
    }
}
