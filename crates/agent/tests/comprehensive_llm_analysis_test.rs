//! NeoTalk ç»¼åˆLLMåˆ†ææµ‹è¯•
//!
//! æµ‹è¯•ç»´åº¦:
//! 1. ç©ºå“åº”é—®é¢˜æ·±åº¦åˆ†æ
//! 2. å‘½ä»¤ä¸‹å‘åŠŸèƒ½æµ‹è¯•
//! 3. è§„åˆ™å¼•æ“ç”Ÿæˆæ­£ç¡®ç‡
//! 4. å·¥ä½œæµç”Ÿæˆæ­£ç¡®ç‡å’Œå¯æ‰§è¡Œç‡
//!
//! **æµ‹è¯•æ—¥æœŸ**: 2026-01-17
//! **LLMåç«¯**: Ollama (qwen3:1.7b)

use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};
use serde_json::Value;

use edge_ai_llm::backends::create_backend;
use edge_ai_core::llm::backend::{LlmRuntime, GenerationParams, LlmInput};
use edge_ai_core::message::{Message, MessageRole, Content};
use edge_ai_rules::{RuleEngine, dsl::RuleDslParser};
use edge_ai_tools::{ToolRegistry, ToolCall, ToolRegistryBuilder};

// ============================================================================
// æµ‹è¯•é…ç½®
// ============================================================================

const TEST_MODEL: &str = "qwen3:1.7b";
const OLLAMA_ENDPOINT: &str = "http://localhost:11434";

#[derive(Debug, Clone)]
pub struct TestConfig {
    pub model: String,
    pub endpoint: String,
    pub timeout_secs: u64,
}

impl Default for TestConfig {
    fn default() -> Self {
        Self {
            model: TEST_MODEL.to_string(),
            endpoint: OLLAMA_ENDPOINT.to_string(),
            timeout_secs: 60,
        }
    }
}

// ============================================================================
// ç©ºå“åº”åˆ†æå™¨
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmptyResponseAnalysis {
    pub total_requests: usize,
    pub empty_responses: usize,
    pub empty_rate: f64,
    pub empty_by_category: HashMap<String, usize>,
    pub response_lengths: Vec<usize>,
    pub avg_response_length: f64,
    pub raw_responses: Vec<RawResponseData>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RawResponseData {
    pub user_input: String,
    pub content: String,
    pub thinking: String,
    pub content_len: usize,
    pub thinking_len: usize,
    pub is_empty: bool,
    pub reason: String,
}

pub struct EmptyResponseAnalyzer {
    llm: Arc<dyn LlmRuntime>,
    config: TestConfig,
}

impl EmptyResponseAnalyzer {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let config = TestConfig::default();
        let llm_config = serde_json::json!({
            "endpoint": config.endpoint,
            "model": config.model
        });

        let llm = create_backend("ollama", &llm_config)?;

        Ok(Self { llm, config })
    }

    /// æ·±åº¦åˆ†æç©ºå“åº”é—®é¢˜
    pub async fn analyze_empty_responses(&self, test_inputs: Vec<&str>) -> EmptyResponseAnalysis {
        let mut raw_responses = Vec::new();
        let mut empty_by_category = HashMap::new();
        let mut response_lengths = Vec::new();

        for input in test_inputs {
            let system_prompt = "ä½ æ˜¯ NeoTalk æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚";

            let messages = vec![
                Message {
                    role: MessageRole::System,
                    content: Content::Text(system_prompt.to_string()),
                    timestamp: None,
                },
                Message {
                    role: MessageRole::User,
                    content: Content::Text(input.to_string()),
                    timestamp: None,
                },
            ];

            let llm_input = LlmInput {
                messages,
                params: GenerationParams {
                    max_tokens: Some(200),
                    temperature: Some(0.7),
                    ..Default::default()
                },
                model: Some(self.config.model.clone()),
                stream: false,
                tools: None,
            };

            match tokio::time::timeout(
                Duration::from_secs(self.config.timeout_secs),
                self.llm.generate(llm_input)
            ).await {
                Ok(Ok(output)) => {
                    let response_text = output.text;
                    let is_empty = response_text.trim().is_empty();

                    // åˆ†æç©ºå“åº”åŸå› 
                    let reason = if is_empty {
                        "å“åº”ä¸ºç©º".to_string()
                    } else if response_text.len() < 5 {
                        format!("å“åº”è¿‡çŸ­({}å­—ç¬¦)", response_text.len())
                    } else {
                        "æ­£å¸¸".to_string()
                    };

                    // å°è¯•è·å–åŸå§‹Ollamaå“åº”æ•°æ®ï¼ˆé€šè¿‡æ¨¡æ‹Ÿï¼‰
                    let raw_data = RawResponseData {
                        user_input: input.to_string(),
                        content: response_text.clone(),
                        thinking: "".to_string(),  // éœ€è¦ä»Ollamaè·å–åŸå§‹æ•°æ®
                        content_len: response_text.len(),
                        thinking_len: 0,
                        is_empty,
                        reason,
                    };

                    *empty_by_category.entry(raw_data.reason.clone()).or_insert(0) += 1;
                    response_lengths.push(response_text.len());
                    raw_responses.push(raw_data);
                }
                Ok(Err(e)) => {
                    let raw_data = RawResponseData {
                        user_input: input.to_string(),
                        content: "".to_string(),
                        thinking: "".to_string(),
                        content_len: 0,
                        thinking_len: 0,
                        is_empty: true,
                        reason: format!("LLMé”™è¯¯: {:?}", e),
                    };
                    *empty_by_category.entry(raw_data.reason.clone()).or_insert(0) += 1;
                    response_lengths.push(0);
                    raw_responses.push(raw_data);
                }
                Err(_) => {
                    let raw_data = RawResponseData {
                        user_input: input.to_string(),
                        content: "".to_string(),
                        thinking: "".to_string(),
                        content_len: 0,
                        thinking_len: 0,
                        is_empty: true,
                        reason: "è¶…æ—¶".to_string(),
                    };
                    *empty_by_category.entry(raw_data.reason.clone()).or_insert(0) += 1;
                    response_lengths.push(0);
                    raw_responses.push(raw_data);
                }
            }
        }

        let total_requests = raw_responses.len();
        let empty_responses = raw_responses.iter().filter(|r| r.is_empty).count();
        let empty_rate = if total_requests > 0 {
            (empty_responses as f64 / total_requests as f64) * 100.0
        } else {
            0.0
        };

        let avg_response_length = if !response_lengths.is_empty() {
            response_lengths.iter().sum::<usize>() as f64 / response_lengths.len() as f64
        } else {
            0.0
        };

        EmptyResponseAnalysis {
            total_requests,
            empty_responses,
            empty_rate,
            empty_by_category,
            response_lengths,
            avg_response_length,
            raw_responses,
        }
    }
}

// ============================================================================
// å‘½ä»¤ä¸‹å‘æµ‹è¯•å™¨
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CommandExecutionResult {
    pub command: String,
    pub parameters: Value,
    pub llm_response: String,
    pub parsed_command: Option<ParsedCommand>,
    pub execution_success: bool,
    pub execution_time_ms: u128,
    pub error_message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ParsedCommand {
    pub action: String,
    pub device_type: Option<String>,
    pub device_id: Option<String>,
    pub parameters: HashMap<String, Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CommandExecutionTestResult {
    pub total_commands: usize,
    pub successful_parses: usize,
    pub successful_executions: usize,
    pub parse_rate: f64,
    pub execution_rate: f64,
    pub results: Vec<CommandExecutionResult>,
}

pub struct CommandExecutorTester {
    llm: Arc<dyn LlmRuntime>,
    config: TestConfig,
}

impl CommandExecutorTester {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let config = TestConfig::default();
        let llm_config = serde_json::json!({
            "endpoint": config.endpoint,
            "model": config.model
        });

        let llm = create_backend("ollama", &llm_config)?;

        Ok(Self { llm, config })
    }

    /// æµ‹è¯•å‘½ä»¤ä¸‹å‘åŠŸèƒ½
    pub async fn test_command_execution(&self, commands: Vec<(&str, Value)>) -> CommandExecutionTestResult {
        let mut results = Vec::new();

        for (command, params) in commands {
            let system_prompt = format!(r#"ä½ æ˜¯ NeoTalk æ™ºèƒ½åŠ©æ‰‹ã€‚
å½“ç”¨æˆ·è¦æ±‚æ‰§è¡Œè®¾å¤‡æ§åˆ¶æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›å¤:
{{"action": "è®¾å¤‡æ“ä½œ", "device_type": "è®¾å¤‡ç±»å‹", "device_id": "è®¾å¤‡ID", "parameters": {{...}}}}

ä¾‹å¦‚: æ‰“å¼€å®¢å…çš„ç¯
å›å¤: {{"action": "turn_on", "device_type": "light", "device_id": "living_room_light", "parameters": {{"power": "on"}}}}

ç”¨æˆ·å‘½ä»¤: {}
å‚æ•°: {:?}"#, command, params);

            let messages = vec![
                Message {
                    role: MessageRole::System,
                    content: Content::Text(system_prompt),
                    timestamp: None,
                },
                Message {
                    role: MessageRole::User,
                    content: Content::Text(command.to_string()),
                    timestamp: None,
                },
            ];

            let llm_input = LlmInput {
                messages,
                params: GenerationParams {
                    max_tokens: Some(200),
                    temperature: Some(0.3),  // é™ä½æ¸©åº¦ä»¥æé«˜ä¸€è‡´æ€§
                    ..Default::default()
                },
                model: Some(self.config.model.clone()),
                stream: false,
                tools: None,
            };

            let start = std::time::Instant::now();

            let result = match tokio::time::timeout(
                Duration::from_secs(self.config.timeout_secs),
                self.llm.generate(llm_input)
            ).await {
                Ok(Ok(output)) => {
                    let llm_response = output.text;
                    let parsed = self.parse_command_response(&llm_response);
                    let execution_success = parsed.is_some();

                    CommandExecutionResult {
                        command: command.to_string(),
                        parameters: params,
                        llm_response,
                        parsed_command: parsed,
                        execution_success,
                        execution_time_ms: start.elapsed().as_millis(),
                        error_message: if execution_success { None } else { Some("æ— æ³•è§£æå‘½ä»¤".to_string()) },
                    }
                }
                Ok(Err(e)) => {
                    CommandExecutionResult {
                        command: command.to_string(),
                        parameters: params,
                        llm_response: "".to_string(),
                        parsed_command: None,
                        execution_success: false,
                        execution_time_ms: start.elapsed().as_millis(),
                        error_message: Some(format!("LLMé”™è¯¯: {:?}", e)),
                    }
                }
                Err(_) => {
                    CommandExecutionResult {
                        command: command.to_string(),
                        parameters: params,
                        llm_response: "".to_string(),
                        parsed_command: None,
                        execution_success: false,
                        execution_time_ms: start.elapsed().as_millis(),
                        error_message: Some("è¶…æ—¶".to_string()),
                    }
                }
            };

            results.push(result);
        }

        let total_commands = results.len();
        let successful_parses = results.iter().filter(|r| r.parsed_command.is_some()).count();
        let successful_executions = results.iter().filter(|r| r.execution_success).count();

        CommandExecutionTestResult {
            total_commands,
            successful_parses,
            successful_executions,
            parse_rate: if total_commands > 0 {
                (successful_parses as f64 / total_commands as f64) * 100.0
            } else {
                0.0
            },
            execution_rate: if total_commands > 0 {
                (successful_executions as f64 / total_commands as f64) * 100.0
            } else {
                0.0
            },
            results,
        }
    }

    fn parse_command_response(&self, response: &str) -> Option<ParsedCommand> {
        // å°è¯•è§£æJSONå“åº”
        if let Ok(json) = serde_json::from_str::<Value>(response) {
            if let Some(obj) = json.as_object() {
                let action = obj.get("action")
                    .and_then(|v| v.as_str())
                    .unwrap_or("unknown")
                    .to_string();

                let device_type = obj.get("device_type")
                    .and_then(|v| v.as_str())
                    .map(String::from);

                let device_id = obj.get("device_id")
                    .and_then(|v| v.as_str())
                    .map(String::from);

                let mut parameters = HashMap::new();
                if let Some(params) = obj.get("parameters") {
                    if let Some(obj) = params.as_object() {
                        for (key, value) in obj {
                            parameters.insert(key.clone(), value.clone());
                        }
                    }
                }

                return Some(ParsedCommand {
                    action,
                    device_type,
                    device_id,
                    parameters,
                });
            }
        }

        // å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å‘½ä»¤
        let lower = response.to_lowercase();
        if lower.contains("æ‰“å¼€") || lower.contains("å¯åŠ¨") || lower.contains("on") {
            Some(ParsedCommand {
                action: "turn_on".to_string(),
                device_type: None,
                device_id: None,
                parameters: HashMap::new(),
            })
        } else if lower.contains("å…³é—­") || lower.contains("åœæ­¢") || lower.contains("off") {
            Some(ParsedCommand {
                action: "turn_off".to_string(),
                device_type: None,
                device_id: None,
                parameters: HashMap::new(),
            })
        } else {
            None
        }
    }
}

// ============================================================================
// è§„åˆ™å¼•æ“ç”Ÿæˆæµ‹è¯•å™¨
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RuleGenerationResult {
    pub description: String,
    pub llm_generated_dsl: String,
    pub is_valid_dsl: bool,
    pub parse_error: Option<String>,
    pub parse_success: bool,
    pub execution_time_ms: u128,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RuleGenerationTestResult {
    pub total_rules: usize,
    pub valid_dsl_count: usize,
    pub parse_success_count: usize,
    pub dsl_validity_rate: f64,
    pub parse_success_rate: f64,
    pub results: Vec<RuleGenerationResult>,
}

pub struct RuleGenerationTester {
    llm: Arc<dyn LlmRuntime>,
    config: TestConfig,
}

impl RuleGenerationTester {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let config = TestConfig::default();
        let llm_config = serde_json::json!({
            "endpoint": config.endpoint,
            "model": config.model
        });

        let llm = create_backend("ollama", &llm_config)?;

        Ok(Self { llm, config })
    }

    /// æµ‹è¯•è§„åˆ™å¼•æ“ç”Ÿæˆæ­£ç¡®ç‡
    pub async fn test_rule_generation(&self, rule_descriptions: Vec<&str>) -> RuleGenerationTestResult {
        let mut results = Vec::new();

        let dsl_template = r#"RULE "è§„åˆ™åç§°"
WHEN device_id.metric > 50
FOR 5 minutes
DO
    NOTIFY "å‘Šè­¦æ¶ˆæ¯"
    EXECUTE device_id.command(param=value)
END"#;

        for description in rule_descriptions {
            let system_prompt = format!(r#"ä½ æ˜¯ NeoTalk è§„åˆ™å¼•æ“åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„æè¿°ç”Ÿæˆè§„åˆ™DSLã€‚

DSLæ ¼å¼:
{}

å¯ç”¨è®¾å¤‡:
- temp_sensor: æ¸©åº¦ä¼ æ„Ÿå™¨ï¼Œmetrics: [temperature], commands: []
- humidity_sensor: æ¹¿åº¦ä¼ æ„Ÿå™¨ï¼Œmetrics: [humidity], commands: []
- light_switch: æ™ºèƒ½ç¯ï¼Œmetrics: [power, brightness], commands: [turn_on, turn_off, set_brightness]
- air_conditioner: ç©ºè°ƒï¼Œmetrics: [current_temp, target_temp], commands: [turn_on, turn_off, set_temperature]

è¯·åªè¿”å›DSLä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#, dsl_template);

            let messages = vec![
                Message {
                    role: MessageRole::System,
                    content: Content::Text(system_prompt),
                    timestamp: None,
                },
                Message {
                    role: MessageRole::User,
                    content: Content::Text(description.to_string()),
                    timestamp: None,
                },
            ];

            let llm_input = LlmInput {
                messages,
                params: GenerationParams {
                    max_tokens: Some(300),
                    temperature: Some(0.3),
                    ..Default::default()
                },
                model: Some(self.config.model.clone()),
                stream: false,
                tools: None,
            };

            let start = std::time::Instant::now();

            let result = match tokio::time::timeout(
                Duration::from_secs(self.config.timeout_secs),
                self.llm.generate(llm_input)
            ).await {
                Ok(Ok(output)) => {
                    let llm_generated_dsl = output.text;
                    let is_valid_dsl = self.looks_like_valid_dsl(&llm_generated_dsl);
                    let parse_result = RuleDslParser::parse(&llm_generated_dsl);
                    let parse_success = parse_result.is_ok();

                    RuleGenerationResult {
                        description: description.to_string(),
                        llm_generated_dsl,
                        is_valid_dsl,
                        parse_error: parse_result.err().map(|e| e.to_string()),
                        parse_success,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
                Ok(Err(e)) => {
                    RuleGenerationResult {
                        description: description.to_string(),
                        llm_generated_dsl: "".to_string(),
                        is_valid_dsl: false,
                        parse_error: Some(format!("LLMé”™è¯¯: {:?}", e)),
                        parse_success: false,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
                Err(_) => {
                    RuleGenerationResult {
                        description: description.to_string(),
                        llm_generated_dsl: "".to_string(),
                        is_valid_dsl: false,
                        parse_error: Some("è¶…æ—¶".to_string()),
                        parse_success: false,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
            };

            results.push(result);
        }

        let total_rules = results.len();
        let valid_dsl_count = results.iter().filter(|r| r.is_valid_dsl).count();
        let parse_success_count = results.iter().filter(|r| r.parse_success).count();

        RuleGenerationTestResult {
            total_rules,
            valid_dsl_count,
            parse_success_count,
            dsl_validity_rate: if total_rules > 0 {
                (valid_dsl_count as f64 / total_rules as f64) * 100.0
            } else {
                0.0
            },
            parse_success_rate: if total_rules > 0 {
                (parse_success_count as f64 / total_rules as f64) * 100.0
            } else {
                0.0
            },
            results,
        }
    }

    fn looks_like_valid_dsl(&self, text: &str) -> bool {
        let trimmed = text.trim();
        !trimmed.is_empty()
            && (trimmed.contains("RULE")
                || trimmed.contains("WHEN")
                || trimmed.contains("DO"))
    }
}

// ============================================================================
// å·¥ä½œæµç”Ÿæˆæµ‹è¯•å™¨
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowGenerationResult {
    pub description: String,
    pub llm_generated_workflow: String,
    pub has_valid_structure: bool,
    pub has_steps: bool,
    pub has_conditions: bool,
    pub is_executable: bool,
    pub execution_time_ms: u128,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowGenerationTestResult {
    pub total_workflows: usize,
    pub valid_structure_count: usize,
    pub has_steps_count: usize,
    pub executable_count: usize,
    pub structure_validity_rate: f64,
    pub executability_rate: f64,
    pub results: Vec<WorkflowGenerationResult>,
}

pub struct WorkflowGenerationTester {
    llm: Arc<dyn LlmRuntime>,
    config: TestConfig,
}

impl WorkflowGenerationTester {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let config = TestConfig::default();
        let llm_config = serde_json::json!({
            "endpoint": config.endpoint,
            "model": config.model
        });

        let llm = create_backend("ollama", &llm_config)?;

        Ok(Self { llm, config })
    }

    /// æµ‹è¯•å·¥ä½œæµç”Ÿæˆæ­£ç¡®ç‡å’Œå¯æ‰§è¡Œç‡
    pub async fn test_workflow_generation(&self, workflow_descriptions: Vec<&str>) -> WorkflowGenerationTestResult {
        let mut results = Vec::new();

        let workflow_template = r#"WORKFLOW "å·¥ä½œæµåç§°"
STEPS:
    1. IF condition THEN action
    2. action
    3. WHILE condition action
CONDITIONS:
    - condition_expression
ACTIONS:
    - action_name
END"#;

        for description in workflow_descriptions {
            let system_prompt = format!(r#"ä½ æ˜¯ NeoTalk å·¥ä½œæµå¼•æ“åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„æè¿°ç”Ÿæˆå·¥ä½œæµå®šä¹‰ã€‚

å·¥ä½œæµæ ¼å¼:
{}

å¯ç”¨æ“ä½œ:
- check_device_status: æ£€æŸ¥è®¾å¤‡çŠ¶æ€
- send_command: å‘é€è®¾å¤‡å‘½ä»¤
- wait: ç­‰å¾…ä¸€æ®µæ—¶é—´
- notify: å‘é€é€šçŸ¥
- log: è®°å½•æ—¥å¿—
- trigger_rule: è§¦å‘è§„åˆ™

è¯·åªè¿”å›å·¥ä½œæµå®šä¹‰ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#, workflow_template);

            let messages = vec![
                Message {
                    role: MessageRole::System,
                    content: Content::Text(system_prompt),
                    timestamp: None,
                },
                Message {
                    role: MessageRole::User,
                    content: Content::Text(description.to_string()),
                    timestamp: None,
                },
            ];

            let llm_input = LlmInput {
                messages,
                params: GenerationParams {
                    max_tokens: Some(400),
                    temperature: Some(0.3),
                    ..Default::default()
                },
                model: Some(self.config.model.clone()),
                stream: false,
                tools: None,
            };

            let start = std::time::Instant::now();

            let result = match tokio::time::timeout(
                Duration::from_secs(self.config.timeout_secs),
                self.llm.generate(llm_input)
            ).await {
                Ok(Ok(output)) => {
                    let llm_generated_workflow = output.text;
                    let has_valid_structure = self.looks_like_valid_workflow(&llm_generated_workflow);
                    let has_steps = has_steps(&llm_generated_workflow);
                    let has_conditions = has_conditions(&llm_generated_workflow);
                    let is_executable = has_valid_structure && has_steps;

                    WorkflowGenerationResult {
                        description: description.to_string(),
                        llm_generated_workflow,
                        has_valid_structure,
                        has_steps,
                        has_conditions,
                        is_executable,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
                Ok(Err(_e)) => {
                    WorkflowGenerationResult {
                        description: description.to_string(),
                        llm_generated_workflow: "".to_string(),
                        has_valid_structure: false,
                        has_steps: false,
                        has_conditions: false,
                        is_executable: false,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
                Err(_) => {
                    WorkflowGenerationResult {
                        description: description.to_string(),
                        llm_generated_workflow: "".to_string(),
                        has_valid_structure: false,
                        has_steps: false,
                        has_conditions: false,
                        is_executable: false,
                        execution_time_ms: start.elapsed().as_millis(),
                    }
                }
            };

            results.push(result);
        }

        let total_workflows = results.len();
        let valid_structure_count = results.iter().filter(|r| r.has_valid_structure).count();
        let has_steps_count = results.iter().filter(|r| r.has_steps).count();
        let executable_count = results.iter().filter(|r| r.is_executable).count();

        WorkflowGenerationTestResult {
            total_workflows,
            valid_structure_count,
            has_steps_count,
            executable_count,
            structure_validity_rate: if total_workflows > 0 {
                (valid_structure_count as f64 / total_workflows as f64) * 100.0
            } else {
                0.0
            },
            executability_rate: if total_workflows > 0 {
                (executable_count as f64 / total_workflows as f64) * 100.0
            } else {
                0.0
            },
            results,
        }
    }

    fn looks_like_valid_workflow(&self, text: &str) -> bool {
        let trimmed = text.trim();
        !trimmed.is_empty()
            && (trimmed.contains("WORKFLOW")
                || trimmed.contains("STEPS")
                || trimmed.contains("æ­¥éª¤"))
    }
}

fn has_steps(text: &str) -> bool {
    text.contains("æ­¥éª¤") || text.contains("STEPS") || text.contains("1.") || text.contains("ç¬¬ä¸€æ­¥")
}

fn has_conditions(text: &str) -> bool {
    text.contains("IF") || text.contains("å¦‚æœ") || text.contains("WHEN") || text.contains("å½“")
}

// ============================================================================
// ç»¼åˆæµ‹è¯•
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComprehensiveTestResult {
    pub empty_response_analysis: EmptyResponseAnalysis,
    pub command_execution: CommandExecutionTestResult,
    pub rule_generation: RuleGenerationTestResult,
    pub workflow_generation: WorkflowGenerationTestResult,
    pub overall_score: f64,
}

pub async fn run_comprehensive_test() -> ComprehensiveTestResult {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   NeoTalk ç»¼åˆLLMåˆ†ææµ‹è¯•                                            â•‘");
    println!("â•‘   æ¨¡å‹: {}                                                      â•‘", TEST_MODEL);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // 1. ç©ºå“åº”åˆ†æ
    println!("\nğŸ“Š æµ‹è¯•1: ç©ºå“åº”é—®é¢˜åˆ†æ");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let analyzer = match EmptyResponseAnalyzer::new().await {
        Ok(a) => a,
        Err(e) => {
            println!("âš ï¸  æ— æ³•åˆ›å»ºåˆ†æå™¨: {:?}ï¼Œè·³è¿‡æµ‹è¯•", e);
            println!("\nè¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve");
            println!("å®‰è£…æ¨¡å‹: ollama pull {}", TEST_MODEL);

            // è¿”å›ç©ºç»“æœ
            return ComprehensiveTestResult {
                empty_response_analysis: EmptyResponseAnalysis {
                    total_requests: 0,
                    empty_responses: 0,
                    empty_rate: 0.0,
                    empty_by_category: HashMap::new(),
                    response_lengths: Vec::new(),
                    avg_response_length: 0.0,
                    raw_responses: Vec::new(),
                },
                command_execution: CommandExecutionTestResult {
                    total_commands: 0,
                    successful_parses: 0,
                    successful_executions: 0,
                    parse_rate: 0.0,
                    execution_rate: 0.0,
                    results: Vec::new(),
                },
                rule_generation: RuleGenerationTestResult {
                    total_rules: 0,
                    valid_dsl_count: 0,
                    parse_success_count: 0,
                    dsl_validity_rate: 0.0,
                    parse_success_rate: 0.0,
                    results: Vec::new(),
                },
                workflow_generation: WorkflowGenerationTestResult {
                    total_workflows: 0,
                    valid_structure_count: 0,
                    has_steps_count: 0,
                    executable_count: 0,
                    structure_validity_rate: 0.0,
                    executability_rate: 0.0,
                    results: Vec::new(),
                },
                overall_score: 0.0,
            };
        }
    };

    let test_inputs = vec![
        "ä½ å¥½",
        "è¯·å‘Šè¯‰æˆ‘å½“å‰æ—¶é—´",
        "å¸®æˆ‘æ‰“å¼€å®¢å…çš„ç¯",
        "å…³é—­å§å®¤çš„ç©ºè°ƒ",
        "æŸ¥çœ‹æ¸©åº¦ä¼ æ„Ÿå™¨æ•°æ®",
        "è®¾ç½®ç©ºè°ƒæ¸©åº¦åˆ°26åº¦",
        "å¯åŠ¨æ‰€æœ‰è®¾å¤‡",
        "åœæ­¢æµ‡æ°´ç³»ç»Ÿ",
        "æŸ¥çœ‹æ‰€æœ‰åœ¨çº¿è®¾å¤‡",
        "åˆ›å»ºä¸€æ¡æ–°è§„åˆ™",
        "æ¸©åº¦è¶…è¿‡30åº¦æ—¶æ‰“å¼€é£æ‰‡",
        "æ¹¿åº¦ä½äº40%æ—¶å¯åŠ¨åŠ æ¹¿å™¨",
        "æœ‰äººåœ¨æ—¶è‡ªåŠ¨å¼€ç¯",
        "ç¦»å¼€å®¶æ—¶å…³é—­æ‰€æœ‰ç”µå™¨",
        "æ—©ä¸Š7ç‚¹è‡ªåŠ¨æ‰“å¼€çª—å¸˜",
        "æ£€æµ‹åˆ°çƒŸé›¾æ—¶æŠ¥è­¦",
        "å®¤å†…PM2.5è¶…è¿‡100æ—¶å¯åŠ¨ç©ºæ°”å‡€åŒ–å™¨",
        "ç”µä»·ä½è°·æ—¶ç»™ç”µåŠ¨è½¦å……ç”µ",
        "ç”¨æ°´é‡å¼‚å¸¸æ—¶é€šçŸ¥ç”¨æˆ·",
        "å¤œé—´å®‰é˜²æ¨¡å¼å¯åŠ¨",
    ];

    let empty_response_analysis = analyzer.analyze_empty_responses(test_inputs).await;

    println!("æ€»è¯·æ±‚æ•°: {}", empty_response_analysis.total_requests);
    println!("ç©ºå“åº”æ•°: {}", empty_response_analysis.empty_responses);
    println!("ç©ºå“åº”ç‡: {:.1}%", empty_response_analysis.empty_rate);
    println!("å¹³å‡å“åº”é•¿åº¦: {:.1}å­—ç¬¦", empty_response_analysis.avg_response_length);
    println!("\nç©ºå“åº”åˆ†ç±»:");
    for (category, count) in &empty_response_analysis.empty_by_category {
        println!("  - {}: {}æ¬¡", category, count);
    }

    // 2. å‘½ä»¤ä¸‹å‘æµ‹è¯•
    println!("\nâš¡ æµ‹è¯•2: å‘½ä»¤ä¸‹å‘åŠŸèƒ½");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let command_tester = match CommandExecutorTester::new().await {
        Ok(c) => c,
        Err(_) => {
            return ComprehensiveTestResult {
                empty_response_analysis,
                command_execution: CommandExecutionTestResult {
                    total_commands: 0,
                    successful_parses: 0,
                    successful_executions: 0,
                    parse_rate: 0.0,
                    execution_rate: 0.0,
                    results: Vec::new(),
                },
                rule_generation: RuleGenerationTestResult {
                    total_rules: 0,
                    valid_dsl_count: 0,
                    parse_success_count: 0,
                    dsl_validity_rate: 0.0,
                    parse_success_rate: 0.0,
                    results: Vec::new(),
                },
                workflow_generation: WorkflowGenerationTestResult {
                    total_workflows: 0,
                    valid_structure_count: 0,
                    has_steps_count: 0,
                    executable_count: 0,
                    structure_validity_rate: 0.0,
                    executability_rate: 0.0,
                    results: Vec::new(),
                },
                overall_score: 0.0,
            };
        }
    };

    let commands = vec![
        ("æ‰“å¼€å®¢å…çš„ç¯", serde_json::json!({"device": "light", "action": "on"})),
        ("å…³é—­å§å®¤ç©ºè°ƒ", serde_json::json!({"device": "ac", "action": "off"})),
        ("è®¾ç½®æ¸©åº¦ä¸º26åº¦", serde_json::json!({"device": "thermostat", "temp": 26})),
        ("å¯åŠ¨æµ‡æ°´ç³»ç»Ÿ", serde_json::json!({"device": "irrigation", "action": "on"})),
        ("æ‰“å¼€æ‰€æœ‰é£æ‰‡", serde_json::json!({"device": "fan", "action": "on"})),
        ("å…³é—­é—¨é”", serde_json::json!({"device": "lock", "action": "lock"})),
        ("æ‰“å¼€çª—å¸˜", serde_json::json!({"device": "curtain", "action": "open"})),
        ("è®¾ç½®äº®åº¦ä¸º80%", serde_json::json!({"device": "light", "brightness": 80})),
        ("å¯åŠ¨é™¤æ¹¿æœº", serde_json::json!({"device": "dehumidifier", "action": "on"})),
        ("å…³é—­æ‰€æœ‰ç¯å…‰", serde_json::json!({"device": "all_lights", "action": "off"})),
    ];

    let command_execution = command_tester.test_command_execution(commands).await;

    println!("æ€»å‘½ä»¤æ•°: {}", command_execution.total_commands);
    println!("æˆåŠŸè§£æ: {}", command_execution.successful_parses);
    println!("æˆåŠŸæ‰§è¡Œ: {}", command_execution.successful_executions);
    println!("è§£æç‡: {:.1}%", command_execution.parse_rate);
    println!("æ‰§è¡Œç‡: {:.1}%", command_execution.execution_rate);

    // 3. è§„åˆ™ç”Ÿæˆæµ‹è¯•
    println!("\nğŸ“œ æµ‹è¯•3: è§„åˆ™å¼•æ“ç”Ÿæˆ");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let rule_tester = match RuleGenerationTester::new().await {
        Ok(r) => r,
        Err(_) => {
            return ComprehensiveTestResult {
                empty_response_analysis,
                command_execution,
                rule_generation: RuleGenerationTestResult {
                    total_rules: 0,
                    valid_dsl_count: 0,
                    parse_success_count: 0,
                    dsl_validity_rate: 0.0,
                    parse_success_rate: 0.0,
                    results: Vec::new(),
                },
                workflow_generation: WorkflowGenerationTestResult {
                    total_workflows: 0,
                    valid_structure_count: 0,
                    has_steps_count: 0,
                    executable_count: 0,
                    structure_validity_rate: 0.0,
                    executability_rate: 0.0,
                    results: Vec::new(),
                },
                overall_score: 0.0,
            };
        }
    };

    let rule_descriptions = vec![
        "å½“æ¸©åº¦è¶…è¿‡30åº¦æ—¶ï¼Œæ‰“å¼€é£æ‰‡",
        "æ¹¿åº¦ä½äº40%æ—¶ï¼Œå¯åŠ¨åŠ æ¹¿å™¨",
        "æ£€æµ‹åˆ°æœ‰äººç§»åŠ¨æ—¶ï¼Œè‡ªåŠ¨å¼€ç¯",
        "å½“CO2æµ“åº¦è¶…è¿‡1000ppmæ—¶ï¼Œå¯åŠ¨æ–°é£ç³»ç»Ÿ",
        "å½“PM2.5è¶…è¿‡100æ—¶ï¼Œå¯åŠ¨ç©ºæ°”å‡€åŒ–å™¨",
        "å½“æ°´ä½è¶…è¿‡è­¦æˆ’çº¿æ—¶ï¼Œå‘é€æŠ¥è­¦",
        "å½“å®¤å†…æ— äººæ—¶ï¼Œå…³é—­æ‰€æœ‰ç¯å…‰",
        "å½“ç”¨ç”µé‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå‘é€é€šçŸ¥",
        "å½“é—¨çª—å¼‚å¸¸æ‰“å¼€æ—¶ï¼Œè§¦å‘å®‰é˜²æŠ¥è­¦",
        "å½“æ¸©åº¦ä½äº18åº¦æ—¶ï¼Œå¯åŠ¨åŠ çƒ­æ¨¡å¼",
    ];

    let rule_generation = rule_tester.test_rule_generation(rule_descriptions).await;

    println!("æ€»è§„åˆ™æ•°: {}", rule_generation.total_rules);
    println!("æœ‰æ•ˆDSLæ•°: {}", rule_generation.valid_dsl_count);
    println!("è§£ææˆåŠŸæ•°: {}", rule_generation.parse_success_count);
    println!("DSLæœ‰æ•ˆç‡: {:.1}%", rule_generation.dsl_validity_rate);
    println!("è§£ææˆåŠŸç‡: {:.1}%", rule_generation.parse_success_rate);

    // 4. å·¥ä½œæµç”Ÿæˆæµ‹è¯•
    println!("\nğŸ”„ æµ‹è¯•4: å·¥ä½œæµç”Ÿæˆ");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let workflow_tester = match WorkflowGenerationTester::new().await {
        Ok(w) => w,
        Err(_) => {
            return ComprehensiveTestResult {
                empty_response_analysis,
                command_execution,
                rule_generation,
                workflow_generation: WorkflowGenerationTestResult {
                    total_workflows: 0,
                    valid_structure_count: 0,
                    has_steps_count: 0,
                    executable_count: 0,
                    structure_validity_rate: 0.0,
                    executability_rate: 0.0,
                    results: Vec::new(),
                },
                overall_score: 0.0,
            };
        }
    };

    let workflow_descriptions = vec![
        "å›å®¶æ¨¡å¼ï¼šæ‰“å¼€ç¯å…‰ï¼Œè°ƒèŠ‚ç©ºè°ƒæ¸©åº¦ï¼Œæ’­æ”¾éŸ³ä¹",
        "ç¦»å®¶æ¨¡å¼ï¼šå…³é—­æ‰€æœ‰ç”µå™¨ï¼Œå¯åŠ¨å®‰é˜²ç³»ç»Ÿ",
        "ç¡çœ æ¨¡å¼ï¼šå…³é—­æ‰€æœ‰ç¯å…‰ï¼Œé™ä½ç©ºè°ƒå™ªéŸ³",
        "èµ·åºŠæ¨¡å¼ï¼šæ‰“å¼€çª—å¸˜ï¼Œå¯åŠ¨å’–å•¡æœºï¼Œæ’­æ”¾è½»éŸ³ä¹",
        "è§‚å½±æ¨¡å¼ï¼šå…³é—­çª—å¸˜ï¼Œè°ƒæš—ç¯å…‰ï¼Œè°ƒèŠ‚ç©ºè°ƒ",
        "ä¼šè®®æ¨¡å¼ï¼šå…³é—­èƒŒæ™¯éŸ³ä¹ï¼Œè°ƒäº®ç¯å…‰ï¼Œå¯åŠ¨æŠ•å½±ä»ª",
        "é˜…è¯»æ¨¡å¼ï¼šæ‰“å¼€é˜…è¯»ç¯ï¼Œè°ƒèŠ‚ç©ºè°ƒèˆ’é€‚æ¸©åº¦",
        "è¿åŠ¨æ¨¡å¼ï¼šæ’­æ”¾åŠ¨æ„ŸéŸ³ä¹ï¼Œè°ƒäº®ç¯å…‰ï¼Œå¯åŠ¨é£æ‰‡",
        "èŠ‚èƒ½æ¨¡å¼ï¼šå…³é—­éå¿…è¦è®¾å¤‡ï¼Œè°ƒèŠ‚ç©ºè°ƒè‡³èŠ‚èƒ½æ¸©åº¦",
        "æ¸…æ´æ¨¡å¼ï¼šå¯åŠ¨æ‰«åœ°æœºå™¨äººï¼Œæ‰“å¼€çª—å¸˜",
    ];

    let workflow_generation = workflow_tester.test_workflow_generation(workflow_descriptions).await;

    println!("æ€»å·¥ä½œæµæ•°: {}", workflow_generation.total_workflows);
    println!("æœ‰æ•ˆç»“æ„æ•°: {}", workflow_generation.valid_structure_count);
    println!("åŒ…å«æ­¥éª¤æ•°: {}", workflow_generation.has_steps_count);
    println!("å¯æ‰§è¡Œæ•°: {}", workflow_generation.executable_count);
    println!("ç»“æ„æœ‰æ•ˆç‡: {:.1}%", workflow_generation.structure_validity_rate);
    println!("å¯æ‰§è¡Œç‡: {:.1}%", workflow_generation.executability_rate);

    // 5. ç»¼åˆè¯„åˆ†
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   ç»¼åˆè¯„ä¼°                                                           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let overall_score = (
        (100.0 - empty_response_analysis.empty_rate) * 0.3 +  // ç©ºå“åº”ç‡æƒé‡30%
        command_execution.execution_rate * 0.25 +             // å‘½ä»¤æ‰§è¡Œæƒé‡25%
        rule_generation.parse_success_rate * 0.25 +            // è§„åˆ™ç”Ÿæˆæƒé‡25%
        workflow_generation.executability_rate * 0.2          // å·¥ä½œæµç”Ÿæˆæƒé‡20%
    );

    println!("\nğŸ“ˆ å„é¡¹å¾—åˆ†:");
    println!("   å“åº”å¯ç”¨æ€§: {:.1}/100", 100.0 - empty_response_analysis.empty_rate);
    println!("   å‘½ä»¤æ‰§è¡Œç‡: {:.1}/100", command_execution.execution_rate);
    println!("   è§„åˆ™è§£æç‡: {:.1}/100", rule_generation.parse_success_rate);
    println!("   å·¥ä½œæµå¯æ‰§è¡Œç‡: {:.1}/100", workflow_generation.executability_rate);
    println!("\n   ç»¼åˆè¯„åˆ†: {:.1}/100", overall_score);
    println!("   è¯„çº§: {}", if overall_score >= 80.0 {
        "â­â­â­â­ ä¼˜ç§€"
    } else if overall_score >= 60.0 {
        "â­â­â­ ä¸­ç­‰"
    } else if overall_score >= 40.0 {
        "â­â­ åŠæ ¼"
    } else {
        "â­ éœ€æ”¹è¿›"
    });

    ComprehensiveTestResult {
        empty_response_analysis,
        command_execution,
        rule_generation,
        workflow_generation,
        overall_score,
    }
}

// ============================================================================
// æµ‹è¯•å…¥å£
// ============================================================================

#[tokio::test]
async fn test_comprehensive_llm_analysis() {
    let result = run_comprehensive_test().await;

    // æ–­è¨€å…³é”®æŒ‡æ ‡
    assert!(result.empty_response_analysis.total_requests > 0, "åº”è¯¥æœ‰æµ‹è¯•æ•°æ®");

    // å¦‚æœOllamaå¯ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸€å®šæˆåŠŸç‡
    if result.empty_response_analysis.total_requests > 10 {
        let success_rate = 100.0 - result.empty_response_analysis.empty_rate;
        assert!(success_rate > 0.0, "åº”è¯¥æœ‰è‡³å°‘ä¸€äº›æˆåŠŸçš„å“åº”");
    }
}
