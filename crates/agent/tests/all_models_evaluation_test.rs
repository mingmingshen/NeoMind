//! NeoTalk å…¨æ¨¡å‹ç»¼åˆæµ‹è¯•
//!
//! æµ‹è¯•æ‰€æœ‰æœ¬åœ°LLMæ¨¡å‹åœ¨NeoTalkç³»ç»Ÿä¸­çš„è¡¨ç°
//! è¯„ä¼°ç»´åº¦ï¼šå“åº”å¯ç”¨æ€§ã€å“åº”è´¨é‡ã€æŒ‡ä»¤ç†è§£ã€å“åº”é€Ÿåº¦
//!
//! **æµ‹è¯•æ—¥æœŸ**: 2026-01-17

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::cmp::Ordering;
use serde::{Deserialize, Serialize};

use edge_ai_llm::backends::create_backend;
use edge_ai_core::llm::backend::{GenerationParams, LlmInput};
use edge_ai_core::message::{Message, MessageRole, Content};

const OLLAMA_ENDPOINT: &str = "http://localhost:11434";

/// æ¨¡å‹æµ‹è¯•ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelTestResult {
    pub model_name: String,
    pub total_tests: usize,
    pub successful_responses: usize,
    pub empty_responses: usize,
    pub short_responses: usize,  // < 10 chars
    pub avg_response_length: f64,
    pub avg_response_time_ms: f64,
    pub response_quality_score: f64,
    pub command_understanding_rate: f64,
    pub overall_score: f64,
}

/// å•æ¬¡å¯¹è¯æµ‹è¯•ç»“æœ
#[derive(Debug, Clone)]
struct SingleTestResult {
    pub model: String,
    pub prompt: String,
    pub response: String,
    pub response_length: usize,
    pub response_time_ms: u128,
    pub is_empty: bool,
    pub is_short: bool,
    pub has_command: bool,
}

/// æ¨¡å‹æµ‹è¯•å™¨
pub struct ModelTester {
    endpoint: String,
    timeout_secs: u64,
}

impl ModelTester {
    pub fn new() -> Self {
        Self {
            endpoint: OLLAMA_ENDPOINT.to_string(),
            timeout_secs: 60,
        }
    }

    /// æµ‹è¯•å•ä¸ªæ¨¡å‹
    pub async fn test_model(&self, model_name: &str, test_prompts: Vec<&str>) -> ModelTestResult {
        println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        println!("â•‘   æµ‹è¯•æ¨¡å‹: {:58}â•‘", model_name);
        println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

        let llm_config = serde_json::json!({
            "endpoint": self.endpoint,
            "model": model_name
        });

        let llm = match create_backend("ollama", &llm_config) {
            Ok(l) => Arc::new(l),
            Err(e) => {
                println!("âš ï¸  æ— æ³•åŠ è½½æ¨¡å‹: {:?}", e);
                return ModelTestResult {
                    model_name: model_name.to_string(),
                    total_tests: 0,
                    successful_responses: 0,
                    empty_responses: 0,
                    short_responses: 0,
                    avg_response_length: 0.0,
                    avg_response_time_ms: 0.0,
                    response_quality_score: 0.0,
                    command_understanding_rate: 0.0,
                    overall_score: 0.0,
                };
            }
        };

        let mut results = Vec::new();

        for (i, prompt) in test_prompts.iter().enumerate() {
            print!("[{:2}] {:50} | ", i + 1, &prompt[..prompt.len().min(50)]);

            let start = Instant::now();

            let system_prompt = "ä½ æ˜¯ NeoTalk æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚";

            let messages = vec![
                Message {
                    role: MessageRole::System,
                    content: Content::Text(system_prompt.to_string()),
                    timestamp: None,
                },
                Message {
                    role: MessageRole::User,
                    content: Content::Text(prompt.to_string()),
                    timestamp: None,
                },
            ];

            let llm_input = LlmInput {
                messages,
                params: GenerationParams {
                    max_tokens: Some(200),
                    temperature: Some(0.7),
                    ..Default::default()
                },
                model: Some(model_name.to_string()),
                stream: false,
                tools: None,
            };

            let result = match tokio::time::timeout(
                Duration::from_secs(self.timeout_secs),
                llm.generate(llm_input)
            ).await {
                Ok(Ok(output)) => {
                    let response = output.text;
                    let response_length = response.len();
                    let is_empty = response.trim().is_empty();
                    let is_short = response_length > 0 && response_length < 10;
                    let has_command = self.detect_command(&response);

                    let status = if is_empty {
                        "âŒ ç©º"
                    } else if is_short {
                        "âš ï¸ çŸ­"
                    } else {
                        "âœ…"
                    };

                    println!("{} | {}å­—ç¬¦ | {}ms | {}", status, response_length, start.elapsed().as_millis(),
                        if has_command { "âš¡å‘½ä»¤" } else { "" });

                    SingleTestResult {
                        model: model_name.to_string(),
                        prompt: prompt.to_string(),
                        response,
                        response_length,
                        response_time_ms: start.elapsed().as_millis(),
                        is_empty,
                        is_short,
                        has_command,
                    }
                }
                Ok(Err(e)) => {
                    println!("âŒ é”™è¯¯ | {:?}", e);
                    SingleTestResult {
                        model: model_name.to_string(),
                        prompt: prompt.to_string(),
                        response: String::new(),
                        response_length: 0,
                        response_time_ms: start.elapsed().as_millis(),
                        is_empty: true,
                        is_short: false,
                        has_command: false,
                    }
                }
                Err(_) => {
                    println!("âŒ è¶…æ—¶");
                    SingleTestResult {
                        model: model_name.to_string(),
                        prompt: prompt.to_string(),
                        response: String::new(),
                        response_length: 0,
                        response_time_ms: (self.timeout_secs * 1000) as u128,
                        is_empty: true,
                        is_short: false,
                        has_command: false,
                    }
                }
            };

            results.push(result);
        }

        // è®¡ç®—ç»Ÿè®¡æ•°æ®
        let total_tests = results.len();
        let successful_responses = results.iter().filter(|r| !r.is_empty).count();
        let empty_responses = results.iter().filter(|r| r.is_empty).count();
        let short_responses = results.iter().filter(|r| r.is_short).count();
        let avg_response_length = if !results.is_empty() {
            results.iter().map(|r| r.response_length).sum::<usize>() as f64 / total_tests as f64
        } else {
            0.0
        };
        let avg_response_time_ms = if !results.is_empty() {
            results.iter().map(|r| r.response_time_ms).sum::<u128>() as f64 / total_tests as f64
        } else {
            0.0
        };

        // å“åº”è´¨é‡è¯„åˆ†
        let long_responses = results.iter().filter(|r| r.response_length >= 20).count();
        let response_quality_score = if total_tests > 0 {
            (long_responses as f64 / total_tests as f64) * 100.0
        } else {
            0.0
        };

        // æŒ‡ä»¤ç†è§£ç‡
        let has_command = results.iter().filter(|r| r.has_command).count();
        let command_understanding_rate = if total_tests > 0 {
            (has_command as f64 / total_tests as f64) * 100.0
        } else {
            0.0
        };

        // ç»¼åˆè¯„åˆ†
        let availability_score = if total_tests > 0 {
            (successful_responses as f64 / total_tests as f64) * 100.0
        } else {
            0.0
        };

        let overall_score = availability_score * 0.4 +
            response_quality_score * 0.3 +
            command_understanding_rate * 0.3;

        println!("\nğŸ“Š {} æµ‹è¯•ç»“æœ:", model_name);
        println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
        println!("  æ€»æµ‹è¯•æ•°: {}", total_tests);
        println!("  æˆåŠŸå“åº”: {} ({:.1}%)", successful_responses, availability_score);
        println!("  ç©ºå“åº”: {} ({:.1}%)", empty_responses, (empty_responses as f64 / total_tests as f64) * 100.0);
        println!("  çŸ­å“åº”(<10å­—ç¬¦): {} ({:.1}%)", short_responses, (short_responses as f64 / total_tests as f64) * 100.0);
        println!("  å¹³å‡é•¿åº¦: {:.1} å­—ç¬¦", avg_response_length);
        println!("  å¹³å‡å“åº”æ—¶é—´: {:.1} ms", avg_response_time_ms);
        println!("  å“åº”è´¨é‡: {:.1}/100", response_quality_score);
        println!("  æŒ‡ä»¤ç†è§£: {:.1}/100", command_understanding_rate);
        println!("  ç»¼åˆè¯„åˆ†: {:.1}/100", overall_score);

        ModelTestResult {
            model_name: model_name.to_string(),
            total_tests,
            successful_responses,
            empty_responses,
            short_responses,
            avg_response_length,
            avg_response_time_ms,
            response_quality_score,
            command_understanding_rate,
            overall_score,
        }
    }

    fn detect_command(&self, response: &str) -> bool {
        let lower = response.to_lowercase();
        lower.contains("å‘½ä»¤")
            || lower.contains("æ‰§è¡Œ")
            || lower.contains("æ‰“å¼€")
            || lower.contains("å…³é—­")
            || lower.contains("å¯åŠ¨")
            || lower.contains("åœæ­¢")
            || lower.contains("è®¾ç½®")
    }
}

/// ç»¼åˆæµ‹è¯•ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComprehensiveModelTestResult {
    pub model_results: Vec<ModelTestResult>,
    pub best_overall_model: String,
    pub fastest_model: String,
    pub best_quality_model: String,
    pub most_reliable_model: String,
    pub recommendations: Vec<String>,
}

/// æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
pub async fn test_all_available_models() -> ComprehensiveModelTestResult {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   NeoTalk å…¨æ¨¡å‹ç»¼åˆæµ‹è¯•                                               â•‘");
    println!("â•‘   Ollamaç«¯ç‚¹: {:54}â•‘", OLLAMA_ENDPOINT);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // æµ‹è¯•æç¤ºè¯ - åŒ…å«å„ç§åœºæ™¯
    let test_prompts = vec![
        // åŸºç¡€å¯¹è¯
        "ä½ å¥½",
        "ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·",

        // è®¾å¤‡æ§åˆ¶
        "å¸®æˆ‘æ‰“å¼€å®¢å…çš„ç¯",
        "å…³é—­å§å®¤çš„ç©ºè°ƒ",
        "è®¾ç½®æ¸©åº¦ä¸º26åº¦",

        // æ•°æ®æŸ¥è¯¢
        "å½“å‰æ¸©åº¦æ˜¯å¤šå°‘",
        "æŸ¥çœ‹æ‰€æœ‰ä¼ æ„Ÿå™¨æ•°æ®",
        "ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å¦‚ä½•",

        // å¤æ‚æŒ‡ä»¤
        "åˆ›å»ºä¸€ä¸ªé«˜æ¸©å‘Šè­¦è§„åˆ™",
        "å½“æœ‰äººç§»åŠ¨æ—¶è‡ªåŠ¨å¼€ç¯",
        "è®¾ç½®æ¯å¤©æ—©ä¸Š7ç‚¹è‡ªåŠ¨æ‰“å¼€çª—å¸˜",

        // æ‰¹é‡æ“ä½œ
        "æ‰“å¼€æ‰€æœ‰æˆ¿é—´çš„ç¯",
        "å…³é—­æ‰€æœ‰çš„ç©ºè°ƒ",

        // å‘Šè­¦ç›¸å…³
        "æœ‰æ²¡æœ‰å¼‚å¸¸å‘Šè­¦",
        "æŸ¥çœ‹æ‰€æœ‰å†å²å‘Šè­¦",
    ];

    let tester = ModelTester::new();
    let mut model_results = Vec::new();

    // æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆä¸“æ³¨äºå¯¹è¯æ¨¡å‹ï¼‰
    let models_to_test = vec![
        "qwen3:1.7b",
        "deepseek-r1:1.5b",
        "qwen3-vl:2b",
        "qwen3:0.6b",
        "gemma3:270m",
        "qwen2:1.5b",
        "qwen2.5:3b",
        "gemma3:4b",
    ];

    for model in models_to_test {
        let result = tester.test_model(model, test_prompts.clone()).await;
        model_results.push(result);
    }

    // åˆ†æç»“æœ
    let mut best_overall_model = String::new();
    let mut best_overall_score = 0.0;

    let mut fastest_model = String::new();
    let mut fastest_time = f64::MAX;

    let mut best_quality_model = String::new();
    let mut best_quality_score = 0.0;

    let mut most_reliable_model = String::new();
    let mut best_reliability = 0.0;

    for result in &model_results {
        if result.total_tests > 0 {
            if result.overall_score > best_overall_score {
                best_overall_score = result.overall_score;
                best_overall_model = result.model_name.clone();
            }

            if result.avg_response_time_ms < fastest_time && result.avg_response_time_ms > 0.0 {
                fastest_time = result.avg_response_time_ms;
                fastest_model = result.model_name.clone();
            }

            if result.response_quality_score > best_quality_score {
                best_quality_score = result.response_quality_score;
                best_quality_model = result.model_name.clone();
            }

            let reliability = (result.total_tests - result.empty_responses) as f64 / result.total_tests as f64 * 100.0;
            if reliability > best_reliability {
                best_reliability = reliability;
                most_reliable_model = result.model_name.clone();
            }
        }
    }

    // æ‰“æœ€ç»ˆæ’å
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   æ¨¡å‹æ’å                                                           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let mut sorted_results = model_results.clone();
    sorted_results.sort_by(|a, b| b.overall_score.partial_cmp(&a.overall_score).unwrap_or(Ordering::Equal));

    println!("\n{:20} | {:10} | {:10} | {:10} | {:10} | {:10}",
        "æ¨¡å‹", "å“åº”ç‡%", "è´¨é‡%", "ç†è§£%", "é€Ÿåº¦ms", "ç»¼åˆ%");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    for result in sorted_results {
        let availability = if result.total_tests > 0 {
            (result.successful_responses as f64 / result.total_tests as f64) * 100.0
        } else {
            0.0
        };
        println!("{:20} | {:9.1}% | {:9.1}% | {:9.1}% | {:9.1} | {:9.1}",
            result.model_name,
            availability,
            result.response_quality_score,
            result.command_understanding_rate,
            result.avg_response_time_ms,
            result.overall_score
        );
    }

    // æ¨èå’Œå»ºè®®
    let mut recommendations = Vec::new();

    if !best_overall_model.is_empty() {
        recommendations.push(format!("æœ€ä½³ç»¼åˆæ¨¡å‹: {} (è¯„åˆ†: {:.1})", best_overall_model, best_overall_score));
    }

    if !fastest_model.is_empty() {
        recommendations.push(format!("æœ€å¿«å“åº”æ¨¡å‹: {} ({:.1}ms)", fastest_model, fastest_time));
    }

    if !best_quality_model.is_empty() {
        recommendations.push(format!("æœ€ä½³å“åº”è´¨é‡: {} (è¯„åˆ†: {:.1})", best_quality_model, best_quality_score));
    }

    if !most_reliable_model.is_empty() {
        recommendations.push(format!("æœ€é«˜å¯é æ€§: {} (æ— ç©ºå“åº”ç‡æœ€é«˜)", most_reliable_model));
    }

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   æ¨èä¸å»ºè®®                                                         â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    for (i, rec) in recommendations.iter().enumerate() {
        println!("  {}. {}", i + 1, rec);
    }

    // è®¾è®¡é—®é¢˜åˆ†æ
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   ç³»ç»Ÿè®¾è®¡é—®é¢˜åˆ†æ                                                   â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let low_reliability_models: Vec<_> = model_results.iter()
        .filter(|r| r.total_tests > 0 && {
            let empty_rate = (r.empty_responses as f64 / r.total_tests as f64) * 100.0;
            empty_rate > 20.0
        })
        .collect();

    if !low_reliability_models.is_empty() {
        println!("\nâš ï¸  é«˜ç©ºå“åº”ç‡æ¨¡å‹ (>20%):");
        for model in low_reliability_models {
            println!("   - {}: {:.1}% ç©ºå“åº”ç‡",
                model.model_name,
                (model.empty_responses as f64 / model.total_tests as f64) * 100.0
            );
        }
        println!("\nå»ºè®®: è¿™äº›æ¨¡å‹å¯èƒ½éœ€è¦è°ƒæ•´å“åº”å¤„ç†é€»è¾‘æˆ–promptç­–ç•¥");
    }

    let slow_models: Vec<_> = model_results.iter()
        .filter(|r| r.avg_response_time_ms > 5000.0)
        .collect();

    if !slow_models.is_empty() {
        println!("\nâš ï¸  å“åº”ç¼“æ…¢æ¨¡å‹ (>5000ms):");
        for model in slow_models {
            println!("   - {}: {:.1}ms", model.model_name, model.avg_response_time_ms);
        }
        println!("\nå»ºè®®: è¿™äº›æ¨¡å‹å¯èƒ½ä¸é€‚åˆå®æ—¶äº¤äº’åœºæ™¯");
    }

    let low_quality_models: Vec<_> = model_results.iter()
        .filter(|r| r.response_quality_score < 50.0)
        .collect();

    if !low_quality_models.is_empty() {
        println!("\nâš ï¸  å“åº”è´¨é‡è¾ƒä½æ¨¡å‹ (<50åˆ†):");
        for model in low_quality_models {
            println!("   - {}: {:.1}åˆ†", model.model_name, model.response_quality_score);
        }
        println!("\nå»ºè®®: è¿™äº›æ¨¡å‹å¯èƒ½éœ€è¦æ›´è¯¦ç»†çš„ç³»ç»Ÿæç¤ºè¯");
    }

    println!("\nâœ… æµ‹è¯•å®Œæˆ");

    ComprehensiveModelTestResult {
        model_results,
        best_overall_model,
        fastest_model,
        best_quality_model,
        most_reliable_model,
        recommendations,
    }
}

// ============================================================================
// æµ‹è¯•å…¥å£
// ============================================================================

#[tokio::test]
async fn test_all_models_comprehensive() {
    let _result = test_all_available_models().await;

    // éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹è¢«æµ‹è¯•
    // assert!(!result.model_results.is_empty(), "åº”è¯¥è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹è¢«æµ‹è¯•");
}
