[package]
name = "edge-ai-llm"
version.workspace = true
edition.workspace = true

[dependencies]
edge-ai-core = { path = "../core" }
edge-ai-storage = { path = "../storage" }

tokio = { workspace = true }
tokio-stream = "0.1"
futures = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
async-trait = { workspace = true }
once_cell = "1.19"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }

# HTTP client for cloud backends (optional)
reqwest = { version = "0.12", features = ["json", "stream"], optional = true }

[features]
default = ["ollama"]

# Local LLM backends (ollama requires HTTP client for local API communication)
ollama = ["reqwest"]

# Cloud backends (requires HTTP client)
cloud = ["reqwest"]
openai = ["cloud"]
anthropic = ["cloud"]
google = ["cloud"]
xai = ["cloud"]

# All backends
all = ["ollama", "cloud", "openai", "anthropic", "google", "xai"]

[dev-dependencies]
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
futures = { workspace = true }
