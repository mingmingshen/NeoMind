[package]
name = "edge-ai-llm"
version.workspace = true
edition.workspace = true

[dependencies]
edge-ai-core = { path = "../core" }
edge-ai-storage = { path = "../storage" }

tokio = { workspace = true }
tokio-stream = "0.1"
futures = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
async-trait = { workspace = true }
once_cell = "1.19"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }

# HTTP client for cloud backends (optional)
reqwest = { version = "0.12", features = ["json", "stream"], optional = true }

# Native LLM inference using candle
candle-core = { version = "0.8", optional = true }
candle-nn = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }
candle-flash-attn = { version = "0.8", optional = true }
hf-hub = { version = "0.4", optional = true }
tokenizers = { version = "0.20", optional = true }
rand = { version = "0.8", optional = true }
dirs = { version = "5.0", optional = true }

[features]
default = ["ollama"]

# Local LLM backends
ollama = ["reqwest"]
native = ["candle-core", "candle-nn", "candle-transformers", "hf-hub", "tokenizers", "rand", "dirs"]

# Cloud backends (requires HTTP client)
cloud = ["reqwest"]
openai = ["cloud"]
anthropic = ["cloud"]
google = ["cloud"]
xai = ["cloud"]

# All backends
all = ["ollama", "native", "cloud", "openai", "anthropic", "google", "xai"]

[dev-dependencies]
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
futures = { workspace = true }
