//! Test conversation flow with complex and multiple questions
//!
//! Run with: cargo test -p edge-ai-llm --test conversation_test -- --nocapture

use std::io::Write;
use std::sync::Arc;
use edge_ai_llm::{OllamaConfig, OllamaRuntime};
use edge_ai_core::{
    llm::backend::{LlmRuntime, LlmInput, GenerationParams},
    Message,
};
use futures::StreamExt;

#[tokio::test]
async fn test_complex_conversations() {
    // Initialize logging (use try_init to avoid panic if already set)
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .try_init();

    println!("\n{:=^70}", "");
    println!(" COMPLEX CONVERSATION TEST - Multiple & Long Questions");
    println!("{:=^70}\n", "");

    // Configure Ollama
    let config = OllamaConfig::new("qwen3-vl:2b")
        .with_endpoint("http://localhost:11434");

    let runtime = OllamaRuntime::new(config).expect("Failed to create runtime");
    let runtime = Arc::new(runtime);

    // Complex test cases
    let test_cases = vec![
        ("å¤šæ­¥æ¨ç†", "æˆ‘æœ‰100å…ƒï¼Œä¹°è‹¹æœèŠ±äº†15å…ƒï¼Œä¹°é¦™è•‰èŠ±äº†8å…ƒï¼Œåˆä¹°æ©˜å­èŠ±äº†12å…ƒã€‚æœ€åè¿˜å‰©å¤šå°‘é’±ï¼Ÿè¯·è¯¦ç»†åˆ—å‡ºè®¡ç®—è¿‡ç¨‹ã€‚"),

        ("å¤æ‚é€»è¾‘", "ä¸€ä¸ªå†œåœºæœ‰é¸¡å’Œå…”å­å…±50åªï¼Œå…±æœ‰140æ¡è…¿ã€‚è¯·ç”¨ä»£æ•°æ–¹æ³•åˆ—å‡ºæ–¹ç¨‹ç»„ï¼Œç„¶åè®¡ç®—é¸¡å’Œå…”å­å„æœ‰å¤šå°‘åªï¼Ÿ"),

        ("é•¿é—®é¢˜", "è¯·å¸®æˆ‘å†™ä¸€ä»½å®Œæ•´çš„å‘¨æŠ¥ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š1. æœ¬å‘¨å®Œæˆçš„ä¸»è¦å·¥ä½œï¼ˆè‡³å°‘3é¡¹ï¼‰ï¼›2. é‡åˆ°çš„é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼›3. ä¸‹å‘¨è®¡åˆ’ï¼ˆè‡³å°‘2é¡¹ï¼‰ï¼›4. éœ€è¦åè°ƒçš„äº‹é¡¹ã€‚"),

        ("å¤šé—®é¢˜", "è¯·ä¾æ¬¡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š1. åŒ—äº¬æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ2. 1+2+3+4+5ç­‰äºå¤šå°‘ï¼Ÿ3. ä»€ä¹ˆåŠ¨ç‰©è¢«ç§°ä¸ºæ£®æ—ä¹‹ç‹ï¼Ÿ"),

        ("æ•°æ®åˆ†æ", "æœ‰ä¸€ä¸ªç­çº§ï¼Œè¯­æ–‡å¹³å‡åˆ†85åˆ†ï¼Œæ•°å­¦å¹³å‡åˆ†90åˆ†ï¼Œè‹±è¯­å¹³å‡åˆ†88åˆ†ã€‚å¦‚æœä¸‰ç§‘æƒé‡åˆ†åˆ«æ˜¯30%ã€40%ã€30%ï¼Œè¯·è®¡ç®—åŠ æƒå¹³å‡åˆ†å¹¶åˆ†æå“ªä¸€ç§‘å¯¹æ€»åˆ†å½±å“æœ€å¤§ã€‚"),

        ("åœºæ™¯æ¨ç†", "å°æ˜æ—©ä¸Š8ç‚¹ä»å®¶å‡ºå‘ï¼Œæ­¥è¡Œé€Ÿåº¦æ˜¯æ¯å°æ—¶5å…¬é‡Œã€‚ä»–èµ°äº†2å°æ—¶åä¼‘æ¯30åˆ†é’Ÿï¼Œç„¶åéª‘è‡ªè¡Œè½¦è¿”å›ï¼Œéª‘è½¦é€Ÿåº¦æ˜¯æ¯å°æ—¶15å…¬é‡Œã€‚è¯·é—®å°æ˜ä»€ä¹ˆæ—¶å€™èƒ½å›åˆ°å®¶ï¼Ÿè¯·è¯¦ç»†åˆ†ææ¯ä¸ªæ—¶é—´æ®µã€‚"),
    ];

    let mut total_tests = 0;
    let mut passed_tests = 0;
    let mut failed_tests = 0;

    for (test_name, user_message) in &test_cases {
        total_tests += 1;
        println!("\n{:=^70}", "");
        println!(" [{}/{}] {}", total_tests, test_cases.len(), test_name);
        println!("{:=^70}", "");
        println!("é—®é¢˜: {}\n", user_message);

        // Show question length
        let msg_len = user_message.chars().count();
        println!("é—®é¢˜é•¿åº¦: {} å­—ç¬¦\n", msg_len);

        // Build input
        let input = LlmInput {
            messages: vec![Message::user(*user_message)],
            params: GenerationParams {
                max_tokens: Some(32768),
                temperature: Some(0.4),
                ..Default::default()
            },
            model: Some("qwen3-vl:2b".to_string()),
            stream: true,
            tools: None,
        };

        // Track metrics
        let mut thinking_chars = 0usize;
        let mut content_chars = 0usize;
        let mut chunk_count = 0usize;
        let start_time = std::time::Instant::now();

        // Stream response
        match runtime.generate_stream(input).await {
            Ok(mut stream) => {
                println!("ğŸ“¡ æ¥æ”¶æµä¸­...");

                loop {
                    match stream.next().await {
                        Some(chunk_result) => match chunk_result {
                            Ok((text, is_thinking)) => {
                                chunk_count += 1;
                                if is_thinking {
                                    thinking_chars += text.chars().count();
                                    if thinking_chars % 1000 == 0 && thinking_chars > 0 {
                                        print!("ğŸ’­({}) ", thinking_chars);
                                        std::io::stdout().flush().unwrap();
                                    }
                                } else {
                                    content_chars += text.chars().count();
                                }
                            }
                            Err(e) => {
                                println!("\nâŒ æµé”™è¯¯: {}", e);
                                break;
                            }
                        }
                        None => {
                            break;
                        }
                    }
                }

                let elapsed = start_time.elapsed();

                // Summary
                println!("\nğŸ“Š ç»Ÿè®¡ç»“æœ:");
                println!("  â±ï¸  ç”¨æ—¶: {:.2}s", elapsed.as_secs_f64());
                println!("  ğŸ“¦ æ¥æ”¶å—æ•°: {}", chunk_count);
                println!("  ğŸ’­ æ€è€ƒå­—ç¬¦: {}", thinking_chars);
                println!("  ğŸ“ å†…å®¹å­—ç¬¦: {}", content_chars);

                // Calculate ratio
                let total = thinking_chars + content_chars;
                let thinking_ratio = if total > 0 {
                    (thinking_chars as f64 / total as f64 * 100.0) as u32
                } else {
                    0
                };
                println!("  ğŸ“ˆ æ€è€ƒå æ¯”: {}% ({}% ä¸ºå†…å®¹)", thinking_ratio, 100 - thinking_ratio);

                if content_chars > 50 {
                    println!("  âœ… æµ‹è¯•é€šè¿‡");
                    passed_tests += 1;
                } else if content_chars > 0 {
                    println!("  âš ï¸  å†…å®¹è¾ƒå°‘");
                    passed_tests += 1;
                } else {
                    println!("  âŒ æµ‹è¯•å¤±è´¥: æ— å†…å®¹ç”Ÿæˆ");
                    failed_tests += 1;
                }
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
                failed_tests += 1;
            }
        }
    }

    println!("\n{:=^70}", "");
    println!(" æµ‹è¯•æ±‡æ€»");
    println!("{:=^70}", "");
    println!("  æ€»æµ‹è¯•æ•°: {}", total_tests);
    println!("  é€šè¿‡: {} âœ…", passed_tests);
    println!("  å¤±è´¥: {} âŒ", failed_tests);
    println!("  æˆåŠŸç‡: {}%", (passed_tests as f64 / total_tests as f64 * 100.0) as u32);
    println!("{:=^70}\n", "");
}

#[tokio::test]
async fn test_conversation_with_history() {
    // Initialize logging (use try_init to avoid panic if already set)
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .try_init();

    println!("\n{:=^70}", "");
    println!(" CONVERSATION WITH HISTORY TEST");
    println!("{:=^70}\n", "");

    let config = OllamaConfig::new("qwen3-vl:2b")
        .with_endpoint("http://localhost:11434");
    let runtime = Arc::new(OllamaRuntime::new(config).expect("Failed to create runtime"));

    // Simulate a multi-turn conversation
    let mut messages = vec![
        Message::user("æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´25å²ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ"),
    ];

    for (turn, user_msg) in [
        "ä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—å—ï¼Ÿ",
        "æˆ‘ä»Šå¹´å¤šå¤§ï¼Ÿ",
        "æˆ‘æ˜¯åšä»€ä¹ˆå·¥ä½œçš„ï¼Ÿ",
        "è¯·æ€»ç»“ä¸€ä¸‹æˆ‘çš„ä¿¡æ¯",
    ].iter().enumerate() {
        println!("\n{:-^70}", "");
        println!(" ç¬¬ {} è½®å¯¹è¯", turn + 1);
        println!("{:-^70}", "");
        println!("ç”¨æˆ·: {}", user_msg);

        messages.push(Message::user(*user_msg));

        let input = LlmInput {
            messages: messages.clone(),
            params: GenerationParams {
                max_tokens: Some(32768),
                temperature: Some(0.4),
                ..Default::default()
            },
            model: Some("qwen3-vl:2b".to_string()),
            stream: true,
            tools: None,
        };

        let mut thinking_chars = 0usize;
        let mut content_chars = 0usize;

        match runtime.generate_stream(input).await {
            Ok(mut stream) => {
                let mut response = String::new();
                loop {
                    match stream.next().await {
                        Some(Ok((text, is_thinking))) => {
                            if is_thinking {
                                thinking_chars += text.chars().count();
                            } else {
                                response.push_str(&text);
                                content_chars += text.chars().count();
                            }
                        }
                        Some(Err(e)) => {
                            println!("âŒ é”™è¯¯: {}", e);
                            break;
                        }
                        None => break,
                    }
                }

                // Show truncated response
                let display_response = if response.chars().count() > 200 {
                    format!("{}...", &response.chars().take(200).collect::<String>())
                } else {
                    response.clone()
                };
                println!("åŠ©æ‰‹: {}", display_response);
                println!("(æ€è€ƒ: {} å­—ç¬¦, å†…å®¹: {} å­—ç¬¦)", thinking_chars, content_chars);

                // Add assistant response to history
                messages.push(Message::assistant(&response));
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
            }
        }
    }

    println!("\n{:=^70}", "");
    println!(" å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ");
    println!("{:=^70}\n", "");
}
