//! Semantic inference for device data fields.
//!
//! This module uses a hybrid approach for semantic understanding:
//! 1. AI-powered inference (when available)
//! 2. Heuristic rule-based inference (fallback)
//! 3. Default markers (last resort)
//!
//! The system gracefully degrades if LLM is unavailable or returns invalid data.

use crate::discovery::types::*;
use crate::discovery::{StructureAnalyzer, StatisticsAnalyzer, HexAnalyzer, ValuePattern, ValueStatistics, DataType, ValueRange, InferredType};
use edge_ai_core::{LlmRuntime, Message, GenerationParams, llm::backend::LlmInput};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use tracing::trace;

/// Configuration for semantic inference
#[derive(Debug, Clone)]
pub struct InferenceConfig {
    /// Whether LLM inference is enabled
    pub llm_enabled: bool,
    /// LLM request timeout
    pub llm_timeout: Duration,
    /// Minimum confidence threshold for LLM results
    pub min_llm_confidence: f32,
    /// Whether to use heuristic fallback when LLM fails
    pub use_heuristic_fallback: bool,
    /// Maximum batch size for LLM inference
    pub max_batch_size: usize,
}

impl Default for InferenceConfig {
    fn default() -> Self {
        Self {
            llm_enabled: true,
            llm_timeout: Duration::from_secs(30),  // Increased for batch processing
            min_llm_confidence: 0.3,
            use_heuristic_fallback: true,
            max_batch_size: 20,  // Increased to handle more fields at once
        }
    }
}

/// Source of semantic inference result
#[derive(Debug, Clone, PartialEq)]
pub enum InferenceSource {
    /// Result from AI/LLM analysis
    AI,
    /// Result from heuristic rule-based analysis
    Heuristic,
    /// Result from user-provided mapping
    User,
    /// Default fallback
    Default,
}

/// Enhanced field semantic with source tracking (internal for LLM enhancement)
#[derive(Debug, Clone)]
pub struct FieldSemantic {
    /// Inferred semantic type
    pub semantic_type: SemanticType,
    /// Standardized name for the field
    pub standard_name: String,
    /// Display name (human-readable, Chinese)
    pub display_name: String,
    /// Recommended unit of measurement
    pub recommended_unit: Option<String>,
    /// Confidence score (0.0 - 1.0) - internal for LLM enhancement logic only
    pub confidence: f32,
    /// Explanation/reasoning for the inference
    pub reasoning: String,
    /// Source fields that contributed to this semantic
    pub source_fields: Vec<String>,
    /// Where this inference came from
    pub source: InferenceSource,
}

impl FieldSemantic {
    /// Create a new field semantic
    pub fn new(
        semantic_type: SemanticType,
        standard_name: String,
        display_name: String,
    ) -> Self {
        Self {
            semantic_type,
            standard_name,
            display_name,
            recommended_unit: None,
            confidence: 0.5,
            reasoning: String::new(),
            source_fields: Vec::new(),
            source: InferenceSource::Default,
        }
    }

    /// Set confidence score
    pub fn with_confidence(mut self, confidence: f32) -> Self {
        self.confidence = confidence.clamp(0.0, 1.0);
        self
    }

    /// Set reasoning
    pub fn with_reasoning(mut self, reasoning: String) -> Self {
        self.reasoning = reasoning;
        self
    }

    /// Set recommended unit
    pub fn with_unit(mut self, unit: Option<String>) -> Self {
        self.recommended_unit = unit;
        self
    }

    /// Set source field
    pub fn with_source_field(mut self, field: String) -> Self {
        self.source_fields.push(field);
        self
    }

    /// Set source
    pub fn with_source(mut self, source: InferenceSource) -> Self {
        self.source = source;
        self
    }
}

/// Enhancement data for a metric (generated by LLM)
#[derive(Debug, Clone)]
pub struct MetricEnhancement {
    /// Chinese display name for the metric
    pub display_name: String,
    /// Brief description of what the metric represents
    pub description: String,
    /// Recommended unit of measurement
    pub unit: Option<String>,
}

/// AI-powered semantic inference for device fields with robust fallback
pub struct SemanticInference {
    llm: Option<Arc<dyn LlmRuntime>>,
    config: InferenceConfig,
    structure_analyzer: StructureAnalyzer,
    stats_analyzer: StatisticsAnalyzer,
    hex_analyzer: HexAnalyzer,
}

impl SemanticInference {
    /// Create a new semantic inference engine
    pub fn new(llm: Arc<dyn LlmRuntime>) -> Self {
        Self {
            llm: Some(llm),
            config: InferenceConfig::default(),
            structure_analyzer: StructureAnalyzer::new(),
            stats_analyzer: StatisticsAnalyzer::new(),
            hex_analyzer: HexAnalyzer::new(),
        }
    }

    /// Create with optional LLM (for graceful degradation)
    pub fn with_optional_llm(llm: Option<Arc<dyn LlmRuntime>>) -> Self {
        Self {
            llm,
            config: InferenceConfig::default(),
            structure_analyzer: StructureAnalyzer::new(),
            stats_analyzer: StatisticsAnalyzer::new(),
            hex_analyzer: HexAnalyzer::new(),
        }
    }

    /// Create with custom config
    pub fn with_config(llm: Option<Arc<dyn LlmRuntime>>, config: InferenceConfig) -> Self {
        Self {
            llm,
            config,
            structure_analyzer: StructureAnalyzer::new(),
            stats_analyzer: StatisticsAnalyzer::new(),
            hex_analyzer: HexAnalyzer::new(),
        }
    }

    /// Check if LLM is available
    pub fn is_llm_available(&self) -> bool {
        self.config.llm_enabled && self.llm.is_some()
    }

    /// Infer semantic meaning for a single field with multi-tier fallback
    pub async fn infer_field_semantic(
        &self,
        field_name: &str,
        field_values: &[serde_json::Value],
        context: &InferenceContext,
    ) -> FieldSemantic {
        // Try Tier 1: AI inference (if available)
        if self.is_llm_available() {
            match tokio::time::timeout(
                self.config.llm_timeout,
                self.llm_field_inference(field_name, field_values, context)
            ).await {
                Ok(Some(result)) if result.confidence >= self.config.min_llm_confidence => {
                    return result;
                }
                Ok(Some(result)) => {
                    // LLM returned low confidence, try to enhance with heuristics
                    let heuristic = self.heuristic_inference(field_name, field_values, context);
                    if heuristic.confidence > result.confidence {
                        return heuristic;
                    }
                    return result;
                }
                Ok(None) => {
                    tracing::debug!("LLM inference returned no result for field: {}", field_name);
                }
                Err(_) => {
                    tracing::warn!("LLM inference timed out for field: {}", field_name);
                }
            }
        }

        // Tier 2: Heuristic rule-based inference
        if self.config.use_heuristic_fallback {
            let result = self.heuristic_inference(field_name, field_values, context);
            if result.semantic_type != SemanticType::Unknown {
                return result;
            }
        }

        // Tier 3: Default fallback
        self.default_fallback(field_name, field_values)
    }

    /// Infer semantics for multiple fields in batch
    pub async fn infer_fields_batch(
        &self,
        fields: &HashMap<String, Vec<serde_json::Value>>,
        context: &InferenceContext,
    ) -> HashMap<String, FieldSemantic> {
        let mut results = HashMap::new();

        if self.is_llm_available() && !fields.is_empty() {
            // Try LLM batch inference
            let batch: Vec<_> = fields.iter().take(self.config.max_batch_size).collect();

            match tokio::time::timeout(
                self.config.llm_timeout * 2,
                self.llm_batch_inference(&batch, context)
            ).await {
                Ok(llm_results) => {
                    for (field_name, values) in fields {
                        if let Some(semantic) = llm_results.get(field_name.as_str())
                            && semantic.confidence >= self.config.min_llm_confidence {
                                results.insert((*field_name).clone(), semantic.clone());
                                continue;
                            }
                        // Use heuristic for remaining fields
                        let semantic = self.heuristic_inference(field_name, values, context);
                        results.insert((*field_name).clone(), semantic);
                    }
                    return results;
                }
                Err(_) => {
                    tracing::warn!("LLM batch inference timed out, falling back to heuristics");
                }
            }
        }

        // Fallback to heuristic for all fields
        for (field_name, values) in fields {
            let semantic = self.heuristic_inference(field_name, values, context);
            results.insert(field_name.clone(), semantic);
        }

        results
    }

    /// Analyze a discovered path and enhance it with semantic information
    pub async fn enhance_path(
        &self,
        path: &DiscoveredPath,
        context: &InferenceContext,
    ) -> DiscoveredMetric {
        let field_name = Self::extract_field_name(&path.path);

        let semantic = self.infer_field_semantic(
            &field_name,
            &path.sample_values,
            context,
        ).await;

        DiscoveredMetric {
            name: semantic.standard_name.clone(),
            display_name: semantic.display_name.clone(),
            description: semantic.reasoning.clone(),
            path: path.path.clone(),
            data_type: path.data_type.clone(),
            semantic_type: semantic.semantic_type.clone(),
            unit: semantic.recommended_unit.clone(),
            value_range: path.value_range.clone(),
            is_readable: true,
            is_writable: false,
        }
    }

    /// Perform fast heuristic-only analysis (no LLM calls)
    ///
    /// This is used for initial draft creation - fast and doesn't block.
    /// LLM enhancement happens only when user approves the device.
    pub async fn analyze_samples_fast(
        &self,
        device_id: &str,
        samples: &[serde_json::Value],
    ) -> Vec<DiscoveredMetric> {
        self.analyze_samples_hybrid_with_options(device_id, samples, &InferenceContext::default(), false).await
    }

    /// Perform hybrid analysis on device samples using all analyzers
    ///
    /// This is the main entry point for the hybrid analysis pipeline:
    /// 1. Structure Analysis (fast, deterministic)
    /// 2. Statistics Analysis (value ranges, patterns)
    /// 3. Hex Analysis (hex-encoded data detection)
    /// 4. Semantic Inference (AI with fallback) - BATCHED for efficiency
    pub async fn analyze_samples_hybrid(
        &self,
        device_id: &str,
        samples: &[serde_json::Value],
        context: &InferenceContext,
    ) -> Vec<DiscoveredMetric> {
        self.analyze_samples_hybrid_with_options(device_id, samples, context, true).await
    }

    /// Internal analysis implementation with LLM toggle
    async fn analyze_samples_hybrid_with_options(
        &self,
        device_id: &str,
        samples: &[serde_json::Value],
        context: &InferenceContext,
        use_llm: bool,
    ) -> Vec<DiscoveredMetric> {
        use crate::discovery::InferredType;

        if samples.is_empty() {
            return vec![];
        }

        // Stage 1: Structure Analysis - Extract paths and data types
        let structure_result = self.structure_analyzer.analyze(samples);
        tracing::debug!(
            "Structure analysis for '{}': {} paths discovered, consistent={}",
            device_id,
            structure_result.paths.len(),
            structure_result.is_consistent
        );

        // Collect sample values by path for statistics analysis
        let mut path_values: std::collections::HashMap<String, Vec<serde_json::Value>> = std::collections::HashMap::new();

        for sample in samples {
            self.collect_path_values(&mut path_values, sample, "$");
        }

        // Identify array element paths that should be grouped
        // For example: sensors[0].name, sensors[0].value, sensors[1].name, sensors[1].value
        // should be grouped into a single "sensors" array metric
        let mut array_parent_paths: std::collections::HashSet<String> = std::collections::HashSet::new();
        let mut array_element_patterns: std::collections::HashMap<String, Vec<String>> = std::collections::HashMap::new();

        for path_info in &structure_result.paths {
            if path_info.is_array_element {
                // Extract the parent array path by removing [index] suffix
                // e.g., "sensors[0].name" -> "sensors", "data.sensors[0]" -> "data.sensors"
                let parent_path = if let Some(idx) = path_info.path.find('[') {
                    let before_bracket = &path_info.path[..idx];
                    // Remove trailing dot if present (e.g., "sensors.[0]." -> "sensors")
                    before_bracket.strip_suffix('.').unwrap_or(before_bracket).to_string()
                } else {
                    continue;
                };

                // Also check if there's more after the array access (nested object in array)
                // If the path has structure like "sensors[0].xxx", we want to treat "sensors" as the array path
                array_parent_paths.insert(parent_path.clone());
                array_element_patterns.entry(parent_path).or_default().push(path_info.path.clone());
            }
        }

        // Stage 4: Batch Semantic Inference - Single LLM call for all fields (only if use_llm is true)
        // Build a map of field_name -> values for batch processing
        // Skip array element paths (they will be grouped under parent array path)
        let mut fields_for_batch: std::collections::HashMap<String, Vec<serde_json::Value>> = std::collections::HashMap::new();
        for path_info in &structure_result.paths {
            // Skip array element paths - they will be handled as part of the parent array
            if path_info.is_array_element && array_element_patterns.values().any(|paths| paths.contains(&path_info.path)) {
                continue;
            }

            let field_name = Self::extract_field_name(&path_info.path);
            if let Some(values) = path_values.get(&path_info.path) {
                fields_for_batch.insert(field_name, values.clone());
            }
        }

        // Single LLM call for all fields (only if LLM is enabled and use_llm is true)
        let batch_semantics: std::collections::HashMap<String, FieldSemantic> = if use_llm && self.is_llm_available() && !fields_for_batch.is_empty() {
            let batch: Vec<_> = fields_for_batch.iter().collect();
            match tokio::time::timeout(
                self.config.llm_timeout * 3,  // Longer timeout for batch
                self.llm_batch_inference(&batch, context)
            ).await {
                Ok(results) => {
                    tracing::info!("LLM batch inference completed for {} fields", results.len());
                    results
                }
                Err(_) => {
                    tracing::warn!("LLM batch inference timed out, falling back to heuristics");
                    std::collections::HashMap::new()
                }
            }
        } else {
            std::collections::HashMap::new()
        };

        let mut metrics = Vec::new();

        // Stage 2 & 3 & 4: Process each path with combined analysis
        for path_info in &structure_result.paths {
            // Skip array element paths - they will be handled as part of the parent array
            if path_info.is_array_element && array_element_patterns.values().any(|paths| paths.contains(&path_info.path)) {
                continue;
            }

            // Skip Object type paths that are intermediate layers (contain other paths)
            // We only want leaf nodes with primitive values
            if matches!(path_info.data_type, InferredType::Object) {
                // Check if this object path contains other non-object paths
                // If there are paths like "data.values.xxx" and we're at "data.values",
                // then "data.values" is an intermediate layer and should be skipped
                let has_child_primitives = structure_result.paths.iter().any(|other_path| {
                    other_path.path != path_info.path
                        && other_path.path.starts_with(&format!("{}.", path_info.path))
                        && !matches!(other_path.data_type, InferredType::Object)
                        && !other_path.is_array_element
                });

                if has_child_primitives {
                    trace!(
                        "Skipping intermediate object path '{}' - has child primitive paths",
                    path_info.path
                );
                    continue;
                }
            }

            let values = path_values.get(&path_info.path)
                .map(|v| v.as_slice())
                .unwrap_or(&[]);

            if values.is_empty() {
                continue;
            }

            // Get data type from structure analysis
            let data_type = match path_info.data_type {
                InferredType::Integer => DataType::Integer,
                InferredType::Float => DataType::Float,
                InferredType::Boolean => DataType::Boolean,
                InferredType::String => DataType::String,
                InferredType::Array(_) => DataType::Array,
                InferredType::Object => DataType::Object,
                InferredType::Null => DataType::Null,
                InferredType::Mixed(_) | InferredType::Unknown => DataType::Unknown,
            };

            // Statistics Analysis
            let stats_result = self.stats_analyzer.analyze_path(&path_info.path, values);

            // Hex Analysis
            let hex_info = self.hex_analyzer.analyze_value(&path_info.path, values.first().unwrap_or(&serde_json::Value::Null));

            // Semantic Inference - use batch result or fall back to individual inference
            let field_name = Self::extract_field_name(&path_info.path);
            let semantic = if let Some(batch_result) = batch_semantics.get(&field_name) {
                // Use batch inference result if confidence is good
                if batch_result.confidence >= self.config.min_llm_confidence {
                    batch_result.clone()
                } else {
                    // Low confidence from batch, try heuristic for this specific field
                    self.heuristic_inference(&field_name, values, context)
                }
            } else if use_llm {
                // No batch result (LLM failed or timeout), use individual inference with fallback
                // ONLY do this if use_llm is true
                self.infer_field_semantic(&field_name, values, context).await
            } else {
                // Fast mode: skip LLM entirely, use heuristics directly
                self.heuristic_inference(&field_name, values, context)
            };

            // Build value range from statistics
            let value_range = if let ValueStatistics::Numeric(num) = &stats_result.stats {
                Some(ValueRange {
                    min: num.min,
                    max: num.max,
                    avg: num.mean,
                    std_dev: if num.std_dev > 0.0 { Some(num.std_dev) } else { None },
                    count: num.count,
                })
            } else {
                None
            };

            // Determine unit: combine statistics detection with semantic suggestion
            let unit = stats_result.unit
                .clone()
                .or(semantic.recommended_unit.clone());

            // Strip $. prefix from path for consistency with manual definitions
            // Handle various edge cases: "$.key", "$. key", "$.key.subkey"
            let metric_path = if path_info.path.starts_with("$.") {
                // Remove "$." or "$." (with potential space) prefix
                let rest = path_info.path[2..].trim_start();
                if rest.is_empty() {
                    &path_info.path
                } else {
                    rest
                }
            } else {
                &path_info.path
            };

            let metric = DiscoveredMetric {
                name: semantic.standard_name.clone(),
                display_name: semantic.display_name.clone(),
                description: if hex_info.is_hex {
                    format!("{} (Hex: {})", semantic.reasoning, hex_info.display_hint)
                } else {
                    semantic.reasoning.clone()
                },
                path: metric_path.to_string(),
                data_type,
                semantic_type: semantic.semantic_type.clone(),
                unit,
                value_range,
                is_readable: true,
                is_writable: false,
            };

            metrics.push(metric);
        }

        // Create array metrics for parent array paths
        // When we detect array elements like sensors[0].name, sensors[0].value, etc.
        // Create a single "sensors" array metric instead
        for parent_path in array_parent_paths {
            // Convert JSONPath format to field name (remove $. prefix)
            // Handle edge cases: "$.key", "$. key", etc.
            let field_name = if parent_path.starts_with("$.") {
                let rest = parent_path[2..].trim_start();
                if rest.is_empty() { &parent_path } else { rest }
            } else {
                &parent_path
            };

            // Collect array samples from original data
            let mut array_samples = Vec::new();
            for sample in samples.iter().take(10) {
                // Navigate to the array path in the sample
                if let Some(array_value) = Self::navigate_to_path(sample, &parent_path)
                    && array_value.is_array() {
                        array_samples.push(array_value.clone());
                    }
            }

            if !array_samples.is_empty() {
                // Infer element type from samples
                let element_type = Self::infer_array_element_type(&array_samples);

                let metric = DiscoveredMetric {
                    name: field_name.to_string(),
                    display_name: Self::to_display_name(field_name),
                    description: format!("Array of {}", element_type.display_name()),
                    path: parent_path.clone(),
                    data_type: DataType::Array,
                    semantic_type: SemanticType::Unknown,
                    unit: None,
                    value_range: None,
                    is_readable: true,
                    is_writable: false,
                };
                metrics.push(metric);
            }
        }

        // Log summary
        tracing::info!(
            "Hybrid analysis for '{}' complete: {} metrics generated",
            device_id,
            metrics.len()
        );

        metrics
    }

    /// Collect values by path from a sample (helper for hybrid analysis)
    fn collect_path_values(
        &self,
        path_values: &mut std::collections::HashMap<String, Vec<serde_json::Value>>,
        value: &serde_json::Value,
        current_path: &str,
    ) {
        match value {
            serde_json::Value::Object(map) => {
                for (key, val) in map {
                    let new_path = format!("{}.{}", current_path, key);
                    path_values.entry(new_path.clone()).or_default().push(val.clone());
                    self.collect_path_values(path_values, val, &new_path);
                }
            }
            serde_json::Value::Array(arr) => {
                for (idx, val) in arr.iter().enumerate() {
                    let new_path = format!("{}[{}]", current_path, idx);
                    path_values.entry(new_path.clone()).or_default().push(val.clone());
                    self.collect_path_values(path_values, val, &new_path);
                }
            }
            _ => {
                // Primitive value at current path - already recorded
            }
        }
    }

    /// LLM-based field inference (Tier 1)
    async fn llm_field_inference(
        &self,
        field_name: &str,
        field_values: &[serde_json::Value],
        context: &InferenceContext,
    ) -> Option<FieldSemantic> {
        let llm = self.llm.as_ref()?;

        let sample_values: String = field_values.iter()
            .take(5)
            .map(|v| v.to_string())
            .collect::<Vec<_>>()
            .join(", ");

        let device_hint = context.device_type_hint
            .as_deref()
            .unwrap_or("unknown device");

        let prompt = format!(
            "Analyze this field from an IoT device and determine its semantic meaning.\n\
            \n\
            Device Type: {}\n\
            Field Name: {}\n\
            Sample Values: {}\n\
            \n\
            Respond with a JSON object in this exact format:\n\
            {{\n\
              \"semantic_type\": \"temperature|humidity|pressure|light|motion|switch|dimmer|color|power|energy|co2|pm25|voc|speed|flow|level|status|error|alarm|battery|rssi|unknown\",\n\
              \"standard_name\": \"standardized English name (e.g., 'temperature', 'humidity')\",\n\
              \"display_name\": \"Chinese display name (e.g., '温度', '湿度')\",\n\
              \"unit\": \"recommended unit or null\",\n\
              \"confidence\": 0.0-1.0,\n\
              \"reasoning\": \"brief explanation\"\n\
            }}",
            device_hint, field_name, sample_values
        );

        let input = LlmInput {
            messages: vec![
                Message::system("You are an IoT data analyst. Analyze device fields and determine their semantic meaning. \
                              Respond ONLY with valid JSON, no additional text."),
                Message::user(prompt),
            ],
            params: GenerationParams {
                temperature: Some(0.1),
                max_tokens: Some(300),
                ..Default::default()
            },
            model: None,
            stream: false,
            tools: None,
        };

        match llm.generate(input).await {
            Ok(output) => {
                let response = output.text.trim().trim_start_matches("```json").trim_start_matches("```").trim();
                if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(response) {
                    return Some(Self::parse_llm_semantic_result(field_name, parsed, InferenceSource::AI));
                }
                tracing::warn!("Failed to parse LLM response as JSON: {}", response);
                None
            }
            Err(e) => {
                tracing::warn!("LLM generation failed for field '{}': {}", field_name, e);
                None
            }
        }
    }

    /// LLM-based batch field inference
    async fn llm_batch_inference(
        &self,
        fields: &[(&String, &Vec<serde_json::Value>)],
        context: &InferenceContext,
    ) -> HashMap<String, FieldSemantic> {
        let llm = match self.llm.as_ref() {
            Some(l) => l,
            None => return HashMap::new(),
        };
        let mut results = HashMap::new();

        let fields_desc = fields.iter()
            .map(|(name, values)| {
                let samples = values.iter()
                    .take(3)
                    .map(|v| v.to_string())
                    .collect::<Vec<_>>()
                    .join(", ");
                format!("- {}: [{}]", name, samples)
            })
            .collect::<Vec<_>>()
            .join("\n");

        let device_hint = context.device_type_hint
            .as_deref()
            .unwrap_or("unknown device");

        let prompt = format!(
            "Analyze these fields from an IoT device and determine their semantic meanings.\n\
            \n\
            Device Type: {}\n\
            Fields:\n\
            {}\n\
            \n\
            Respond with a JSON object mapping each field name to its semantic analysis:\n\
            {{\n\
              \"field_name\": {{\n\
                \"semantic_type\": \"temperature|humidity|...\",\n\
                \"standard_name\": \"standardized name\",\n\
                \"display_name\": \"Chinese name\",\n\
                \"unit\": \"unit or null\",\n\
                \"confidence\": 0.0-1.0,\n\
                \"reasoning\": \"brief explanation\"\n\
              }}\n\
            }}",
            device_hint, fields_desc
        );

        let input = LlmInput {
            messages: vec![
                Message::system("You are an IoT data analyst. Analyze device fields. Respond ONLY with valid JSON."),
                Message::user(prompt),
            ],
            params: GenerationParams {
                temperature: Some(0.1),
                max_tokens: Some(800),
                ..Default::default()
            },
            model: None,
            stream: false,
            tools: None,
        };

        if let Ok(output) = llm.generate(input).await {
            let response = output.text.trim().trim_start_matches("```json").trim_start_matches("```").trim();
            if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(response)
                && let Some(obj) = parsed.as_object() {
                    for (field_name, semantic_data) in obj {
                        let semantic = Self::parse_llm_semantic_result(field_name, semantic_data.clone(), InferenceSource::AI);
                        results.insert(field_name.clone(), semantic);
                    }
                }
        }

        results
    }

    /// Enhance discovered metrics with LLM-generated descriptions and display names
    ///
    /// This is called when user approves a device - generates human-readable
    /// descriptions, Chinese display names, and proper units for all metrics at once.
    pub async fn enhance_metrics_with_llm(
        &self,
        device_id: &str,
        device_category: &str,
        metrics: &[crate::discovery::DiscoveredMetric],
    ) -> Vec<(String, MetricEnhancement)> {
        

        let llm = match self.llm.as_ref() {
            Some(l) => l,
            None => return vec![],
        };

        if metrics.is_empty() {
            return vec![];
        }

        // Build the metrics description for LLM
        let metrics_desc = metrics.iter()
            .map(|m| {
                let value_desc = if let Some(ref range) = m.value_range {
                    format!("min={:.2}, max={:.2}, avg={:.2}", range.min, range.max, range.avg)
                } else if let DataType::Boolean = m.data_type {
                    "true/false".to_string()
                } else {
                    "various values".to_string()
                };

                format!(
                    "- {} (path: {}, data_type: {}, semantic_type: {}, values: {})",
                    m.name,
                    m.path,
                    format!("{:?}", m.data_type).to_lowercase(),
                    format!("{:?}", m.semantic_type).to_lowercase(),
                    value_desc
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        let prompt = format!(
            "You are analyzing an IoT device. Enhance the following metrics with user-friendly descriptions.\n\
            \n\
            Device ID: {}\n\
            Category: {}\n\
            \n\
            Metrics:\n\
            {}\n\
            \n\
            For each metric, provide:\n\
            - display_name: Chinese name for display (e.g., '温度' for temperature)\n\
            - description: Brief Chinese description of what this metric represents\n\
            - unit: Recommended unit (e.g., '°C', '%', 'true/false', or null if not applicable)\n\
            \n\
            Respond with JSON:\n\
            {{\n\
              \"metrics\": {{\n\
                \"metric_name\": {{\n\
                  \"display_name\": \"Chinese display name\",\n\
                  \"description\": \"Brief description in Chinese\",\n\
                  \"unit\": \"unit or null\"\n\
                }}\n\
              }}\n\
            }}",
            device_id, device_category, metrics_desc
        );

        let input = LlmInput {
            messages: vec![
                Message::system("You are an IoT expert. Generate user-friendly descriptions for device metrics. Respond ONLY with valid JSON."),
                Message::user(prompt),
            ],
            params: GenerationParams {
                temperature: Some(0.1),
                max_tokens: Some(1500),
                ..Default::default()
            },
            model: None,
            stream: false,
            tools: None,
        };

        match tokio::time::timeout(
            Duration::from_secs(45),  // Longer timeout for enhancement
            llm.generate(input)
        ).await {
            Ok(Ok(output)) => {
                let response = output.text.trim().trim_start_matches("```json").trim_start_matches("```").trim();
                if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(response)
                    && let Some(obj) = parsed.get("metrics").and_then(|v| v.as_object()) {
                        tracing::info!("LLM enhancement completed for {} metrics", obj.len());
                        let mut results = Vec::new();
                        for (metric_name, enhancement_data) in obj {
                            results.push((
                                metric_name.clone(),
                                MetricEnhancement {
                                    display_name: enhancement_data.get("display_name")
                                        .and_then(|v| v.as_str())
                                        .unwrap_or(metric_name)
                                        .to_string(),
                                    description: enhancement_data.get("description")
                                        .and_then(|v| v.as_str())
                                        .unwrap_or("")
                                        .to_string(),
                                    unit: enhancement_data.get("unit")
                                        .and_then(|v| {
                                            if v.is_null() { None } else { v.as_str() }
                                        })
                                        .map(|s| s.to_string()),
                                }
                            ));
                        }
                        return results;
                    }
                tracing::warn!("Failed to parse LLM enhancement response");
                vec![]
            }
            Ok(Err(e)) => {
                tracing::warn!("LLM enhancement generation failed: {}", e);
                vec![]
            }
            Err(_) => {
                tracing::warn!("LLM enhancement timed out");
                vec![]
            }
        }
    }

    /// Heuristic rule-based inference (Tier 2)
    fn heuristic_inference(
        &self,
        field_name: &str,
        field_values: &[serde_json::Value],
        _context: &InferenceContext,
    ) -> FieldSemantic {
        let _name_lower = field_name.to_lowercase();

        // First try rule-based semantic type
        let first_value = field_values.first().cloned();
        let semantic_type = SemanticType::infer_from_context(field_name, &first_value);

        // Use statistics analyzer to detect patterns
        let stats_result = self.stats_analyzer.analyze_path(field_name, field_values);

        // Use hex analyzer for hex-like values
        let hex_info = self.hex_analyzer.analyze_value(field_name, field_values.first().unwrap_or(&serde_json::Value::Null));

        // Determine confidence based on match quality
        let mut confidence = 0.5;
        let mut reasoning = format!("Heuristic inference based on field name '{}'", field_name);

        // Boost confidence if multiple indicators agree
        if semantic_type != SemanticType::Unknown {
            confidence += 0.2;

            // Check if statistics pattern matches semantic type
            let matches_pattern = matches!(
                (&semantic_type, &stats_result.pattern),
                (SemanticType::Temperature, ValuePattern::TemperatureCelsius | ValuePattern::TemperatureFahrenheit)
                    | (SemanticType::Humidity, ValuePattern::Percentage)
                    | (SemanticType::Battery, ValuePattern::Percentage)
                    | (SemanticType::Switch, ValuePattern::BooleanLike)
            );

            if matches_pattern {
                confidence += 0.15;
                reasoning.push_str(&format!(" + pattern match ({:?})", stats_result.pattern));
            }
        }

        // Hex handling
        if hex_info.is_hex
            && let Some(decoded) = hex_info.decoded_integer {
                reasoning = format!("Hex value detected, decoded as {}", decoded);
            }

        // Build result
        let standard_name = Self::standardize_name(field_name);
        // Use field_name directly as display_name (JSON key)
        let display_name = field_name.to_string();
        let recommended_unit = stats_result.unit.clone().or_else(|| {
            semantic_type.default_unit().map(|u| u.to_string())
        });

        FieldSemantic {
            semantic_type,
            standard_name,
            display_name,
            recommended_unit,
            confidence: (confidence as f32).clamp(0.0, 1.0),
            reasoning,
            source_fields: vec![field_name.to_string()],
            source: InferenceSource::Heuristic,
        }
    }

    /// Default fallback (Tier 3)
    fn default_fallback(&self, field_name: &str, field_values: &[serde_json::Value]) -> FieldSemantic {
        let first_value = field_values.first();
        let data_type_hint = match first_value {
            Some(serde_json::Value::Number(n)) if n.is_i64() => "integer",
            Some(serde_json::Value::Number(_)) => "float",
            Some(serde_json::Value::String(_)) => "string",
            Some(serde_json::Value::Bool(_)) => "boolean",
            _ => "value",
        };

        let standard_name = Self::standardize_name(field_name);
        let display_name = field_name.to_string();

        FieldSemantic {
            semantic_type: SemanticType::Unknown,
            standard_name: format!("{}_{}", data_type_hint, standard_name),
            display_name,
            recommended_unit: None,
            confidence: 0.2,
            reasoning: format!("Default fallback - could not infer semantic meaning for '{}'", field_name),
            source_fields: vec![field_name.to_string()],
            source: InferenceSource::Default,
        }
    }

    /// Parse LLM semantic inference result
    fn parse_llm_semantic_result(field_name: &str, value: serde_json::Value, source: InferenceSource) -> FieldSemantic {
        let semantic_type_str = value.get("semantic_type")
            .and_then(|v| v.as_str())
            .unwrap_or("unknown");

        let semantic_type = match semantic_type_str {
            "temperature" => SemanticType::Temperature,
            "humidity" => SemanticType::Humidity,
            "pressure" => SemanticType::Pressure,
            "light" => SemanticType::Light,
            "motion" => SemanticType::Motion,
            "switch" => SemanticType::Switch,
            "dimmer" => SemanticType::Dimmer,
            "color" => SemanticType::Color,
            "power" => SemanticType::Power,
            "energy" => SemanticType::Energy,
            "co2" => SemanticType::Co2,
            "pm25" => SemanticType::Pm25,
            "voc" => SemanticType::Voc,
            "speed" => SemanticType::Speed,
            "flow" => SemanticType::Flow,
            "level" => SemanticType::Level,
            "status" => SemanticType::Status,
            "error" => SemanticType::Error,
            "alarm" => SemanticType::Alarm,
            "battery" => SemanticType::Battery,
            "rssi" => SemanticType::Rssi,
            _ => SemanticType::Unknown,
        };

        let standard_name = value.get("standard_name")
            .and_then(|v| v.as_str())
            .unwrap_or(&Self::standardize_name(field_name))
            .to_string();

        let display_name = value.get("display_name")
            .and_then(|v| v.as_str())
            .unwrap_or(field_name)
            .to_string();

        let confidence = value.get("confidence")
            .and_then(|v| v.as_f64())
            .unwrap_or(0.5) as f32;

        let reasoning = value.get("reasoning")
            .and_then(|v| v.as_str())
            .unwrap_or("AI inference")
            .to_string();

        let recommended_unit = value.get("unit")
            .and_then(|v| {
                if v.is_null() { None } else { v.as_str().map(|s| s.to_string()) }
            });

        FieldSemantic {
            semantic_type,
            standard_name,
            display_name,
            recommended_unit,
            confidence,
            reasoning,
            source_fields: vec![field_name.to_string()],
            source,
        }
    }

    /// Extract field name from a JSON path
    fn extract_field_name(path: &str) -> String {
        let parts: Vec<&str> = path.split('.').collect();
        let last = parts.last().unwrap_or(&path);

        last.split('[')
            .next()
            .unwrap_or(last)
            .to_string()
    }

    /// Standardize a field name
    fn standardize_name(name: &str) -> String {
        name.to_lowercase()
            .replace([' ', '-', '_'], "_")
            .trim_matches('_')
            .to_string()
    }

    /// Convert field name to display name (Title Case)
    fn to_display_name(name: &str) -> String {
        name.split(['_', '-'])
            .map(|word| {
                let mut chars = word.chars();
                match chars.next() {
                    None => String::new(),
                    Some(first) => first.to_uppercase().collect::<String>() + chars.as_str(),
                }
            })
            .collect::<Vec<_>>()
            .join(" ")
    }

    /// Navigate to a path in a JSON value
    fn navigate_to_path(value: &serde_json::Value, path: &str) -> Option<serde_json::Value> {
        // Strip $. prefix with whitespace handling
        let path = if path.starts_with("$.") {
            let rest = path[2..].trim_start();
            if rest.is_empty() { path } else { rest }
        } else {
            path
        };
        let parts: Vec<&str> = path.split('.').collect();

        let mut current = value;
        for part in parts {
            if part.is_empty() {
                continue;
            }
            // Handle array indexing like sensors[0]
            if let Some(bracket_idx) = part.find('[') {
                let field_name = &part[..bracket_idx];
                let rest = &part[bracket_idx..];

                // First navigate to the field
                if let Some(obj) = current.as_object() {
                    current = obj.get(field_name)?;
                } else {
                    return None;
                }

                // Then handle array indices
                let mut current_arr = current;
                for bracket in rest.split(']') {
                    if bracket.is_empty() || bracket.starts_with('[') {
                        continue;
                    }
                    let idx_str = bracket.trim_start_matches('[');
                    if let Ok(idx) = idx_str.parse::<usize>() {
                        if let Some(arr) = current_arr.as_array() {
                            current_arr = arr.get(idx)?;
                        } else {
                            return None;
                        }
                    }
                }
                current = current_arr;
            } else {
                // Simple field access
                if let Some(obj) = current.as_object() {
                    current = obj.get(part)?;
                } else {
                    return None;
                }
            }
        }
        Some(current.clone())
    }

    /// Infer the element type of an array from samples
    fn infer_array_element_type(samples: &[serde_json::Value]) -> InferredType {
        use serde_json::Value;

        let mut has_numbers = false;
        let mut has_strings = false;
        let mut has_bools = false;
        let mut has_objects = false;
        let mut has_arrays = false;
        let mut total = 0;

        for sample in samples {
            if let Some(arr) = sample.as_array() {
                for elem in arr {
                    total += 1;
                    match elem {
                        Value::Number(_) => has_numbers = true,
                        Value::String(_) => has_strings = true,
                        Value::Bool(_) => has_bools = true,
                        Value::Object(_) => has_objects = true,
                        Value::Array(_) => has_arrays = true,
                        Value::Null => {}
                    }
                }
            }
        }

        if total == 0 {
            return InferredType::Array(Box::new(InferredType::Unknown));
        }

        // Determine dominant type
        let _num_count = if has_numbers { total } else { 0 };
        let _str_count = if has_strings { total } else { 0 };
        let _bool_count = if has_bools { total } else { 0 };
        let _obj_count = if has_objects { total } else { 0 };
        let _arr_count = if has_arrays { total } else { 0 };

        // Simple heuristic: count unique types seen
        let type_count = [has_numbers, has_strings, has_bools, has_objects, has_arrays]
            .iter()
            .filter(|&&x| x)
            .count();

        if type_count == 1 {
            if has_objects {
                InferredType::Array(Box::new(InferredType::Object))
            } else if has_arrays {
                InferredType::Array(Box::new(InferredType::Array(Box::new(InferredType::Unknown))))
            } else if has_numbers {
                // Could be integer or float, use number for now
                InferredType::Array(Box::new(InferredType::Float))
            } else if has_strings {
                InferredType::Array(Box::new(InferredType::String))
            } else if has_bools {
                InferredType::Array(Box::new(InferredType::Boolean))
            } else {
                InferredType::Array(Box::new(InferredType::Unknown))
            }
        } else if type_count > 1 {
            // Mixed array - use unknown
            InferredType::Array(Box::new(InferredType::Unknown))
        } else {
            InferredType::Array(Box::new(InferredType::Unknown))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_extract_field_name() {
        assert_eq!(
            SemanticInference::extract_field_name("payload.sensors[0].v"),
            "v"
        );
        assert_eq!(
            SemanticInference::extract_field_name("temperature"),
            "temperature"
        );
        assert_eq!(
            SemanticInference::extract_field_name("data.values[0]"),
            "values"
        );
    }

    #[test]
    fn test_standardize_name() {
        assert_eq!(
            SemanticInference::standardize_name("Temperature Sensor"),
            "temperature_sensor"
        );
        assert_eq!(
            SemanticInference::standardize_name("humidity-value"),
            "humidity_value"
        );
        assert_eq!(
            SemanticInference::standardize_name("  device_status  "),
            "device_status"
        );
    }

    #[test]
    fn test_parse_llm_semantic_result() {
        let json = serde_json::json!({
            "semantic_type": "temperature",
            "standard_name": "temperature",
            "display_name": "温度",
            "unit": "°C",
            "confidence": 0.9,
            "reasoning": "Field name contains 'temp'"
        });

        let result = SemanticInference::parse_llm_semantic_result("temp_celsius", json, InferenceSource::AI);
        assert_eq!(result.semantic_type, SemanticType::Temperature);
        assert_eq!(result.standard_name, "temperature");
        assert_eq!(result.display_name, "温度");
        assert_eq!(result.recommended_unit, Some("°C".to_string()));
        assert_eq!(result.confidence, 0.9);
        assert_eq!(result.source, InferenceSource::AI);
    }

    #[test]
    fn test_inference_config_default() {
        let config = InferenceConfig::default();
        assert!(config.llm_enabled);
        assert!(config.use_heuristic_fallback);
        assert_eq!(config.min_llm_confidence, 0.3);
    }
}
