//! Full Integration Test with Real LLM, Metrics, Commands, and Notifications
//!
//! This test verifies:
//! 1. Agent can understand real metrics with LLM
//! 2. Agent can generate and execute commands
//! 3. Agent can send notifications/alerts
//! 4. End-to-end workflow with real data flow

use std::sync::Arc;
use std::time::{Duration, Instant};
use neomind_core::{EventBus, MetricValue, NeoTalkEvent, LlmRuntime, message::{Message, MessageRole, Content}};
use neomind_storage::{
    AgentStore, AgentSchedule, AgentStats, AgentStatus, AiAgent, AgentMemory,
    WorkingMemory, ShortTermMemory, LongTermMemory, ScheduleType, ResourceType, AgentResource,
    TimeSeriesStore, DataPoint,
};
use neomind_agent::ai_agent::{AgentExecutor, AgentExecutorConfig};
use neomind_llm::backends::ollama::{OllamaRuntime, OllamaConfig};
use neomind_messages::{MessageManager, MessageSeverity, channels::ConsoleChannel};
use neomind_core::llm::backend::{LlmInput, GenerationParams};

// ============================================================================
// Test Context with All Components
// ============================================================================

struct FullTestContext {
    pub store: Arc<AgentStore>,
    pub executor: AgentExecutor,
    pub event_bus: Arc<EventBus>,
    pub llm_runtime: Arc<OllamaRuntime>,
    pub time_series: Arc<TimeSeriesStore>,
    pub message_manager: Arc<MessageManager>,
}

impl FullTestContext {
    async fn new() -> anyhow::Result<Self> {
        let store = AgentStore::memory()?;
        let event_bus = Arc::new(EventBus::new());

        // Create LLM runtime
        let ollama_config = OllamaConfig {
            endpoint: "http://localhost:11434".to_string(),
            model: "qwen2.5:3b".to_string(),
            timeout_secs: 120,
        };
        let llm_runtime = Arc::new(OllamaRuntime::new(ollama_config)?);

        // Create time series storage (memory-based for testing)
        let time_series = TimeSeriesStore::memory()?;

        // Create message manager with console channel
        let message_manager = Arc::new(MessageManager::new());
        let console_channel = Arc::new(ConsoleChannel::new("console".to_string()));
        // Note: MessageManager now initializes with default channels via register_default_channels
        message_manager.register_default_channels().await;

        // Note: We can't create device_service here without the full devices crate
        // So commands will be simulated
        let executor_config = AgentExecutorConfig {
            store: store.clone(),
            time_series_storage: Some(time_series.clone()),
            device_service: None,
            event_bus: Some(event_bus.clone()),
            message_manager: Some(message_manager.clone()),
            llm_runtime: Some(llm_runtime.clone() as Arc<dyn neomind_core::llm::backend::LlmRuntime + Send + Sync>),
            llm_backend_store: None,
        };

        let executor = AgentExecutor::new(executor_config).await?;

        Ok(Self {
            store,
            executor,
            event_bus,
            llm_runtime,
            time_series,
            message_manager,
        })
    }

    /// Simulate metric data for a device
    async fn inject_metric_data(&self, device_id: &str, metric: &str, value: f64) -> anyhow::Result<()> {
        let timestamp = chrono::Utc::now().timestamp_millis();

        // Store in time series
        let point = DataPoint {
            timestamp,
            value: serde_json::json!(value),
            quality: Some(1.0),
            metadata: None,
        };

        self.time_series.write(
            device_id,
            metric,
            point
        ).await?;

        // Also publish to event bus
        let event = NeoTalkEvent::DeviceMetric {
            device_id: device_id.to_string(),
            metric: metric.to_string(),
            value: MetricValue::Float(value),
            timestamp: chrono::Utc::now().timestamp(),
            quality: Some(1.0),
        };
        let _ = self.event_bus.publish(event).await;

        Ok(())
    }

    /// Inject multiple historical data points
    async fn inject_historical_data(
        &self,
        device_id: &str,
        metric: &str,
        values: Vec<(i64, f64)>,
    ) -> anyhow::Result<()> {
        let mut points = Vec::new();
        for (ts, v) in &values {
            points.push(DataPoint {
                timestamp: ts * 1000,
                value: serde_json::json!(*v),
                quality: Some(1.0),
                metadata: None,
            });
        }

        self.time_series.write_batch(device_id, metric, points).await?;
        Ok(())
    }

    /// Create an agent with metric resources
    async fn create_monitoring_agent(
        &self,
        name: &str,
        metrics: Vec<(String, String)>,
        user_prompt: &str,
    ) -> anyhow::Result<AiAgent> {
        let now = chrono::Utc::now().timestamp();

        let mut resources = Vec::new();
        for (device_id, metric_name) in &metrics {
            resources.push(AgentResource {
                resource_type: ResourceType::Metric,
                resource_id: format!("{}:{}", device_id, metric_name),
                name: format!("{} - {}", device_id, metric_name),
                config: serde_json::json!({}),
            });
        }

        let agent = AiAgent {
            id: uuid::Uuid::new_v4().to_string(),
            name: name.to_string(),
            description: None,
            user_prompt: user_prompt.to_string(),
            llm_backend_id: None,
            parsed_intent: None,
            resources,
            schedule: AgentSchedule {
                schedule_type: ScheduleType::Interval,
                interval_seconds: Some(60),
                cron_expression: None,
                timezone: None,
                event_filter: None,
            },
            status: AgentStatus::Active,
            created_at: now,
            updated_at: now,
            last_execution_at: None,
            stats: AgentStats {
                total_executions: 0,
                successful_executions: 0,
                failed_executions: 0,
                avg_duration_ms: 0,
                last_duration_ms: Some(0),
            },
            memory: AgentMemory {
                state_variables: Default::default(),
                baselines: Default::default(),
                learned_patterns: vec![],
                trend_data: vec![],
                updated_at: now,
                working: WorkingMemory::default(),
                short_term: ShortTermMemory::default(),
                long_term: LongTermMemory::default(),
            },
            error_message: None,
            priority: 128,
            conversation_history: vec![],
            user_messages: vec![],
            conversation_summary: None,
            context_window_size: 10,
        };

        self.store.save_agent(&agent).await?;
        Ok(agent)
    }

    /// Create an executor agent with command resources
    async fn create_executor_agent(
        &self,
        name: &str,
        commands: Vec<(String, String, serde_json::Value)>,
        user_prompt: &str,
    ) -> anyhow::Result<AiAgent> {
        let now = chrono::Utc::now().timestamp();

        let mut resources = Vec::new();
        for (device_id, command, params) in &commands {
            resources.push(AgentResource {
                resource_type: ResourceType::Command,
                resource_id: format!("{}:{}", device_id, command),
                name: format!("{} - {}", device_id, command),
                config: serde_json::json!({
                    "parameters": params
                }),
            });
        }

        // Also add metric resources for monitoring
        for (device_id, _command, _params) in &commands {
            resources.push(AgentResource {
                resource_type: ResourceType::Metric,
                resource_id: format!("{}:temperature", device_id),
                name: format!("{} - temperature", device_id),
                config: serde_json::json!({}),
            });
        }

        let agent = AiAgent {
            id: uuid::Uuid::new_v4().to_string(),
            name: name.to_string(),
            description: None,
            user_prompt: user_prompt.to_string(),
            llm_backend_id: None,
            parsed_intent: None,
            resources,
            schedule: AgentSchedule {
                schedule_type: ScheduleType::Interval,
                interval_seconds: Some(60),
                cron_expression: None,
                timezone: None,
                event_filter: None,
            },
            status: AgentStatus::Active,
            created_at: now,
            updated_at: now,
            last_execution_at: None,
            stats: AgentStats {
                total_executions: 0,
                successful_executions: 0,
                failed_executions: 0,
                avg_duration_ms: 0,
                last_duration_ms: Some(0),
            },
            memory: AgentMemory {
                state_variables: Default::default(),
                baselines: Default::default(),
                learned_patterns: vec![],
                trend_data: vec![],
                updated_at: now,
                working: WorkingMemory::default(),
                short_term: ShortTermMemory::default(),
                long_term: LongTermMemory::default(),
            },
            error_message: None,
            priority: 128,
            conversation_history: vec![],
            user_messages: vec![],
            conversation_summary: None,
            context_window_size: 10,
        };

        self.store.save_agent(&agent).await?;
        Ok(agent)
    }

    /// Send a test notification
    async fn send_notification(&self, severity: MessageSeverity, message: &str) -> anyhow::Result<()> {
        let msg = neomind_messages::Message::alert(
            severity,
            "Agent Notification".to_string(),
            message.to_string(),
            "test_agent".to_string(),
        );
        let msg = self.message_manager.create_message(msg).await?;

        println!("  ğŸ“¢ é€šçŸ¥å‘é€: {} - {}", severity, message);
        println!("     Message ID: {}", msg.id);

        Ok(())
    }

    /// Query LLM directly
    async fn query_llm(&self, system_prompt: &str, user_message: &str) -> anyhow::Result<String> {
        let messages = vec![
            Message::new(MessageRole::System, Content::text(system_prompt)),
            Message::new(MessageRole::User, Content::text(user_message)),
        ];

        let input = LlmInput {
            messages,
            params: GenerationParams {
                temperature: Some(0.7),
                max_tokens: Some(500),
                ..Default::default()
            },
            model: Some("qwen2.5:3b".to_string()),
            stream: false,
            tools: None,
        };

        let output = self.llm_runtime.generate(input).await?;
        Ok(output.text)
    }
}

// ============================================================================
// Helper: Check Ollama availability
// ============================================================================

fn ollama_available() -> bool {
    let addr = std::net::SocketAddr::from(([127, 0, 0, 1], 11434));
    std::net::TcpStream::connect_timeout(&addr, Duration::from_secs(2)).is_ok()
}

// ============================================================================
// Integration Tests
// ============================================================================

#[tokio::test]
#[ignore = "Requires Ollama LLM backend"]
async fn test_llm_understands_metrics() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = FullTestContext::new().await?;

    println!("\n=== æµ‹è¯•: LLM ç†è§£æŒ‡æ ‡æ•°æ® ===\n");

    // Inject some temperature data
    let device_id = "sensor_temp_01";
    println!("1. æ³¨å…¥æ¨¡æ‹Ÿæ¸©åº¦æ•°æ®...");
    for i in 0..10 {
        let temp = 20.0 + (i as f64 * 2.0); // 20, 22, 24, 26, 28, 30, 32, 34, 36, 38
        ctx.inject_metric_data(device_id, "temperature", temp).await?;
        println!("   {}â„ƒ @ t-{}min", temp, (10 - i) * 5);
        tokio::time::sleep(Duration::from_millis(10)).await;
    }

    // Create agent with this metric
    let agent = ctx.create_monitoring_agent(
        "æ¸©åº¦åˆ†æAgent",
        vec![(device_id.to_string(), "temperature".to_string())],
        "åˆ†ææ¸©åº¦ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ£€æµ‹å¼‚å¸¸å€¼å’Œè¶‹åŠ¿",
    ).await?;

    println!("\n2. æ‰§è¡ŒAgentåˆ†æ...");
    let agent = ctx.store.get_agent(&agent.id).await?.unwrap();
    let record = ctx.executor.execute_agent(agent.clone()).await?;

    println!("\n3. Agentå“åº”:");
    println!("   çŠ¶æ€: {:?}", record.status);
    println!("   æƒ…å†µåˆ†æ: {}", record.decision_process.situation_analysis);
    println!("   ç»“è®º: {}", record.decision_process.conclusion);

    // Verify data was collected
    println!("\n4. æ”¶é›†çš„æ•°æ®:");
    for data in &record.decision_process.data_collected {
        println!("   - {}: {}", data.source, data.data_type);
        if let Some(value) = data.values.get("value") {
            println!("     æœ€æ–°å€¼: {}", value);
        }
        if let Some(count) = data.values.get("points_count") {
            println!("     æ•°æ®ç‚¹æ•°: {}", count);
        }
    }

    assert!(!record.decision_process.data_collected.is_empty(), "åº”è¯¥æ”¶é›†åˆ°æ•°æ®");

    println!("\nâœ… LLMç†è§£æŒ‡æ ‡æµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}

#[tokio::test]
#[ignore = "Requires Ollama LLM backend"]
async fn test_llm_generates_commands() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = FullTestContext::new().await?;

    println!("\n=== æµ‹è¯•: LLM ç”Ÿæˆæ§åˆ¶æŒ‡ä»¤ ===\n");

    // Setup: Inject temperature data that exceeds threshold
    let device_id = "hvac_controller";
    println!("1. æ³¨å…¥é«˜æ¸©æ•°æ®...");

    for temp in &[25.0, 28.0, 31.0, 33.0, 35.0, 32.0] {
        ctx.inject_metric_data(device_id, "temperature", *temp).await?;
        println!("   æ³¨å…¥: {}â„ƒ", temp);
        tokio::time::sleep(Duration::from_millis(10)).await;
    }

    // Create executor agent with command resource
    let agent = ctx.create_executor_agent(
        "ç©ºè°ƒæ§åˆ¶Agent",
        vec![(device_id.to_string(), "turn_on".to_string(), serde_json::json!({"mode": "cool"}))],
        "ç›‘æ§æ¸©åº¦ï¼Œå½“æ¸©åº¦è¶…è¿‡30åº¦æ—¶å¼€å¯ç©ºè°ƒï¼Œä½äº25åº¦æ—¶å…³é—­ç©ºè°ƒ",
    ).await?;

    println!("\n2. æ‰§è¡ŒAgentå†³ç­–...");

    let agent = ctx.store.get_agent(&agent.id).await?.unwrap();
    let record = ctx.executor.execute_agent(agent.clone()).await?;

    println!("\n3. å†³ç­–åˆ†æ:");
    println!("   çŠ¶æ€: {:?}", record.status);
    println!("   æƒ…å†µåˆ†æ: {}", record.decision_process.situation_analysis);

    println!("\n   æ¨ç†æ­¥éª¤:");
    for (i, step) in record.decision_process.reasoning_steps.iter().enumerate() {
        println!("     æ­¥éª¤{}: {}", i + 1, step.description);
        println!("       è¾“å…¥: {:?}", step.input);
        println!("       è¾“å‡º: {:?}", step.output);
    }

    println!("\n   å†³ç­–:");
    for (i, decision) in record.decision_process.decisions.iter().enumerate() {
        println!("     å†³ç­–{}: {}", i + 1, decision.decision_type);
        println!("       æè¿°: {}", decision.description);
        println!("       åŠ¨ä½œ: {}", decision.action);
    }

    println!("\n   ç»“è®º: {}", record.decision_process.conclusion);

    // Check execution result
    if let Some(ref result) = record.result {
        println!("\n   æ‰§è¡Œç»“æœ:");
        for action in &result.actions_executed {
            println!("     - åŠ¨ä½œ: {} ({})", action.action_type,
                if action.success { "æˆåŠŸ" } else { "å¤±è´¥" });
        }
    }

    println!("\nâœ… LLMç”ŸæˆæŒ‡ä»¤æµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}

#[tokio::test]
#[ignore = "Requires Ollama LLM backend"]
async fn test_notification_sending() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = FullTestContext::new().await?;

    println!("\n=== æµ‹è¯•: é€šçŸ¥å‘é€ ===\n");

    println!("1. æµ‹è¯•ä¸åŒä¸¥é‡çº§åˆ«çš„é€šçŸ¥...");

    ctx.send_notification(MessageSeverity::Info, "è¿™æ˜¯ä¿¡æ¯çº§åˆ«é€šçŸ¥").await?;
    ctx.send_notification(MessageSeverity::Warning, "è¿™æ˜¯è­¦å‘Šçº§åˆ«é€šçŸ¥").await?;
    ctx.send_notification(MessageSeverity::Critical, "è¿™æ˜¯ä¸¥é‡çº§åˆ«é€šçŸ¥").await?;

    println!("\n2. æµ‹è¯•Agentè§¦å‘çš„é€šçŸ¥...");

    // Create a monitoring agent that should alert on high temperature
    let device_id = "furnace_sensor";
    ctx.inject_metric_data(device_id, "temperature", 85.0).await?;

    let agent = ctx.create_monitoring_agent(
        "ç†”ç‚‰ç›‘æ§Agent",
        vec![(device_id.to_string(), "temperature".to_string())],
        "ç›‘æ§ç†”ç‚‰æ¸©åº¦ï¼Œè¶…è¿‡80åº¦å‘é€ä¸¥é‡å‘Šè­¦",
    ).await?;

    println!("\n3. æ‰§è¡ŒAgent...");
    let agent = ctx.store.get_agent(&agent.id).await?.unwrap();
    let record = ctx.executor.execute_agent(agent.clone()).await?;

    println!("\n4. Agentå“åº”:");
    println!("   åˆ†æ: {}", record.decision_process.situation_analysis);

    // Simulate sending alert based on agent's decision
    if !record.decision_process.decisions.is_empty() {
        println!("\n5. æ¨¡æ‹Ÿå‘é€å‘Šè­¦é€šçŸ¥...");
        ctx.send_notification(MessageSeverity::Critical, &format!(
            "Agent {} æ£€æµ‹åˆ°å¼‚å¸¸: {}",
            agent.name,
            record.decision_process.conclusion
        )).await?;
    }

    println!("\nâœ… é€šçŸ¥å‘é€æµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}

#[tokio::test]
#[ignore = "Requires Ollama LLM backend"]
async fn test_conversation_with_llm() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = FullTestContext::new().await?;

    println!("\n=== æµ‹è¯•: ä¸LLMçš„å¯¹è¯ä¸Šä¸‹æ–‡ ===\n");

    // Direct LLM conversation test
    println!("1. ç›´æ¥æŸ¥è¯¢LLM - æŒ‡æ ‡ç†è§£èƒ½åŠ›...");
    let response1 = ctx.query_llm(
        "ä½ æ˜¯ä¸€ä¸ªç‰©è”ç½‘ç›‘æ§åŠ©æ‰‹ã€‚",
        "å½“å‰æ¸©åº¦ä¸º35åº¦ï¼Œæ¹¿åº¦ä¸º60%ï¼Œè¯·åˆ†æè¿™ä¸ªç¯å¢ƒçŠ¶æ€ã€‚"
    ).await?;

    println!("   LLMå“åº”: {}", response1);

    println!("\n2. æµ‹è¯•æŒ‡ä»¤ç”Ÿæˆèƒ½åŠ›...");
    let response2 = ctx.query_llm(
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¶å±…æ§åˆ¶åŠ©æ‰‹ã€‚",
        "å®¢å…æ¸©åº¦ç°åœ¨æ˜¯32åº¦ï¼Œè¶…è¿‡äº†28åº¦çš„é˜ˆå€¼ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿè¯·ç”¨JSONæ ¼å¼å›å¤ï¼š{\"action\": \"xxx\", \"device\": \"xxx\", \"reason\": \"xxx\"}"
    ).await?;

    println!("   LLMå“åº”: {}", response2);

    println!("\n3. æµ‹è¯•å¼‚å¸¸æ£€æµ‹èƒ½åŠ›...");
    let response3 = ctx.query_llm(
        "ä½ æ˜¯ä¸€ä¸ªè®¾å¤‡ç›‘æ§åŠ©æ‰‹ã€‚",
        "æ¸©åº¦ä¼ æ„Ÿå™¨åœ¨è¿‡å»1å°æ—¶å†…ä»25åº¦é€æ¸ä¸Šå‡åˆ°42åº¦ï¼Œè¿™æ˜¯å¦å¼‚å¸¸ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ"
    ).await?;

    println!("   LLMå“åº”: {}", response3);

    println!("\nâœ… LLMå¯¹è¯æµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}

#[tokio::test]
#[ignore = "Requires Ollama LLM backend"]
async fn test_end_to_end_workflow() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = FullTestContext::new().await?;

    println!("\n=== ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯• ===\n");

    let device_id = "smart_home_temp";
    let agent_name = "æ™ºèƒ½å®¶å±…ç®¡å®¶";
    let prompt = "ä½ æ˜¯æ™ºèƒ½å®¶å±…ç®¡å®¶ï¼Œè´Ÿè´£ï¼š
1. ç›‘æ§å®¤å†…æ¸©åº¦ï¼ˆæ­£å¸¸èŒƒå›´ï¼š20-26åº¦ï¼‰
2. æ¸©åº¦è¶…è¿‡26åº¦æ—¶ï¼Œå»ºè®®å¼€å¯ç©ºè°ƒ
3. æ¸©åº¦ä½äº20åº¦æ—¶ï¼Œå»ºè®®å¼€å¯æš–æ°”
4. å¼‚å¸¸æƒ…å†µï¼ˆè¶…è¿‡30åº¦æˆ–ä½äº15åº¦ï¼‰æ—¶å‘é€å‘Šè­¦";

    println!("åœºæ™¯: æ¸©åº¦é€æ¸ä¸Šå‡ï¼Œæµ‹è¯•Agentå“åº”\n");

    // Round 1: Normal temperature
    println!("--- åœºæ™¯1: æ­£å¸¸æ¸©åº¦ (22åº¦) ---");
    ctx.inject_metric_data(device_id, "temperature", 22.0).await?;

    let agent = ctx.create_monitoring_agent(
        agent_name,
        vec![(device_id.to_string(), "temperature".to_string())],
        prompt,
    ).await?;

    let agent_id = agent.id.clone();

    let agent = ctx.store.get_agent(&agent_id).await?.unwrap();
    let record1 = ctx.executor.execute_agent(agent.clone()).await?;
    println!("åˆ†æ: {}", record1.decision_process.situation_analysis);
    println!("ç»“è®º: {}", record1.decision_process.conclusion);

    // Round 2: Slightly high
    println!("\n--- åœºæ™¯2: ç¨é«˜æ¸©åº¦ (27åº¦) ---");
    ctx.inject_metric_data(device_id, "temperature", 27.0).await?;
    tokio::time::sleep(Duration::from_millis(100)).await;

    let agent = ctx.store.get_agent(&agent_id).await?.unwrap();
    let record2 = ctx.executor.execute_agent(agent.clone()).await?;
    println!("åˆ†æ: {}", record2.decision_process.situation_analysis);
    println!("ç»“è®º: {}", record2.decision_process.conclusion);

    // Round 3: Too high - should alert
    println!("\n--- åœºæ™¯3: è¿‡é«˜æ¸©åº¦ (31åº¦) ---");
    ctx.inject_metric_data(device_id, "temperature", 31.0).await?;
    tokio::time::sleep(Duration::from_millis(100)).await;

    let agent = ctx.store.get_agent(&agent_id).await?.unwrap();
    let record3 = ctx.executor.execute_agent(agent.clone()).await?;
    println!("åˆ†æ: {}", record3.decision_process.situation_analysis);

    // Check if alert should be sent
    if record3.decision_process.conclusion.contains("å¼‚å¸¸") ||
       record3.decision_process.conclusion.contains("å‘Šè­¦") ||
       record3.decision_process.conclusion.contains("é«˜") {
        println!("âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸æ¡ä»¶ï¼");
        ctx.send_notification(MessageSeverity::Warning, &record3.decision_process.conclusion).await?;
    }

    // Verify conversation history
    println!("\n--- å¯¹è¯å†å² ---");
    let agent = ctx.store.get_agent(&agent_id).await?.unwrap();
    println!("æ€»è½®æ¬¡: {}", agent.conversation_history.len());

    for (i, turn) in agent.conversation_history.iter().enumerate() {
        println!("è½®æ¬¡{}:", i + 1);
        println!("  æ—¶é—´: {}", chrono::DateTime::from_timestamp(turn.timestamp, 0)
            .map(|dt| dt.format("%H:%M:%S").to_string())
            .unwrap_or_else(|| "?".to_string()));
        println!("  æ•°æ®ç‚¹: {}", turn.input.data_collected.len());
        println!("  æˆåŠŸ: {}", turn.success);
    }

    println!("\nâœ… ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}

#[tokio::test]
async fn test_data_collection_and_context() -> anyhow::Result<()> {
    let ctx = FullTestContext::new().await?;

    println!("\n=== æµ‹è¯•: æ•°æ®æ”¶é›†å’Œä¸Šä¸‹æ–‡æ„å»º ===\n");

    // Create multiple devices with different metrics
    let devices = vec![
        ("temp_sensor_01", "temperature"),
        ("temp_sensor_02", "temperature"),
        ("humidity_sensor", "humidity"),
        ("energy_meter", "power"),
    ];

    println!("1. åˆ›å»ºå¤šæŒ‡æ ‡Agent...");
    let mut metrics = Vec::new();
    for (id, metric) in &devices {
        metrics.push((id.to_string(), metric.to_string()));
    }

    let agent = ctx.create_monitoring_agent(
        "å¤šä¼ æ„Ÿå™¨ç›‘æ§Agent",
        metrics,
        "ç›‘æ§æ‰€æœ‰ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š",
    ).await?;

    println!("2. æ³¨å…¥æ¨¡æ‹Ÿæ•°æ®...");
    for (device_id, metric) in &devices {
        let value = rand::random::<f64>() * 50.0 + 10.0; // 10-60 range
        ctx.inject_metric_data(device_id, metric, value).await?;
        println!("   {}: {} = {:.1}", device_id, metric, value);
        tokio::time::sleep(Duration::from_millis(10)).await;
    }

    println!("\n3. æ‰§è¡ŒAgent...");
    let agent = ctx.store.get_agent(&agent.id).await?.unwrap();
    let start = Instant::now();
    let record = ctx.executor.execute_agent(agent.clone()).await?;
    let elapsed = start.elapsed();

    println!("\n4. æ‰§è¡Œç»“æœ:");
    println!("   è€—æ—¶: {:?}", elapsed);
    println!("   æ•°æ®æ”¶é›†: {} ä¸ªæ•°æ®æº", record.decision_process.data_collected.len());

    println!("\n   æ”¶é›†åˆ°çš„æ•°æ®:");
    for data in &record.decision_process.data_collected {
        if let Some(val) = data.values.get("value") {
            println!("     - {}: {:.1}", data.source, val);
        }
    }

    println!("\nâœ… æ•°æ®æ”¶é›†æµ‹è¯•é€šè¿‡ï¼");
    Ok(())
}
