//! Real Tool Calling Evaluation Test (Updated with actual tools)
//!
//! This test evaluates tool calling with REAL LLM and REAL tool execution.
//! Tests are aligned with the actual available tools in the system.
//!
//! Run with:
//!   cargo test -p neomind-agent --test tool_calling_evaluation_real -- --ignored --nocapture

use std::sync::Arc;
use std::time::{Duration, Instant};

use neomind_agent::session::SessionManager;
use neomind_llm::{OllamaConfig, OllamaRuntime};

// ============================================================================
// Test Cases - Aligned with actual available tools
// ============================================================================

/// Actual available tools in the system:
///
/// **Device Tools:**
///   - device_discover - åˆ—å‡ºæ‰€æœ‰è®¾å¤‡
///   - get_device_data - è·å–è®¾å¤‡å½“å‰æ‰€æœ‰æ•°æ®
///   - query_data - æŸ¥è¯¢è®¾å¤‡å†å²æ•°æ®
///   - device_control - æ§åˆ¶è®¾å¤‡
///   - device_analyze - åˆ†æè®¾å¤‡æ•°æ®è¶‹åŠ¿
///
/// **Rule Tools:**
///   - list_rules - åˆ—å‡ºæ‰€æœ‰è‡ªåŠ¨åŒ–è§„åˆ™
///   - create_rule - åˆ›å»ºè‡ªåŠ¨åŒ–è§„åˆ™
///   - delete_rule - åˆ é™¤è§„åˆ™
///
/// **Agent Tools:**
///   - list_agents - åˆ—å‡ºæ‰€æœ‰AI Agent
///   - get_agent - è·å–Agentè¯¦ç»†ä¿¡æ¯
///   - execute_agent - æ‰§è¡ŒAgent
///   - control_agent - æ§åˆ¶Agentï¼ˆæš‚åœ/æ¢å¤/åˆ é™¤ï¼‰
///   - create_agent - åˆ›å»ºAgent
///   - agent_memory - æŸ¥è¯¢Agentè®°å¿†
///   - get_agent_executions - è·å–Agentæ‰§è¡Œå†å²
///   - get_agent_execution_detail - è·å–å•æ¬¡æ‰§è¡Œè¯¦æƒ…
///   - get_agent_conversation - è·å–Agentå¯¹è¯å†å²
///
/// **System Tools:**
///   - system_help - è·å–ç³»ç»Ÿå¸®åŠ©

#[derive(Debug, Clone)]
struct TestCase {
    name: String,
    query: String,
    expected_tools: Vec<String>,
    min_tools: usize,
    description: String,
}

fn get_test_cases() -> Vec<TestCase> {
    vec![
        // === Device Tools Tests ===
        TestCase {
            name: "è®¾å¤‡å‘ç°".to_string(),
            query: "åˆ—å‡ºæ‰€æœ‰è®¾å¤‡".to_string(),
            expected_tools: vec!["device_discover".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è®¾å¤‡å‘ç°å·¥å…·".to_string(),
        },
        TestCase {
            name: "è®¾å¤‡åˆ—è¡¨æŸ¥è¯¢".to_string(),
            query: "æŸ¥çœ‹æœ‰å“ªäº›è®¾å¤‡".to_string(),
            expected_tools: vec!["device_discover".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è®¾å¤‡å‘ç°å·¥å…·".to_string(),
        },
        TestCase {
            name: "è®¾å¤‡æ•°æ®æŸ¥è¯¢".to_string(),
            query: "æŸ¥è¯¢è®¾å¤‡ abc123 çš„å½“å‰æ•°æ®".to_string(),
            expected_tools: vec!["get_device_data".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è·å–è®¾å¤‡æ•°æ®å·¥å…·".to_string(),
        },
        TestCase {
            name: "è®¾å¤‡å†å²æ•°æ®".to_string(),
            query: "æŸ¥çœ‹è®¾å¤‡ abc123 çš„ç”µæ± ç”µé‡å†å²è¶‹åŠ¿".to_string(),
            expected_tools: vec!["query_data".to_string(), "get_device_data".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨æ•°æ®æŸ¥è¯¢å·¥å…·".to_string(),
        },
        // === Rule Tools Tests ===
        TestCase {
            name: "è§„åˆ™åˆ—è¡¨".to_string(),
            query: "åˆ—å‡ºæ‰€æœ‰è‡ªåŠ¨åŒ–è§„åˆ™".to_string(),
            expected_tools: vec!["list_rules".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è§„åˆ™åˆ—è¡¨å·¥å…·".to_string(),
        },
        TestCase {
            name: "æŸ¥çœ‹è§„åˆ™".to_string(),
            query: "æ˜¾ç¤ºæ‰€æœ‰è§„åˆ™".to_string(),
            expected_tools: vec!["list_rules".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è§„åˆ™åˆ—è¡¨å·¥å…·".to_string(),
        },
        // === Agent Tools Tests ===
        TestCase {
            name: "Agentåˆ—è¡¨".to_string(),
            query: "åˆ—å‡ºæ‰€æœ‰AI Agent".to_string(),
            expected_tools: vec!["list_agents".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨Agentåˆ—è¡¨å·¥å…·".to_string(),
        },
        TestCase {
            name: "Agentè¯¦æƒ…".to_string(),
            query: "æŸ¥çœ‹Agent agent_1çš„è¯¦ç»†ä¿¡æ¯".to_string(),
            expected_tools: vec!["get_agent".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è·å–Agentè¯¦æƒ…å·¥å…·".to_string(),
        },
        TestCase {
            name: "Agentæ‰§è¡Œå†å²".to_string(),
            query: "æŸ¥çœ‹Agent agent_1çš„æ‰§è¡Œå†å²".to_string(),
            expected_tools: vec!["get_agent_executions".to_string(), "get_agent".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨Agentæ‰§è¡Œå†å²å·¥å…·".to_string(),
        },
        // === Multi-Tool Tests ===
        TestCase {
            name: "è®¾å¤‡+è§„åˆ™".to_string(),
            query: "è¯·åŒæ—¶åˆ—å‡ºæ‰€æœ‰è®¾å¤‡å’Œæ‰€æœ‰è‡ªåŠ¨åŒ–è§„åˆ™".to_string(),
            expected_tools: vec!["device_discover".to_string(), "list_rules".to_string()],
            min_tools: 2,
            description: "åº”è¯¥åŒæ—¶è°ƒç”¨è®¾å¤‡å’Œè§„åˆ™å·¥å…·".to_string(),
        },
        TestCase {
            name: "è®¾å¤‡å‘ç°+æ•°æ®".to_string(),
            query: "å‘ç°è®¾å¤‡åæŸ¥çœ‹ç¬¬ä¸€ä¸ªè®¾å¤‡çš„è¯¦ç»†æ•°æ®".to_string(),
            expected_tools: vec!["device_discover".to_string(), "get_device_data".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è®¾å¤‡å‘ç°å·¥å…·".to_string(),
        },
        TestCase {
            name: "å…¨é¢æŸ¥è¯¢".to_string(),
            query: "æˆ‘éœ€è¦æŸ¥çœ‹ï¼šæ‰€æœ‰è®¾å¤‡ã€æ‰€æœ‰è§„åˆ™ã€æ‰€æœ‰Agent".to_string(),
            expected_tools: vec![
                "device_discover".to_string(),
                "list_rules".to_string(),
                "list_agents".to_string(),
            ],
            min_tools: 2,
            description: "åº”è¯¥è°ƒç”¨å¤šä¸ªå·¥å…·".to_string(),
        },
        // === Context/Reference Tests ===
        TestCase {
            name: "ç³»ç»Ÿå¸®åŠ©".to_string(),
            query: "è¿™ä¸ªç³»ç»Ÿèƒ½åšä»€ä¹ˆï¼Ÿ".to_string(),
            expected_tools: vec!["system_help".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨ç³»ç»Ÿå¸®åŠ©å·¥å…·".to_string(),
        },
        // === Control Tests ===
        TestCase {
            name: "è®¾å¤‡æ§åˆ¶".to_string(),
            query: "æ‰“å¼€è®¾å¤‡ light_001".to_string(),
            expected_tools: vec!["device_control".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨è®¾å¤‡æ§åˆ¶å·¥å…·".to_string(),
        },
        TestCase {
            name: "åˆ›å»ºè§„åˆ™".to_string(),
            query: "åˆ›å»ºä¸€ä¸ªæ¸©åº¦è¶…è¿‡30åº¦æ—¶å‘Šè­¦çš„è§„åˆ™".to_string(),
            expected_tools: vec!["create_rule".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨åˆ›å»ºè§„åˆ™å·¥å…·".to_string(),
        },
        TestCase {
            name: "åˆ›å»ºAgent".to_string(),
            query: "åˆ›å»ºä¸€ä¸ªç›‘æ§æ¸©åº¦çš„Agent".to_string(),
            expected_tools: vec!["create_agent".to_string()],
            min_tools: 1,
            description: "åº”è¯¥è°ƒç”¨åˆ›å»ºAgentå·¥å…·".to_string(),
        },
    ]
}

// ============================================================================
// Evaluation Metrics
// ============================================================================

#[derive(Debug, Default)]
struct EvaluationMetrics {
    total_tests: usize,
    tests_with_tool_calls: usize,
    total_expected_tool_calls: usize,
    total_actual_tool_calls: usize,
    correct_tool_calls: usize,
    multi_tool_requests: usize,
    successful_multi_tools: usize,
    empty_responses: usize,
    total_time_ms: u128,
    tool_execution_errors: usize,
}

impl EvaluationMetrics {
    fn precision(&self) -> f64 {
        if self.total_actual_tool_calls == 0 {
            0.0
        } else {
            self.correct_tool_calls as f64 / self.total_actual_tool_calls as f64
        }
    }

    fn recall(&self) -> f64 {
        if self.total_expected_tool_calls == 0 {
            0.0
        } else {
            self.correct_tool_calls as f64 / self.total_expected_tool_calls as f64
        }
    }

    fn f1_score(&self) -> f64 {
        let p = self.precision();
        let r = self.recall();
        if p + r == 0.0 {
            0.0
        } else {
            2.0 * p * r / (p + r)
        }
    }

    fn multi_tool_success_rate(&self) -> f64 {
        if self.multi_tool_requests == 0 {
            0.0
        } else {
            self.successful_multi_tools as f64 / self.multi_tool_requests as f64
        }
    }

    fn tool_call_detection_rate(&self) -> f64 {
        if self.total_tests == 0 {
            0.0
        } else {
            self.tests_with_tool_calls as f64 / self.total_tests as f64
        }
    }

    fn average_time_ms(&self) -> f64 {
        if self.total_tests == 0 {
            0.0
        } else {
            self.total_time_ms as f64 / self.total_tests as f64
        }
    }
}

// ============================================================================
// Test Runner
// ============================================================================

async fn run_test_case(
    session_manager: &SessionManager,
    session_id: &str,
    test_case: &TestCase,
) -> anyhow::Result<TestResult> {
    let start = Instant::now();

    // Use REAL agent with REAL tool execution
    let response = session_manager
        .process_message(session_id, &test_case.query)
        .await?;

    let elapsed = start.elapsed();

    // Check if tools were called
    let tool_calls = &response.tool_calls;
    let called_tool_names: Vec<String> = tool_calls.iter().map(|t| t.name.clone()).collect();

    // Check if expected tools were called (partial match)
    let mut correct_calls = 0;
    for expected in &test_case.expected_tools {
        if called_tool_names.iter().any(|name| name.contains(expected)) {
            correct_calls += 1;
        }
    }

    // Check for tool execution errors
    let tool_errors: Vec<String> = tool_calls
        .iter()
        .filter_map(|t| {
            if let Some(result) = &t.result {
                if result.get("success").and_then(|s| s.as_bool()) == Some(false) {
                    Some(
                        result
                            .get("error")
                            .and_then(|e| e.as_str())
                            .unwrap_or("Unknown error")
                            .to_string(),
                    )
                } else {
                    None
                }
            } else {
                None
            }
        })
        .collect();

    Ok(TestResult {
        test_name: test_case.name.clone(),
        query: test_case.query.clone(),
        response_content: response.message.content.clone(),
        tool_calls: called_tool_names.clone(),
        expected_tools: test_case.expected_tools.clone(),
        min_tools: test_case.min_tools,
        correct_calls,
        total_expected: test_case.expected_tools.len(),
        total_actual: tool_calls.len(),
        elapsed_ms: elapsed.as_millis(),
        has_tool_calls: !tool_calls.is_empty(),
        meets_minimum: tool_calls.len() >= test_case.min_tools,
        is_multi_tool: test_case.min_tools > 1,
        multi_tool_success: test_case.min_tools > 1 && tool_calls.len() >= test_case.min_tools,
        tool_errors,
        tools_used: response.tools_used,
    })
}

#[derive(Debug)]
struct TestResult {
    test_name: String,
    query: String,
    response_content: String,
    tool_calls: Vec<String>,
    expected_tools: Vec<String>,
    min_tools: usize,
    correct_calls: usize,
    total_expected: usize,
    total_actual: usize,
    elapsed_ms: u128,
    has_tool_calls: bool,
    meets_minimum: bool,
    is_multi_tool: bool,
    multi_tool_success: bool,
    tool_errors: Vec<String>,
    tools_used: Vec<String>,
}

// ============================================================================
// Output Formatting
// ============================================================================

fn print_metrics(metrics: &EvaluationMetrics) {
    println!("\n{}", "=".repeat(70));
    println!("ğŸ“Š REAL TOOL CALLING EVALUATION RESULTS (Updated Tools)");
    println!("{}", "=".repeat(70));

    println!("\nğŸ“ˆ Core Metrics:");
    println!(
        "  Precision (ç²¾ç¡®åº¦):   {:.1}%",
        metrics.precision() * 100.0
    );
    println!("  Recall (å¬å›ç‡):      {:.1}%", metrics.recall() * 100.0);
    println!("  F1 Score:             {:.1}%", metrics.f1_score() * 100.0);

    println!("\nğŸ”§ Tool Call Detection:");
    println!(
        "  Detection Rate:       {:.1}%",
        metrics.tool_call_detection_rate() * 100.0
    );
    println!(
        "  Total Expected:       {}",
        metrics.total_expected_tool_calls
    );
    println!(
        "  Total Actual:         {}",
        metrics.total_actual_tool_calls
    );
    println!("  Correct Matches:      {}", metrics.correct_tool_calls);
    println!("  Tool Errors:          {}", metrics.tool_execution_errors);

    println!("\nğŸ¯ Multi-Tool Performance:");
    println!("  Multi-Tool Requests:  {}", metrics.multi_tool_requests);
    println!("  Successful:          {}", metrics.successful_multi_tools);
    println!(
        "  Success Rate:         {:.1}%",
        metrics.multi_tool_success_rate() * 100.0
    );

    println!("\nâ±ï¸  Performance:");
    println!("  Average Response:     {:.0}ms", metrics.average_time_ms());
    println!("  Total Time:           {}ms", metrics.total_time_ms);

    println!("\n{}", "=".repeat(70));
}

fn print_detailed_results(results: &[TestResult]) {
    println!("\n{}", "-".repeat(70));
    println!("ğŸ“‹ DETAILED TEST RESULTS");
    println!("{}", "-".repeat(70));

    for (i, result) in results.iter().enumerate() {
        println!(
            "\n[{}] {} - {}ms",
            i + 1,
            result.test_name,
            result.elapsed_ms
        );
        println!("    Query: {}", result.query);
        println!("    Tools called: {:?}", result.tool_calls);
        println!("    Expected patterns: {:?}", result.expected_tools);
        println!(
            "    Correct matches: {}/{}",
            result.correct_calls, result.total_expected
        );

        if !result.tool_errors.is_empty() {
            println!("    âš ï¸  Tool errors: {:?}", result.tool_errors);
        }

        if result.has_tool_calls {
            println!("    âœ… Tools executed");
        } else {
            println!("    âŒ No tools called");
        }

        if result.is_multi_tool {
            if result.multi_tool_success {
                println!("    âœ… Multi-tool successful");
            } else {
                println!(
                    "    âš ï¸  Multi-tool partial (expected >= {}, got {})",
                    result.min_tools, result.total_actual
                );
            }
        }

        // Response preview
        let preview: String = result.response_content.chars().take(150).collect();
        println!(
            "    Response: {}{}",
            preview,
            if result.response_content.len() > 150 {
                "..."
            } else {
                ""
            }
        );
    }
}

// ============================================================================
// Test
// ============================================================================

#[tokio::test]
#[ignore = "Requires Ollama LLM backend. Run with: cargo test --test tool_calling_evaluation_real -- --ignored --nocapture"]
async fn test_real_tool_calling_evaluation() -> anyhow::Result<()> {
    // Check Ollama availability
    let addr = std::net::SocketAddr::from(([127, 0, 0, 1], 11434));
    if std::net::TcpStream::connect_timeout(&addr, Duration::from_secs(2)).is_err() {
        println!("âš ï¸  Ollama not available, skipping test");
        return Ok(());
    }

    let model_name = std::env::var("MODEL").unwrap_or_else(|_| "qwen2.5:3b".to_string());

    let ollama_endpoint =
        std::env::var("OLLAMA_ENDPOINT").unwrap_or_else(|_| "http://localhost:11434".to_string());

    println!("\nğŸš€ Real Tool Calling Evaluation (Updated Tool List)");
    println!("ğŸ“¦ Model: {}", model_name);
    println!("ğŸ”— Endpoint: {}", ollama_endpoint);

    // Create session manager with REAL tools
    let session_manager = SessionManager::memory();
    let session_id = session_manager.create_session().await?;

    // Configure LLM
    let ollama_config = OllamaConfig {
        endpoint: ollama_endpoint,
        model: model_name.clone(),
        timeout_secs: 120,
    };

    let llm_runtime = Arc::new(OllamaRuntime::new(ollama_config.clone())?);
    let agent = session_manager.get_session(&session_id).await?;
    agent.set_custom_llm(llm_runtime).await;

    println!("âœ… Session created: {}", session_id);
    println!("ğŸ”§ Tools: REAL (device_discover, list_rules, list_agents, etc.)");

    println!("\nğŸ“‹ Available Tools:");
    println!(
        "  Device: device_discover, get_device_data, query_data, device_control, device_analyze"
    );
    println!("  Rule:   list_rules, create_rule, delete_rule");
    println!("  Agent:  list_agents, get_agent, execute_agent, control_agent, create_agent, etc.");
    println!("  System: system_help");

    let test_cases = get_test_cases();
    println!("\nğŸ“‹ Test cases: {}", test_cases.len());

    let mut metrics = EvaluationMetrics::default();
    let mut results = Vec::new();

    for test_case in &test_cases {
        metrics.total_tests += 1;
        metrics.total_expected_tool_calls += test_case.expected_tools.len();

        if test_case.min_tools > 1 {
            metrics.multi_tool_requests += 1;
        }

        print!("\n[Testing] {}...", test_case.name);

        // Create a new session for each test to avoid cached results
        let test_session_id = session_manager.create_session().await?;
        let llm_runtime = Arc::new(OllamaRuntime::new(ollama_config.clone())?);
        let agent = session_manager.get_session(&test_session_id).await?;
        agent.set_custom_llm(llm_runtime).await;

        match run_test_case(&session_manager, &test_session_id, test_case).await {
            Ok(result) => {
                println!(
                    " âœ“ ({}ms, {} tools)",
                    result.elapsed_ms, result.total_actual
                );

                metrics.total_actual_tool_calls += result.total_actual;
                metrics.correct_tool_calls += result.correct_calls;
                metrics.total_time_ms += result.elapsed_ms;

                if result.has_tool_calls {
                    metrics.tests_with_tool_calls += 1;
                }

                if result.multi_tool_success {
                    metrics.successful_multi_tools += 1;
                }

                if result.response_content.is_empty() {
                    metrics.empty_responses += 1;
                }

                metrics.tool_execution_errors += result.tool_errors.len();

                results.push(result);
            }
            Err(e) => {
                println!(" âœ— Error: {}", e);
            }
        }
    }

    // Print results
    print_detailed_results(&results);
    print_metrics(&metrics);

    // Validation
    println!("\nğŸ¯ Validation:");
    let detection_rate = metrics.tool_call_detection_rate();
    let precision = metrics.precision();
    let multi_tool_rate = metrics.multi_tool_success_rate();

    if detection_rate >= 0.8 {
        println!("  âœ… Detection rate >= 80%: {:.1}%", detection_rate * 100.0);
    } else {
        println!("  âš ï¸  Detection rate < 80%: {:.1}%", detection_rate * 100.0);
    }

    if precision >= 0.6 {
        println!("  âœ… Precision >= 60%: {:.1}%", precision * 100.0);
    } else {
        println!("  âš ï¸  Precision < 60%: {:.1}%", precision * 100.0);
    }

    if multi_tool_rate >= 0.5 {
        println!(
            "  âœ… Multi-tool rate >= 50%: {:.1}%",
            multi_tool_rate * 100.0
        );
    } else {
        println!(
            "  âš ï¸  Multi-tool rate < 50%: {:.1}%",
            multi_tool_rate * 100.0
        );
    }

    if metrics.tool_execution_errors == 0 {
        println!("  âœ… No tool execution errors");
    } else {
        println!(
            "  âš ï¸  Tool execution errors: {}",
            metrics.tool_execution_errors
        );
    }

    let overall_score =
        (metrics.precision() + metrics.recall() + metrics.multi_tool_success_rate()) / 3.0;
    println!("\nğŸ† Overall Score: {:.1}%", overall_score * 100.0);

    if overall_score >= 0.65 {
        println!("âœ… EVALUATION PASSED");
    } else {
        println!("âš ï¸  EVALUATION NEEDS IMPROVEMENT");
    }

    Ok(())
}
