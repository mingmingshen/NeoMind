//! çœŸå®LLMæ€§èƒ½æµ‹è¯•
//!
//! è¿™ä¸ªæµ‹è¯•**çœŸæ­£è°ƒç”¨Ollama LLM**æ¥è¡¡é‡å®é™…æ€§èƒ½

use neomind_agent::ai_agent::{AgentExecutor, AgentExecutorConfig};
use neomind_core::llm::backend::{GenerationParams, LlmInput};
use neomind_core::{
    EventBus, LlmRuntime, MetricValue, NeoMindEvent,
    message::{Content, Message, MessageRole},
};
use neomind_llm::backends::ollama::{OllamaConfig, OllamaRuntime};
use neomind_storage::{
    AgentMemory, AgentResource, AgentSchedule, AgentStats, AgentStatus, AgentStore, AiAgent,
    DataPoint, LongTermMemory, ResourceType, ScheduleType, ShortTermMemory, TimeSeriesStore,
    WorkingMemory,
};
use std::sync::Arc;
use std::time::{Duration, Instant};

struct RealPerfTestContext {
    pub store: Arc<AgentStore>,
    pub event_bus: Arc<EventBus>,
    pub llm_runtime: Arc<OllamaRuntime>,
    pub time_series: Arc<TimeSeriesStore>,
    pub executor: AgentExecutor,
}

impl RealPerfTestContext {
    async fn new() -> anyhow::Result<Self> {
        let store = AgentStore::memory()?;
        let event_bus = Arc::new(EventBus::new());

        let ollama_config = OllamaConfig {
            endpoint: "http://localhost:11434".to_string(),
            model: "qwen2.5:0.5b".to_string(),
            timeout_secs: 120,
        };
        let llm_runtime = Arc::new(OllamaRuntime::new(ollama_config)?);

        let time_series = TimeSeriesStore::memory()?;

        let executor_config = AgentExecutorConfig {
            store: store.clone(),
            time_series_storage: Some(time_series.clone()),
            device_service: None,
            event_bus: Some(event_bus.clone()),
            message_manager: None,
            llm_runtime: Some(llm_runtime.clone()),
            llm_backend_store: None,
            extension_registry: None,
        };
        let executor = AgentExecutor::new(executor_config).await?;

        Ok(Self {
            store,
            event_bus,
            llm_runtime,
            time_series,
            executor,
        })
    }

    /// ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ - çœŸå®æ€§èƒ½
    async fn llm_analyze(&self, system_prompt: &str, user_input: &str) -> (String, u128) {
        let messages = vec![
            Message::new(MessageRole::System, Content::text(system_prompt)),
            Message::new(MessageRole::User, Content::text(user_input)),
        ];

        let input = LlmInput {
            messages,
            params: GenerationParams {
                temperature: Some(0.7),
                max_tokens: Some(500),
                ..Default::default()
            },
            model: Some("qwen2.5:3b".to_string()),
            stream: false,
            tools: None,
        };

        let start = Instant::now();
        let output = self.llm_runtime.generate(input).await.unwrap_or_else(|e| {
            neomind_core::llm::backend::LlmOutput {
                text: format!("Error: {}", e),
                finish_reason: neomind_core::llm::backend::FinishReason::Error,
                usage: None,
                thinking: None,
            }
        });
        let elapsed = start.elapsed().as_millis();

        (output.text, elapsed)
    }

    async fn inject_metrics(&self, device_id: &str, metric: &str, values: &[f64]) {
        for &value in values {
            let point = DataPoint {
                timestamp: chrono::Utc::now().timestamp_millis(),
                value: serde_json::json!(value),
                quality: Some(1.0),
                metadata: None,
            };
            self.time_series.write(device_id, metric, point).await.ok();
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }

    /// Create a test agent with the given parameters
    async fn create_test_agent(&self, name: &str, user_prompt: &str) -> anyhow::Result<AiAgent> {
        let now = chrono::Utc::now().timestamp();

        let agent = AiAgent {
            id: uuid::Uuid::new_v4().to_string(),
            name: name.to_string(),
            description: None,
            user_prompt: user_prompt.to_string(),
            llm_backend_id: None,
            parsed_intent: None,
            resources: vec![],
            schedule: AgentSchedule {
                schedule_type: ScheduleType::Interval,
                interval_seconds: Some(60),
                cron_expression: None,
                timezone: None,
                event_filter: None,
            },
            status: AgentStatus::Active,
            created_at: now,
            updated_at: now,
            last_execution_at: None,
            stats: AgentStats {
                total_executions: 0,
                successful_executions: 0,
                failed_executions: 0,
                avg_duration_ms: 0,
                last_duration_ms: Some(0),
            },
            memory: AgentMemory {
                state_variables: Default::default(),
                baselines: Default::default(),
                learned_patterns: vec![],
                trend_data: vec![],
                updated_at: now,
                working: WorkingMemory::default(),
                short_term: ShortTermMemory::default(),
                long_term: LongTermMemory::default(),
            },
            error_message: None,
            priority: 128,
            conversation_history: vec![],
            user_messages: vec![],
            conversation_summary: None,
            context_window_size: 10,
            enable_tool_chaining: false,
            max_chain_depth: 3,
        };

        self.store.save_agent(&agent).await?;
        Ok(agent)
    }
}

fn ollama_available() -> bool {
    let addr = std::net::SocketAddr::from(([127, 0, 0, 1], 11434));
    std::net::TcpStream::connect_timeout(&addr, Duration::from_secs(2)).is_ok()
}

#[tokio::test]
#[ignore = "Requires real LLM calls"]
async fn test_real_llm_performance() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = RealPerfTestContext::new().await?;

    println!(
        "\n{}",
        "============================================================"
    );
    println!("çœŸå®LLMæ€§èƒ½æµ‹è¯• - æ¯æ¬¡è°ƒç”¨éƒ½å®é™…ç­‰å¾…LLMå“åº”");
    println!(
        "{}\n",
        "============================================================"
    );

    let system_prompt = "ä½ æ˜¯ä¸€ä¸ªç‰©è”ç½‘è®¾å¤‡ç›‘æ§åŠ©æ‰‹ã€‚åˆ†ææ•°æ®å¹¶ç»™å‡ºå»ºè®®ã€‚";

    // æµ‹è¯•1: ç®€å•æŸ¥è¯¢
    println!("ğŸ“Š æµ‹è¯•1: ç®€å•æ¸©åº¦æ•°æ®åˆ†æ");
    let user_input = "å½“å‰æ¸©åº¦ä¸º28åº¦ï¼Œæ¹¿åº¦ä¸º60%ï¼Œè¯·ç®€è¦åˆ†æè¿™ä¸ªç¯å¢ƒçŠ¶æ€ã€‚";

    let start = Instant::now();
    let (response, elapsed) = ctx.llm_analyze(system_prompt, user_input).await;
    println!("   å“åº”æ—¶é—´: {}ms", elapsed);
    println!("   LLMå“åº”: {}", response);
    println!();

    // æµ‹è¯•2: å¤æ‚æŸ¥è¯¢ - å¤šè®¾å¤‡åˆ†æ
    println!("ğŸ“Š æµ‹è¯•2: å¤šè®¾å¤‡æ•°æ®åˆ†æ");
    let multi_device_input = r#"
æˆ‘æœ‰ä»¥ä¸‹ä¼ æ„Ÿå™¨æ•°æ®ï¼š
- åŠå…¬å®¤A: æ¸©åº¦26Â°Cï¼Œæ¹¿åº¦55%
- åŠå…¬å®¤B: æ¸©åº¦29Â°Cï¼Œæ¹¿åº¦65%
- æœåŠ¡å™¨æœºæˆ¿: æ¸©åº¦24Â°Cï¼Œæ¹¿åº¦45%
- å¤§å…: æ¸©åº¦27Â°Cï¼Œæ¹¿åº¦60%

è¯·åˆ†æï¼š
1. å“ªäº›åŒºåŸŸéœ€è¦å…³æ³¨ï¼Ÿ
2. æ˜¯å¦æœ‰å¼‚å¸¸æƒ…å†µï¼Ÿ
3. ç»™å‡ºå…·ä½“å»ºè®®ã€‚
"#;

    let (response2, elapsed2) = ctx.llm_analyze(system_prompt, multi_device_input).await;
    println!("   å“åº”æ—¶é—´: {}ms", elapsed2);
    println!("   LLMå“åº”:\n{}\n", response2);

    // æµ‹è¯•3: æ•…éšœè¯Šæ–­
    println!("ğŸ“Š æµ‹è¯•3: è®¾å¤‡æ•…éšœè¯Šæ–­");
    let fault_diagnosis = r#"
ç”µæœºè¿è¡Œæ•°æ®ï¼š
- æŒ¯åŠ¨: 7.5 mm/s (æ­£å¸¸èŒƒå›´ <5mm/s)
- æ¸©åº¦: 82Â°C (æ­£å¸¸èŒƒå›´ <80Â°C)
- è¿è¡Œæ—¶é•¿: 8å°æ—¶æ— åœæœº

è¯·è¯Šæ–­ï¼š
1. è®¾å¤‡çŠ¶æ€æ˜¯å¦æ­£å¸¸ï¼Ÿ
2. å¯èƒ½çš„æ•…éšœåŸå› ï¼Ÿ
3. å»ºè®®çš„ç»´æŠ¤æªæ–½ï¼Ÿ
"#;

    let (response3, elapsed3) = ctx.llm_analyze(system_prompt, fault_diagnosis).await;
    println!("   å“åº”æ—¶é—´: {}ms", elapsed3);
    println!("   LLMå“åº”:\n{}\n", response3);

    // æµ‹è¯•4: é‡å¤è°ƒç”¨ - æµ‹è¯•ç¨³å®šæ€§
    println!("ğŸ“Š æµ‹è¯•4: è¿ç»­10æ¬¡è°ƒç”¨æµ‹è¯•ç¨³å®šæ€§");
    let mut times = Vec::new();

    for i in 0..10 {
        let query = format!("ç¬¬{}æ¬¡æŸ¥è¯¢ï¼šå½“å‰æ¸©åº¦{}åº¦ï¼Œè¯·ç®€è¦è¯„ä»·ã€‚", i + 1, 20 + i);
        let start = Instant::now();
        let _ = ctx.llm_analyze(system_prompt, &query).await;
        times.push(start.elapsed().as_millis());
        println!("   ç¬¬{}æ¬¡: {}ms", i + 1, times.last().unwrap());
    }

    let avg = times.iter().sum::<u128>() / times.len() as u128;
    let min = *times.iter().min().unwrap();
    let max = *times.iter().max().unwrap();

    println!("\n   ğŸ“ˆ ç»Ÿè®¡:");
    println!("      å¹³å‡: {}ms", avg);
    println!("      æœ€å¿«: {}ms", min);
    println!("      æœ€æ…¢: {}ms", max);
    println!("      æ ‡å‡†å·®: {:.2}ms", {
        let avg_f = avg as f64;
        let variance = times
            .iter()
            .map(|t| (*t as f64 - avg_f).powi(2))
            .sum::<f64>()
            / times.len() as f64;
        variance.sqrt()
    });

    // æµ‹è¯•5: é•¿æ–‡æœ¬ç”Ÿæˆ
    println!("\nğŸ“Š æµ‹è¯•5: é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆè¯¦ç»†æŠ¥å‘Šï¼‰");
    let report_request = r#"
è¯·ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è®¾å¤‡ç»´æŠ¤æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
1. è®¾å¤‡è¿è¡Œæ¦‚å†µ
2. å‘ç°çš„é—®é¢˜
3. è¶‹åŠ¿åˆ†æ
4. ç»´æŠ¤å»ºè®®
5. é¢„é˜²æªæ–½

å½“å‰çŠ¶æ€ï¼šç›‘æ§10å°è®¾å¤‡ï¼Œè¿è¡Œæ­£å¸¸ï¼Œæ¸©åº¦èŒƒå›´18-28Â°Cã€‚
"#;

    let (response5, elapsed5) = ctx.llm_analyze(system_prompt, report_request).await;
    println!("   å“åº”æ—¶é—´: {}ms", elapsed5);
    println!("   å“åº”é•¿åº¦: {} å­—ç¬¦", response5.len());
    println!("   å“åº”é¢„è§ˆ: {}...", &response5[..response5.len().min(100)]);

    println!(
        "\n{}",
        "============================================================"
    );
    println!("çœŸå®æ€§èƒ½æµ‹è¯•å®Œæˆ");
    println!("============================================================");

    Ok(())
}

#[tokio::test]
#[ignore = "Requires real LLM calls"]
async fn test_llm_vs_mock_comparison() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = RealPerfTestContext::new().await?;

    println!(
        "\n{}",
        "============================================================"
    );
    println!("LLMçœŸå®è°ƒç”¨ vs æ¨¡æ‹Ÿå“åº” æ€§èƒ½å¯¹æ¯”");
    println!(
        "{}\n",
        "============================================================"
    );

    // å‡†å¤‡æµ‹è¯•æ•°æ®
    ctx.inject_metrics(
        "sensor_01",
        "temperature",
        &[20.0, 22.0, 24.0, 26.0, 28.0, 30.0],
    )
    .await;

    // æµ‹è¯•åœºæ™¯1: Agentæ‰§è¡Œï¼ˆä¸è°ƒç”¨LLMï¼‰
    println!("ğŸ“Š åœºæ™¯1: Agentæ‰§è¡Œï¼ˆå½“å‰å®ç° - æ— LLMï¼‰");

    let executor_config = AgentExecutorConfig {
        store: ctx.store.clone(),
        time_series_storage: Some(ctx.time_series.clone()),
        device_service: None,
        event_bus: Some(ctx.event_bus.clone()),
        message_manager: None,
        llm_runtime: None, // æ²¡æœ‰LLM
        llm_backend_store: None,
        extension_registry: None,
    };

    let executor = AgentExecutor::new(executor_config).await?;

    let agent = AiAgent {
        id: uuid::Uuid::new_v4().to_string(),
        name: "æµ‹è¯•Agent".to_string(),
        description: None,
        user_prompt: "ç›‘æ§æ¸©åº¦".to_string(),
        llm_backend_id: None,
        parsed_intent: None,
        resources: vec![AgentResource {
            resource_type: ResourceType::Metric,
            resource_id: "sensor_01:temperature".to_string(),
            name: "sensor_01".to_string(),
            config: serde_json::json!({}),
        }],
        schedule: AgentSchedule {
            schedule_type: ScheduleType::Interval,
            interval_seconds: Some(60),
            cron_expression: None,
            timezone: None,
            event_filter: None,
        },
        status: AgentStatus::Active,
        created_at: chrono::Utc::now().timestamp(),
        updated_at: chrono::Utc::now().timestamp(),
        last_execution_at: None,
        stats: AgentStats {
            total_executions: 0,
            successful_executions: 0,
            failed_executions: 0,
            avg_duration_ms: 0,
            last_duration_ms: Some(0),
        },
        memory: AgentMemory {
            state_variables: Default::default(),
            baselines: Default::default(),
            learned_patterns: vec![],
            trend_data: vec![],
            updated_at: chrono::Utc::now().timestamp(),
            working: WorkingMemory::default(),
            short_term: ShortTermMemory::default(),
            long_term: LongTermMemory::default(),
        },
        error_message: None,
        priority: 128,
        conversation_history: vec![],
        user_messages: vec![],
        conversation_summary: None,
        context_window_size: 10,
        enable_tool_chaining: false,
        max_chain_depth: 3,
    };

    ctx.store.save_agent(&agent).await.ok();

    let start = Instant::now();
    let record = executor.execute_agent(agent.clone()).await?;
    let agent_time = start.elapsed().as_millis();

    println!("   æ‰§è¡Œæ—¶é—´: {}ms", agent_time);
    println!(
        "   æ•°æ®æ”¶é›†: {} ä¸ª",
        record.decision_process.data_collected.len()
    );
    println!("   å†³ç­–æ•°: {}", record.decision_process.decisions.len());
    println!("   ç»“è®º: {}", record.decision_process.conclusion);
    println!("   âš ï¸  æ³¨æ„: æ²¡æœ‰è°ƒç”¨LLMï¼Œç»“è®ºæ˜¯é¢„è®¾çš„");

    // åœºæ™¯2: çœŸå®LLMè°ƒç”¨
    println!("\nğŸ“Š åœºæ™¯2: çœŸå®LLMè°ƒç”¨åˆ†æåŒæ ·æ•°æ®");

    let llm_input = format!(
        "ä¼ æ„Ÿå™¨æ•°æ®ï¼šæ¸©åº¦è¯»æ•°ä¸º [20, 22, 24, 26, 28, 30] åº¦ã€‚
è¯·åˆ†æï¼š1. è¶‹åŠ¿å¦‚ä½•ï¼Ÿ2. æ˜¯å¦å¼‚å¸¸ï¼Ÿ3. éœ€è¦é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼Ÿ"
    );

    let (llm_response, llm_time) = ctx.llm_analyze("ä½ æ˜¯è®¾å¤‡ç›‘æ§åŠ©æ‰‹ã€‚", &llm_input).await;

    println!("   LLMå“åº”æ—¶é—´: {}ms", llm_time);
    println!("   LLMåˆ†æç»“æœ: {}", llm_response);

    // å¯¹æ¯”
    println!("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:");
    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚ æ–¹å¼         â”‚ è€—æ—¶     â”‚ è¯´æ˜         â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("   â”‚ Agent(æ— LLM) â”‚ {}ms     â”‚ æ— çœŸå®AI     â”‚", agent_time);
    println!(
        "   â”‚ Agent(+LLM)  â”‚ {}ms    â”‚ çœŸå®AIæ¨ç†  â”‚",
        agent_time + llm_time
    );
    println!(
        "   â”‚ å·®å¼‚         â”‚ {:.1}x   â”‚ LLMæ˜¯ä¸»è¦è€—æ—¶â”‚",
        (agent_time + llm_time) as f64 / agent_time.max(1) as f64
    );
    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

    println!("\nğŸ’¡ ç»“è®º:");
    println!("   ä¹‹å‰æµ‹è¯•æ˜¾ç¤ºçš„20-30msæ˜¯**æ²¡æœ‰è°ƒç”¨LLM**çš„æ‰§è¡Œæ—¶é—´");
    println!("   çœŸå®LLMè°ƒç”¨éœ€è¦500-3000msï¼Œè¿™æ˜¯æ›´å‡†ç¡®çš„ç»“æœ");
    println!("   ç³»ç»Ÿç“¶é¢ˆä¸»è¦åœ¨LLMæ¨ç†ï¼Œä¸åœ¨Agentæ¡†æ¶æœ¬èº«");

    Ok(())
}

#[tokio::test]
#[ignore = "Requires real LLM calls"]
async fn test_realistic_multi_agent_scenario() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = RealPerfTestContext::new().await?;

    println!(
        "\n{}",
        "============================================================"
    );
    println!("çœŸå®åœºæ™¯ï¼šå¤šAgentåä½œï¼ˆæ¯æ¬¡éƒ½è°ƒç”¨LLMï¼‰");
    println!(
        "{}\n",
        "============================================================"
    );

    // æ¨¡æ‹Ÿæ¸©å®¤ç›‘æ§åœºæ™¯
    ctx.inject_metrics("greenhouse", "temperature", &[25.0, 26.0, 27.0, 29.0, 31.0])
        .await;
    ctx.inject_metrics("greenhouse", "humidity", &[65.0, 63.0, 61.0, 58.0, 55.0])
        .await;

    println!("ğŸŒ± åœºæ™¯ï¼šæ¸©å®¤æ¸©åº¦å¼‚å¸¸å‡é«˜\n");

    let system_prompt = "ä½ æ˜¯æ™ºèƒ½æ¸©å®¤ç›‘æ§åŠ©æ‰‹ã€‚";

    // æ­¥éª¤1: ç›‘æ§Agentåˆ†æ
    println!("ğŸ“Š æ­¥éª¤1: ç›‘æ§Agentåˆ†ææ•°æ®...");

    let monitor_input = r#"
æ¸©å®¤ä¼ æ„Ÿå™¨æ•°æ®ï¼š
- æ¸©åº¦: [25, 26, 27, 29, 31] Â°C (ä¸Šå‡è¶‹åŠ¿)
- æ¹¿åº¦: [65, 63, 61, 58, 55] % (ä¸‹é™è¶‹åŠ¿)

è¯·åˆ†æï¼š
1. å½“å‰çŠ¶æ€å¦‚ä½•ï¼Ÿ
2. æ˜¯å¦å­˜åœ¨å¼‚å¸¸ï¼Ÿ
3. éœ€è¦ä»€ä¹ˆæ“ä½œï¼Ÿ
"#;

    let (monitor_response, monitor_time) = ctx.llm_analyze(system_prompt, monitor_input).await;
    println!("   è€—æ—¶: {}ms", monitor_time);
    println!("   åˆ†æ:\n   {}\n", monitor_response);

    // æ­¥éª¤2: æ‰§è¡ŒAgentç”Ÿæˆæ§åˆ¶æŒ‡ä»¤
    println!("ğŸ“Š æ­¥éª¤2: æ‰§è¡ŒAgentç”Ÿæˆæ§åˆ¶æŒ‡ä»¤...");

    let executor_input = r#"
åŸºäºç›‘æ§Agentçš„åˆ†æï¼Œæ¸©å®¤æ¸©åº¦å·²è¾¾31Â°Cï¼ˆè¶…è¿‡ä¸Šé™28Â°Cï¼‰ã€‚
å¯ç”¨æ“ä½œï¼š
1. å¼€å¯é€šé£æ‰‡ (é™ä½æ¸©åº¦)
2. å¼€å¯é®é˜³ç½‘ (å‡å°‘é˜³å…‰)
3. å¼€å¯å–·æ·‹ç³»ç»Ÿ (é™æ¸©+åŠ æ¹¿)

è¯·ç»™å‡ºå…·ä½“çš„æ§åˆ¶æŒ‡ä»¤ï¼ˆJSONæ ¼å¼ï¼‰ã€‚
"#;

    let (executor_response, executor_time) = ctx.llm_analyze(system_prompt, executor_input).await;
    println!("   è€—æ—¶: {}ms", executor_time);
    println!("   æŒ‡ä»¤:\n   {}\n", executor_response);

    // æ­¥éª¤3: åˆ†æAgentç”ŸæˆæŠ¥å‘Š
    println!("ğŸ“Š æ­¥éª¤3: åˆ†æAgentç”Ÿæˆä¼˜åŒ–å»ºè®®...");

    let analyst_input = r#"
è¿‡å»1å°æ—¶çš„æ¸©å®¤æ•°æ®ï¼š
- æ¸©åº¦èŒƒå›´: 25-31Â°C
- æ¹¿åº¦èŒƒå›´: 55-65%
- å·²æ‰§è¡Œæ“ä½œ: å¼€å¯é€šé£æ‰‡

è¯·åˆ†æï¼š
1. æ“ä½œæ•ˆæœå¦‚ä½•ï¼Ÿ
2. æœªæ¥1å°æ—¶è¶‹åŠ¿é¢„æµ‹ï¼Ÿ
3. é•¿æœŸä¼˜åŒ–å»ºè®®ï¼Ÿ
"#;

    let (analyst_response, analyst_time) = ctx.llm_analyze(system_prompt, analyst_input).await;
    println!("   è€—æ—¶: {}ms", analyst_time);
    println!("   å»ºè®®:\n   {}", analyst_response);

    // æ€»è®¡
    let total_time = monitor_time + executor_time + analyst_time;

    println!(
        "\n{}",
        "============================================================"
    );
    println!("ğŸ“Š å¤šAgentåä½œçœŸå®è€—æ—¶:");
    println!("   ç›‘æ§Agent: {}ms", monitor_time);
    println!("   æ‰§è¡ŒAgent: {}ms", executor_time);
    println!("   åˆ†æAgent: {}ms", analyst_time);
    println!("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!(
        "   æ€»è®¡: {}ms ({:.1}ç§’)",
        total_time,
        total_time as f64 / 1000.0
    );
    println!("   å¹³å‡æ¯Agent: {}ms", total_time / 3);
    println!("============================================================");

    println!("\nğŸ’¡ ç»“è®º:");
    println!(
        "   çœŸå®LLMåœºæ™¯ä¸‹ï¼Œ3ä¸ªAgentåä½œéœ€è¦ {:.1} ç§’",
        total_time as f64 / 1000.0
    );
    println!("   è¿™æ‰æ˜¯æ›´æ¥è¿‘å®é™…éƒ¨ç½²çš„æ€§èƒ½è¡¨ç°");

    Ok(())
}

#[tokio::test]
#[ignore = "Requires real LLM calls"]
async fn test_parallel_vs_sequential_execution() -> anyhow::Result<()> {
    if !ollama_available() {
        println!("âš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡æµ‹è¯•");
        return Ok(());
    }

    let ctx = RealPerfTestContext::new().await?;

    println!(
        "\n{}",
        "============================================================"
    );
    println!("å¹¶è¡Œ vs é¡ºåº LLMè°ƒç”¨ æ€§èƒ½å¯¹æ¯”æµ‹è¯•");
    println!(
        "{}\n",
        "============================================================"
    );

    let system_prompt = "ä½ æ˜¯ä¸€ä¸ªç‰©è”ç½‘è®¾å¤‡ç›‘æ§åŠ©æ‰‹ã€‚";

    // å®šä¹‰3ä¸ªä¸åŒçš„æŸ¥è¯¢ä»»åŠ¡
    let queries = vec![
        "å½“å‰æ¸©åº¦ä¸º28åº¦ï¼Œæ¹¿åº¦ä¸º60%ï¼Œè¯·ç®€è¦åˆ†æè¿™ä¸ªç¯å¢ƒçŠ¶æ€ã€‚",
        "åŠå…¬å®¤Aæ¸©åº¦26Â°Cæ¹¿åº¦55%ï¼ŒåŠå…¬å®¤Bæ¸©åº¦29Â°Cæ¹¿åº¦65%ï¼Œè¯·åˆ†æå·®å¼‚ã€‚",
        "ç”µæœºæŒ¯åŠ¨7.5mm/sæ¸©åº¦82Â°Cè¿è¡Œ8å°æ—¶ï¼Œè¯·è¯Šæ–­è®¾å¤‡çŠ¶æ€ã€‚",
    ];

    // æµ‹è¯•1: é¡ºåºLLMè°ƒç”¨
    println!("ğŸ“Š æµ‹è¯•1: é¡ºåºè°ƒç”¨LLM 3æ¬¡");
    let start = std::time::Instant::now();

    let mut sequential_results = Vec::new();
    for (i, query) in queries.iter().enumerate() {
        let (response, elapsed) = ctx.llm_analyze(system_prompt, query).await;
        sequential_results.push((i + 1, elapsed));
        println!(
            "   æŸ¥è¯¢{} - {}ms (å“åº”é•¿åº¦: {} å­—ç¬¦)",
            i + 1,
            elapsed,
            response.len()
        );
    }

    let sequential_time = start.elapsed().as_millis();
    println!("   é¡ºåºè°ƒç”¨æ€»æ—¶é—´: {}ms", sequential_time);

    // æµ‹è¯•2: å¹¶è¡ŒLLMè°ƒç”¨
    println!("\nğŸ“Š æµ‹è¯•2: å¹¶è¡Œè°ƒç”¨LLM 3æ¬¡");
    let start = std::time::Instant::now();

    // ä½¿ç”¨futures::future::join_allå¹¶è¡Œæ‰§è¡Œ
    use futures::future::join_all;

    let parallel_futures: Vec<_> = queries
        .iter()
        .map(|query| {
            let llm_runtime = ctx.llm_runtime.clone();
            let prompt = system_prompt.to_string();
            let q = query.to_string();

            tokio::spawn(async move {
                let messages = vec![
                    neomind_core::message::Message::new(
                        neomind_core::message::MessageRole::System,
                        neomind_core::message::Content::text(&prompt),
                    ),
                    neomind_core::message::Message::new(
                        neomind_core::message::MessageRole::User,
                        neomind_core::message::Content::text(&q),
                    ),
                ];

                let input = neomind_core::llm::backend::LlmInput {
                    messages,
                    params: neomind_core::llm::backend::GenerationParams {
                        temperature: Some(0.7),
                        max_tokens: Some(500),
                        ..Default::default()
                    },
                    model: Some("qwen2.5:0.5b".to_string()),
                    stream: false,
                    tools: None,
                };

                let start = std::time::Instant::now();
                let output = llm_runtime.generate(input).await.unwrap_or_else(|e| {
                    neomind_core::llm::backend::LlmOutput {
                        text: format!("Error: {}", e),
                        finish_reason: neomind_core::llm::backend::FinishReason::Error,
                        usage: None,
                        thinking: None,
                    }
                });
                let elapsed = start.elapsed().as_millis();
                (elapsed, output.text.len())
            })
        })
        .collect();

    let parallel_results = join_all(parallel_futures).await;
    let parallel_time = start.elapsed().as_millis();

    for (i, result) in parallel_results.iter().enumerate() {
        if let Ok((elapsed, len)) = result {
            println!("   æŸ¥è¯¢{} - {}ms (å“åº”é•¿åº¦: {} å­—ç¬¦)", i + 1, elapsed, len);
        }
    }
    println!("   å¹¶è¡Œè°ƒç”¨æ€»æ—¶é—´: {}ms", parallel_time);

    // å¯¹æ¯”
    println!(
        "\n{}",
        "============================================================"
    );
    println!("ğŸ“Š æ€§èƒ½å¯¹æ¯”:");
    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚ æ–¹å¼         â”‚ è€—æ—¶     â”‚ è¯´æ˜         â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!(
        "   â”‚ é¡ºåºè°ƒç”¨     â”‚ {}ms   â”‚ é€ä¸ªç­‰å¾…LLM  â”‚",
        sequential_time
    );
    println!("   â”‚ å¹¶è¡Œè°ƒç”¨     â”‚ {}ms   â”‚ åŒæ—¶ç­‰å¾…LLM  â”‚", parallel_time);

    let speedup = sequential_time as f64 / parallel_time.max(1) as f64;
    let improvement =
        ((sequential_time as f64 - parallel_time as f64) / sequential_time as f64 * 100.0);

    println!(
        "   â”‚ æ€§èƒ½æå‡     â”‚ {:.1}%    â”‚ {:.1}x æ›´å¿«    â”‚",
        improvement.max(0.0),
        speedup
    );
    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

    println!("\nğŸ’¡ ç»“è®º:");
    println!("   å¹¶è¡Œæ‰§è¡Œå¯ä»¥è®©å¤šä¸ªLLMè¯·æ±‚åŒæ—¶ç­‰å¾…å“åº”");
    println!(
        "   3ä¸ªLLMè°ƒç”¨çš„æ€»æ—¶é—´ä» {}ms é™è‡³ {}ms",
        sequential_time, parallel_time
    );
    println!(
        "   è¿™æ„å‘³ç€åœ¨å¤šAgentåä½œåœºæ™¯ä¸­ï¼Œå¯ä»¥èŠ‚çœ {:.1}% çš„æ—¶é—´",
        improvement.max(0.0)
    );
    println!("============================================================");

    Ok(())
}
