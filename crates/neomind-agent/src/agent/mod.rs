//! Core AI Agent that orchestrates LLM, memory, and tools.
//!
//! ## Architecture
//!
//! The `Agent` is a high-level AI agent that integrates LLM, tools, and memory.
//!
//! ```text
//! ┌─────────────────────────────────────────────────────┐
//! │                    Agent                            │
//! │  ┌────────────────────────────────────────────────┐ │
//! │  │  LlmInterface (LLM wrapper)                    │ │
//! │  │  - LLM runtime management                     │ │
//! │  │  - chat() / chat_stream()                     │ │
//! │  └────────────────────────────────────────────────┘ │
//! │                                                       │
//! │  + ToolRegistry (function calling)                 │
//! │  + Memory (conversation history)                    │
//! │  + SessionState (metadata tracking)                 │
//! └─────────────────────────────────────────────────────┘
//! ```

pub mod fallback;
pub mod formatter;
pub mod semantic_mapper;
pub mod staged;
pub mod streaming;
pub mod tool_parser;
pub mod tokenizer;
pub mod types;
pub mod conversation_context;
pub mod smart_followup;
pub mod intent_classifier;
pub mod scheduler;
pub mod cache;

use std::pin::Pin;
use std::sync::Arc;

use futures::Stream;
use tokio::sync::RwLock;

// Re-export error types
pub use crate::error::NeoMindError;
use serde_json::Value;

use super::error::Result;
use super::llm::{ChatConfig, LlmInterface};
use crate::context::ResourceIndex;
use neomind_core::{
    Message,
    llm::backend::LlmRuntime,
    config::agent_env_vars,
};
use neomind_llm::{CloudConfig, CloudRuntime, OllamaConfig, OllamaRuntime};

// Type aliases to reduce complexity
pub type SharedToolRegistry = Arc<neomind_tools::ToolRegistry>;
pub type SharedLlmInterface = Arc<LlmInterface>;
pub type SharedSessionState = Arc<RwLock<SessionState>>;
pub type SharedResourceIndex = Arc<RwLock<ResourceIndex>>;
pub type SharedSmartConversation = Arc<tokio::sync::RwLock<crate::smart_conversation::SmartConversationManager>>;
pub type SharedSemanticMapper = Arc<semantic_mapper::SemanticToolMapper>;
pub type EventStream = Pin<Box<dyn Stream<Item = AgentEvent> + Send>>;
pub type MessageStream = Pin<Box<dyn Stream<Item = (String, bool)> + Send>>;

pub use fallback::{FallbackRule, default_fallback_rules, process_fallback};
pub use formatter::{format_summary, format_tool_result};
pub use semantic_mapper::{
    SemanticToolMapper, SemanticMapping, SemanticMatchType,
    DeviceMapping, RuleMapping, WorkflowMapping, MappingStats,
};
pub use streaming::{
    StreamSafeguards, events_to_string_stream, process_stream_events,
    process_stream_events_with_safeguards, process_multimodal_stream_events,
    process_multimodal_stream_events_with_safeguards,
};
pub use types::{
    AgentConfig, AgentEvent, AgentInternalState, AgentMessage, AgentMessageImage, AgentResponse,
    LlmBackend, SessionState, ToolCall,
};
pub use conversation_context::{
    ConversationContext, ConversationTopic, EntityReference, EntityType,
};
pub use smart_followup::{
    SmartFollowUpManager, FollowUpAnalysis, FollowUpItem, FollowUpType,
    FollowUpPriority, DetectedIntent, AvailableDevice,
};
pub use crate::task_orchestrator::{
    TaskOrchestrator, TaskSession, TaskStep, TaskResponse, TaskContext,
    TaskStatus, StepType, ResponseType,
};
pub use crate::context_selector::{
    ContextSelector, IntentAnalyzer, IntentAnalysis, ContextBundle,
    IntentType, Entity, ContextScope,
    DeviceTypeReference, RuleReference, CommandReference,
};

/// Maximum number of tool calls allowed per request to prevent infinite loops
/// Note: This constant is kept for backward compatibility. Config values take precedence.
#[allow(dead_code)]
const MAX_TOOL_CALLS_PER_REQUEST_DEFAULT: usize = 5;

/// === ANTHROPIC-STYLE IMPROVEMENT: Tool Result Clearing ===
///
/// Compacts old tool result messages into concise summaries.
/// This follows Anthropic's guidance: "One of the safest lightest touch forms
/// of compaction is tool result clearing – once a tool has been called deep
/// in the message history, why would the agent need to see the raw result again?"
///
/// Rules:
/// - Keep the most recent N tool results intact (configurable, default: 2)
/// - Older tool results are compressed to one-line summaries
/// - User and system messages are always kept
pub fn compact_tool_results(messages: &[AgentMessage], keep_recent: usize) -> Vec<AgentMessage> {
    let mut result = Vec::new();
    let mut tool_result_count = 0;

    for msg in messages.iter().rev() {
        // Always keep user and system messages
        if msg.role == "user" || msg.role == "system" {
            result.push(msg.clone());
            continue;
        }

        // Check if this is a tool result message (has tool_calls)
        if msg.tool_calls.is_some() && msg.tool_calls.as_ref().is_some_and(|t| !t.is_empty()) {
            tool_result_count += 1;

            // Keep recent tool results intact
            if tool_result_count <= keep_recent {
                result.push(msg.clone());
            } else {
                // Compress old tool results to a brief summary
                let tool_names: Vec<&str> = msg
                    .tool_calls
                    .as_ref()
                    .iter()
                    .flat_map(|calls| calls.iter().map(|t| t.name.as_str()))
                    .collect();

                // Create a compacted summary message
                let summary = if tool_names.len() == 1 {
                    format!("[之前调用了工具: {}]", tool_names[0])
                } else {
                    format!("[之前调用了工具: {}]", tool_names.join(", "))
                };

                result.push(AgentMessage {
                    role: msg.role.clone(),
                    content: summary,
                    tool_calls: None, // Remove actual tool data to save tokens
                    tool_call_id: None,
                    tool_call_name: None,
                    thinking: None, // Never keep thinking in compacted messages
                    images: None,
                    timestamp: msg.timestamp,
                });
            }
        } else {
            // Regular assistant message - keep it
            result.push(msg.clone());
        }
    }

    result.reverse();
    result
}

/// === ENHANCED: Conversation-Level Compression ===
///
/// Extends compression beyond tool results to include conversation messages.
/// This follows the "tiered compression" strategy from LangChain:
/// - Level 1: Keep recent messages intact
/// - Level 2: Summarize older assistant messages to key points
/// - Level 3: Compress very old messages to brief topic markers
///
/// Rules:
/// - Keep N most recent messages intact (default: 6)
/// - Preserve user messages verbatim (higher priority for user intent)
/// - Compress assistant messages to key points (remove verbose explanations)
/// - Group very old messages into topic summaries
/// - Never compress system messages
///
/// Expected impact: 30-50% token reduction for long conversations.
pub fn compact_conversation(
    messages: &[AgentMessage],
    keep_recent: usize,
    target_tokens: usize,
) -> Vec<AgentMessage> {
    if messages.len() <= keep_recent {
        return messages.to_vec();
    }

    let mut result = Vec::new();
    let mut current_tokens = 0;

    // First pass: keep recent messages intact
    let recent_start = messages.len().saturating_sub(keep_recent);
    for msg in &messages[recent_start..] {
        result.push(msg.clone());
        current_tokens += tokenizer::estimate_message_tokens(msg);
    }

    // If we're already under the token limit, return early
    if current_tokens <= target_tokens {
        // Still need to add older messages in reverse order
        for msg in messages[..recent_start].iter().rev() {
            let msg_tokens = tokenizer::estimate_message_tokens(msg);
            if current_tokens + msg_tokens > target_tokens {
                break;
            }
            result.insert(0, msg.clone());
            current_tokens += msg_tokens;
        }
        return result;
    }

    // Second pass: compress older messages
    let mut compressed_older = Vec::new();
    let mut topic_batches: Vec<String> = Vec::new();

    for (i, msg) in messages[..recent_start].iter().enumerate() {
        // Always keep system messages
        if msg.role == "system" {
            compressed_older.push(msg.clone());
            current_tokens += tokenizer::estimate_message_tokens(msg);
            continue;
        }

        // Keep user messages verbatim (they contain critical intent)
        if msg.role == "user" {
            // Truncate very long user messages
            let truncated_content = if msg.content.len() > 200 {
                format!("{}... (消息过长，已截断)", &msg.content[..200])
            } else {
                msg.content.clone()
            };
            compressed_older.push(AgentMessage {
                content: truncated_content,
                ..msg.clone()
            });
            current_tokens += tokenizer::estimate_message_tokens(msg);
            continue;
        }

        // Assistant messages: create brief summary
        if msg.role == "assistant" {
            let summary = summarize_assistant_message(msg);
            topic_batches.push(summary);
        }
    }

    // Create a single summary for old conversation
    if !topic_batches.is_empty() {
        let old_summary = if topic_batches.len() <= 3 {
            format!("[之前的对话: {}]", topic_batches.join("; "))
        } else {
            format!(
                "[之前的对话: 包含{}轮交流，主题涉及{}]",
                topic_batches.len(),
                if topic_batches.len() > 5 { "多个话题" } else { "相关内容" }
            )
        };

        // Create a synthetic summary message
        let timestamp = messages
            .first()
            .and_then(|m| Some(m.timestamp))
            .unwrap_or_else(|| chrono::Utc::now().timestamp());

        compressed_older.push(AgentMessage {
            role: "system".to_string(),
            content: old_summary,
            tool_calls: None,
            tool_call_id: None,
            tool_call_name: None,
            thinking: None,
            images: None,
            timestamp,
        });
    }

    // Combine compressed older messages with recent messages
    let mut final_result = compressed_older;
    final_result.extend(result);

    final_result
}

/// Summarize an assistant message to its key points.
fn summarize_assistant_message(msg: &AgentMessage) -> String {
    let content = msg.content.trim();

    // Early return for short content
    if content.len() <= 50 {
        return content.to_string();
    }

    // Check for common patterns and extract key info
    if content.contains("成功") || content.contains("已完成") {
        if let Some(tool_name) = &msg.tool_call_name {
            return format!("执行了{}", tool_name);
        }
        return "操作已完成".to_string();
    }

    if content.contains("失败") || content.contains("错误") {
        return format!("操作失败: {}", extract_first_phrase(content, 30));
    }

    if content.contains("查询到") || content.contains("数据显示") {
        return format!("查询了数据: {}", extract_first_phrase(content, 30));
    }

    // Generic: extract first meaningful phrase
    extract_first_phrase(content, 40)
}

/// Extract the first meaningful phrase from text.
fn extract_first_phrase(text: &str, max_len: usize) -> String {
    let trimmed = text.trim();
    if trimmed.len() <= max_len {
        return trimmed.to_string();
    }

    // Try to break at sentence boundary
    if let Some(pos) = trimmed.find('。') {
        if pos <= max_len {
            return trimmed[..pos + 1].to_string();
        }
    }

    if let Some(pos) = trimmed.find('.') {
        if pos <= max_len {
            return trimmed[..pos + 1].to_string();
        }
    }

    if let Some(pos) = trimmed.find('，') {
        if pos <= max_len {
            return trimmed[..pos].to_string();
        }
    }

    if let Some(pos) = trimmed.find(',') {
        if pos <= max_len {
            return trimmed[..pos].to_string();
        }
    }

    // Hard truncate with ellipsis
    format!("{}...", &trimmed[..max_len.saturating_sub(3)])
}

/// === ENHANCED: Context Window with Importance-Based Selection ===
///
/// Builds conversation context with:
/// 1. Tool result clearing for old messages
/// 2. Conversation-level compression for long histories
/// 3. Importance-based message selection (P1.2)
/// 4. Token-based windowing with accurate estimation
/// 5. Always keep recent messages (minimum 4) for context continuity
///
/// The `max_tokens` parameter allows dynamic context sizing based on the model's actual capacity.
/// This prevents wasting model capability (e.g., using 5k context with a 32k model) while
/// also preventing errors from exceeding the model's limit (e.g., using 12k context with an 8k model).
fn build_context_window(messages: &[AgentMessage], max_tokens: usize) -> Vec<AgentMessage> {
    // Use the improved tokenizer module for accurate token estimation
    use tokenizer::select_messages_with_importance;

    // First, apply tool result clearing
    let tool_compacted = compact_tool_results(messages, 2); // Default: keep 2 recent results

    // Then apply conversation-level compression if we have many messages
    let conversation_compacted = if tool_compacted.len() > 12 {
        // Only compress if we have more than 12 messages after tool clearing
        compact_conversation(&tool_compacted, 6, max_tokens)
    } else {
        tool_compacted
    };

    // Use importance-based selection for better context quality (P1.2)
    // This prioritizes system messages, user intent, and error handling
    let selected_refs = select_messages_with_importance(
        &conversation_compacted,
        max_tokens,
        4,  // Always keep at least 4 recent messages
        0.15, // Minimum importance threshold
    );

    // Convert references to owned messages
    selected_refs.into_iter().cloned().collect()
}

/// Calculate adaptive context size adjustment based on conversation complexity.
///
/// Returns a multiplier (0.9 to 1.2) that adjusts the context window:
/// - 1.2: High complexity (many entities, topics, recent errors)
/// - 1.0: Normal complexity
/// - 0.9: Low complexity (simple greetings, repetitive)
///
/// Complexity factors:
/// - High entity diversity: +10%
/// - Multiple active topics: +10%
/// - Recent errors: +15%
/// - Simple greetings: -10%
fn calculate_adaptive_context_adjustment(messages: &[AgentMessage]) -> f64 {
    if messages.is_empty() {
        return 1.0;
    }

    let mut adjustment = 1.0f64;

    // Analyze recent messages (last 10) for complexity
    let recent_count = messages.len().min(10);
    let recent = &messages[messages.len().saturating_sub(recent_count)..];

    // 1. Entity diversity: Count unique device/rule/agent mentions
    let mut entities = std::collections::HashSet::new();
    for msg in recent {
        let content = msg.content.to_lowercase();

        // Extract device IDs
        for word in content.split_whitespace() {
            if word.starts_with("device_") || word.starts_with("设备") {
                entities.insert(word.to_string());
            }
            // Rule mentions
            if word.contains("rule") || word.contains("规则") {
                entities.insert("rule".to_string());
            }
            // Agent mentions
            if word.contains("agent") || word.contains("智能体") {
                entities.insert("agent".to_string());
            }
            // Location mentions
            if word.contains("客厅") || word.contains("卧室") || word.contains("厨房")
                || word.contains("living") || word.contains("bedroom") || word.contains("kitchen") {
                entities.insert("location".to_string());
            }
        }
    }

    // High entity diversity: +10%
    if entities.len() >= 4 {
        adjustment += 0.1;
        tracing::debug!("Adaptive context: +10% for high entity diversity ({})", entities.len());
    }

    // 2. Topic variety: Count distinct topics
    let mut topics = std::collections::HashSet::new();
    for msg in recent {
        let content = msg.content.to_lowercase();

        if content.contains("温度") || content.contains("temperature") {
            topics.insert("temperature");
        }
        if content.contains("灯") || content.contains("light") {
            topics.insert("lighting");
        }
        if content.contains("湿度") || content.contains("humidity") {
            topics.insert("humidity");
        }
        if content.contains("创建") || content.contains("create") {
            topics.insert("creation");
        }
        if content.contains("查询") || content.contains("query") || content.contains("list") {
            topics.insert("query");
        }
        if content.contains("控制") || content.contains("control") {
            topics.insert("control");
        }
    }

    // Multiple active topics: +10%
    if topics.len() >= 3 {
        adjustment += 0.1;
        tracing::debug!("Adaptive context: +10% for multiple topics ({})", topics.len());
    }

    // 3. Recent errors: +15%
    let has_recent_errors = recent.iter().any(|msg| {
        let content = msg.content.to_lowercase();
        content.contains("错误") || content.contains("失败") || content.contains("error")
            || content.contains("fail") || msg.role == "tool" && content.contains("\"success\":false")
    });
    if has_recent_errors {
        adjustment += 0.15;
        tracing::debug!("Adaptive context: +15% for recent errors");
    }

    // 4. Simple greetings: -10%
    let is_simple_greeting = messages.len() <= 3 && recent.iter().all(|msg| {
        let content = msg.content.to_lowercase();
        let tokens = content.split_whitespace().count();
        tokens <= 5 || content.contains("你好") || content.contains("hello")
            || content.contains("hi") || content.contains("嗨")
    });
    if is_simple_greeting {
        adjustment -= 0.1;
        tracing::debug!("Adaptive context: -10% for simple greeting");
    }

    // 5. Repetitive content penalty: -5%
    let unique_contents: std::collections::HashSet<_> = recent.iter().map(|m| m.content.as_str()).collect();
    if recent.len() > 3 && unique_contents.len() < recent.len() / 2 {
        adjustment -= 0.05;
        tracing::debug!("Adaptive context: -5% for repetitive content");
    }

    // Clamp adjustment to reasonable bounds
    let adjustment = adjustment.clamp(0.9, 1.2);

    tracing::debug!(
        "Adaptive context adjustment: {:.2} (entities={}, topics={}, has_errors={}, is_greeting={})",
        adjustment,
        entities.len(),
        topics.len(),
        has_recent_errors,
        is_simple_greeting
    );

    adjustment
}

/// Default context window size to use when model capacity is unknown.
/// This is a conservative value that works for most models.
#[allow(dead_code)]
const DEFAULT_CONTEXT_TOKENS: usize = 8_000;

/// AI Agent that orchestrates components.
pub struct Agent {
    /// Configuration
    config: AgentConfig,
    /// Session ID
    session_id: String,
    /// Tool registry
    tools: Arc<neomind_tools::ToolRegistry>,
    /// LLM interface
    llm_interface: Arc<LlmInterface>,
    /// Unified internal state (memory + session + llm_ready)
    /// Single lock reduces contention compared to multiple Arc<RwLock<...>>
    internal_state: Arc<tokio::sync::RwLock<AgentInternalState>>,
    /// Fallback rules for when LLM is unavailable
    fallback_rules: Vec<FallbackRule>,
    /// Process lock to prevent concurrent requests on the same session
    process_lock: Arc<tokio::sync::Mutex<()>>,
    /// Smart conversation manager - intercepts input for追问/确认
    smart_conversation: Arc<tokio::sync::RwLock<crate::smart_conversation::SmartConversationManager>>,
    /// Semantic mapper - converts natural language to technical IDs
    semantic_mapper: Arc<semantic_mapper::SemanticToolMapper>,
    /// Conversation context - enables continuous dialogue with entity references
    conversation_context: Arc<tokio::sync::RwLock<ConversationContext>>,
    /// Smart followup manager - intelligent question generation when input is incomplete
    smart_followup: Arc<tokio::sync::RwLock<SmartFollowUpManager>>,
    /// Context selector - intelligent context selection based on intent analysis
    context_selector: Arc<tokio::sync::RwLock<crate::context_selector::ContextSelector>>,
    /// Last injected context summary hash (for deduplication)
    last_injected_context_hash: Arc<tokio::sync::RwLock<u64>>,
}

impl Agent {
    /// Create a new agent with custom tool registry.
    pub fn with_tools(
        config: AgentConfig,
        session_id: String,
        tools: Arc<neomind_tools::ToolRegistry>,
    ) -> Self {
        let session_id_clone = session_id.clone();

        // Create LLM interface
        let llm_config = ChatConfig {
            model: config.model.clone(),
            temperature: config.temperature,
            top_p: 0.75,
            top_k: 20,  // Lowered for faster responses
            max_tokens: usize::MAX, // No artificial limit - let model decide
            concurrent_limit: 3,    // Default to 3 concurrent LLM requests
        };

        let llm_interface =
            Arc::new(LlmInterface::new(llm_config).with_system_prompt(&config.system_prompt));

        // Create semantic mapper with resource index
        let resource_index = Arc::new(RwLock::new(ResourceIndex::new()));
        let semantic_mapper = Arc::new(semantic_mapper::SemanticToolMapper::new(resource_index.clone()));

        // Create smart followup manager with resource index for device-aware followups
        let smart_followup = Arc::new(tokio::sync::RwLock::new(
            SmartFollowUpManager::with_resource_index(resource_index.clone())
        ));

        Self {
            config,
            session_id,
            tools,
            llm_interface,
            internal_state: Arc::new(tokio::sync::RwLock::new(AgentInternalState::new(
                session_id_clone,
            ))),
            fallback_rules: default_fallback_rules(),
            process_lock: Arc::new(tokio::sync::Mutex::new(())),
            smart_conversation: Arc::new(tokio::sync::RwLock::new(
                crate::smart_conversation::SmartConversationManager::new()
            )),
            semantic_mapper,
            conversation_context: Arc::new(tokio::sync::RwLock::new(ConversationContext::new())),
            smart_followup,
            context_selector: Arc::new(tokio::sync::RwLock::new(crate::context_selector::ContextSelector::new())),
            last_injected_context_hash: Arc::new(tokio::sync::RwLock::new(0)),
        }
    }

    /// Get internal state for streaming (used by streaming module).
    pub fn internal_state(&self) -> Arc<tokio::sync::RwLock<AgentInternalState>> {
        self.internal_state.clone()
    }

    /// Get the LLM interface (for capability checks).
    pub fn llm_interface(&self) -> Arc<LlmInterface> {
        Arc::clone(&self.llm_interface)
    }

    /// Create a new agent with empty tool registry.
    /// Tools should be configured externally through the session manager.
    pub fn new(config: AgentConfig, session_id: String) -> Self {
        // Build tool registry - start empty, tools will be added by session manager
        let mut registry = neomind_tools::ToolRegistryBuilder::new()
            .build();

        // Add agent-specific tools
        use crate::tools::{ThinkTool, ToolSearchTool};
        use crate::tools::{AskUserTool, ConfirmActionTool, ClarifyIntentTool};

        // Create tool search tool (starts with empty tool list)
        let tool_search = ToolSearchTool::from_definitions(&[]);
        registry.register(std::sync::Arc::new(tool_search));

        // Create and register think tool
        let think_tool = ThinkTool::new();
        registry.register(std::sync::Arc::new(think_tool));

        // === 添加用户交互工具 ===
        // ask_user: 向用户询问缺失信息
        let ask_user_tool = AskUserTool::new();
        registry.register(std::sync::Arc::new(ask_user_tool));

        // confirm_action: 二次确认危险操作
        let confirm_tool = ConfirmActionTool::new();
        registry.register(std::sync::Arc::new(confirm_tool));

        // clarify_intent: 澄清模糊意图
        let clarify_tool = ClarifyIntentTool::new();
        registry.register(std::sync::Arc::new(clarify_tool));

        Self::with_tools(config, session_id, Arc::new(registry))
    }

    /// Create with default config and empty tools.
    pub fn with_session(session_id: String) -> Self {
        Self::new(AgentConfig::default(), session_id)
    }

    /// Set custom fallback rules.
    pub fn with_fallback_rules(mut self, rules: Vec<FallbackRule>) -> Self {
        self.fallback_rules = rules;
        self
    }

    /// Configure the LLM backend.
    pub async fn configure_llm(&self, backend: LlmBackend) -> Result<()> {
        tracing::debug!(backend = ?backend, "Agent::configure_llm called");

        // Load timeout from environment variable (or use defaults)
        let ollama_timeout = agent_env_vars::ollama_timeout_secs();
        let cloud_timeout = agent_env_vars::cloud_timeout_secs();

        tracing::debug!(
            ollama_timeout_secs = ollama_timeout,
            cloud_timeout_secs = cloud_timeout,
            "Configuring LLM with timeout values"
        );

        let (llm, model_name) = match backend {
            LlmBackend::Ollama { endpoint, model } => {
                tracing::info!(
                    endpoint = %endpoint, model = %model, timeout = ollama_timeout,
                    "Creating OllamaRuntime"
                );
                let config = OllamaConfig::new(&model)
                    .with_endpoint(&endpoint)
                    .with_timeout_secs(ollama_timeout);
                let runtime =
                    OllamaRuntime::new(config).map_err(|e| NeoMindError::llm(e.to_string()))?;
                (Arc::new(runtime) as Arc<dyn LlmRuntime>, model)
            }
            LlmBackend::OpenAi {
                api_key,
                endpoint,
                model,
            } => {
                tracing::info!(
                    endpoint = %endpoint, model = %model, timeout = cloud_timeout,
                    "Creating CloudRuntime for OpenAI"
                );
                let config = CloudConfig::openai(&api_key)
                    .with_timeout_secs(cloud_timeout);
                let config = if endpoint.is_empty() {
                    config.with_model(&model)
                } else {
                    // Custom endpoint
                    CloudConfig::custom(&api_key, &endpoint)
                        .with_model(&model)
                        .with_timeout_secs(cloud_timeout)
                };
                let runtime =
                    CloudRuntime::new(config).map_err(|e| NeoMindError::llm(e.to_string()))?;
                (Arc::new(runtime) as Arc<dyn LlmRuntime>, model)
            }
        };

        // Update model override
        self.llm_interface.update_model(model_name).await;

        self.llm_interface.set_llm(llm).await;
        self.internal_state.write().await.set_llm_ready(true);

        // Set tool definitions for function calling
        self.update_tool_definitions().await;

        Ok(())
    }

    /// Set a custom LLM runtime directly (for testing purposes).
    pub async fn set_custom_llm(&self, llm: Arc<dyn LlmRuntime>) {
        self.llm_interface.set_llm(llm).await;
        self.internal_state.write().await.set_llm_ready(true);
        self.update_tool_definitions().await;
    }

    /// Update tool definitions in the LLM interface.
    /// Uses simplified tool definitions for better LLM understanding.
    /// Also dynamically updates the system prompt to include tool descriptions.
    pub async fn update_tool_definitions(&self) {
        use neomind_core::llm::backend::ToolDefinition as CoreToolDefinition;
        use neomind_tools::simplified;

        // Use simplified tool definitions for LLM function calling
        let simplified_tools = simplified::get_simplified_tools();
        let core_defs: Vec<CoreToolDefinition> = simplified_tools
            .iter()
            .map(|tool| {
                // Build simplified parameters schema
                let mut properties = serde_json::Map::new();
                let mut required = Vec::new();

                for param in &tool.required {
                    required.push(param.clone());
                    properties.insert(param.clone(), serde_json::json!({
                        "type": "string",
                        "description": param
                    }));
                }
                for (param, info) in &tool.optional {
                    properties.insert(param.clone(), serde_json::json!({
                        "type": "string",
                        "description": info.description,
                        "default": info.default
                    }));
                }

                CoreToolDefinition {
                    name: tool.name.clone(),
                    description: format!("{} (别名: {})", tool.description, tool.aliases.join(", ")),
                    parameters: serde_json::json!({
                        "type": "object",
                        "properties": properties,
                        "required": required
                    }),
                }
            })
            .collect();

        let tool_count = core_defs.len();
        self.llm_interface.set_tool_definitions(core_defs).await;

        // Dynamically update system prompt with tool descriptions
        let dynamic_prompt = self.generate_dynamic_system_prompt(&simplified_tools).await;
        self.llm_interface.set_system_prompt(&dynamic_prompt).await;

        tracing::debug!("Updated {} simplified tool definitions for LLM", tool_count);
    }

    /// Generate a dynamic system prompt with tool descriptions.
    /// This ensures the prompt always reflects the currently available tools.
    async fn generate_dynamic_system_prompt(&self, simplified_tools: &[neomind_tools::simplified::LlmToolDefinition]) -> String {
        // Generate base prompt (static parts: system_prompt + tools)
        let mut prompt = self.generate_base_prompt(simplified_tools);

        // === 动态注入系统资源上下文 ===
        // 这确保 LLM 能够感知当前系统中的实际设备、规则和工作流
        let resource_context = self.semantic_mapper.get_semantic_context().await;
        if !resource_context.is_empty() {
            prompt.push_str("\n\n");
            prompt.push_str(&resource_context);
        }

        prompt
    }

    /// Generate base prompt (static parts: system_prompt + tools).
    /// This avoids rebuilding tool descriptions on every request.
    fn generate_base_prompt(&self, simplified_tools: &[neomind_tools::simplified::LlmToolDefinition]) -> String {
        let mut prompt = String::from(self.config.system_prompt.trim());

        prompt.push_str("\n\n## 可用工具\n\n");

        // Group tools by category for better organization
        let mut device_tools = Vec::new();
        let mut data_tools = Vec::new();
        let mut rule_tools = Vec::new();
        let mut system_tools = Vec::new();

        for tool in simplified_tools {
            if tool.name.contains("device") || tool.name.contains("control") {
                device_tools.push(tool);
            } else if tool.name.contains("data") || tool.name.contains("query") || tool.name.contains("metrics") {
                data_tools.push(tool);
            } else if tool.name.contains("rule") {
                rule_tools.push(tool);
            } else {
                system_tools.push(tool);
            }
        }

        // Add tool sections
        if !device_tools.is_empty() {
            prompt.push_str("### 设备管理\n");
            for tool in device_tools {
                prompt.push_str(&format!("- **{}**: {} (别名: {})\n",
                    tool.name, tool.description, tool.aliases.join(", ")));
            }
            prompt.push('\n');
        }

        if !data_tools.is_empty() {
            prompt.push_str("### 数据查询\n");
            for tool in data_tools {
                prompt.push_str(&format!("- **{}**: {} (别名: {})\n",
                    tool.name, tool.description, tool.aliases.join(", ")));
            }
            prompt.push('\n');
        }

        if !rule_tools.is_empty() {
            prompt.push_str("### 规则管理\n");
            for tool in rule_tools {
                prompt.push_str(&format!("- **{}**: {} (别名: {})\n",
                    tool.name, tool.description, tool.aliases.join(", ")));
            }
            prompt.push('\n');
        }

        if !system_tools.is_empty() {
            prompt.push_str("### 系统工具\n");
            for tool in system_tools {
                prompt.push_str(&format!("- **{}**: {} (别名: {})\n",
                    tool.name, tool.description, tool.aliases.join(", ")));
            }
            prompt.push('\n');
        }

        // Add usage guidance
        prompt.push_str("## 使用指南\n");
        prompt.push_str("- 多个工具调用可以并行执行，提高响应速度\n");
        prompt.push_str("- 设备别名和工具别名都可以使用，系统会自动识别\n");
        prompt.push_str("- 直接回答问题，不要过度思考或展开冗长的推理过程\n");

        prompt
    }

    /// Check if LLM is configured.
    pub async fn is_llm_configured(&self) -> bool {
        self.llm_interface.is_ready().await
    }

    /// Get the session ID.
    pub fn session_id(&self) -> &str {
        &self.session_id
    }

    /// Get the session state.
    pub async fn state(&self) -> SessionState {
        self.internal_state.read().await.session.clone()
    }

    /// Get the conversation history.
    pub async fn history(&self) -> Vec<AgentMessage> {
        self.internal_state.read().await.memory.clone()
    }

    /// Restore conversation history from persisted data.
    pub async fn restore_history(&self, messages: Vec<AgentMessage>) {
        self.internal_state.write().await.restore_memory(messages);
    }

    /// Clear conversation history.
    pub async fn clear_history(&self) {
        self.internal_state.write().await.clear_memory();
    }

    /// Get available tools.
    pub fn available_tools(&self) -> Vec<String> {
        self.tools.list()
    }

    /// Get tool definitions for LLM.
    pub fn tool_definitions(&self) -> Value {
        self.tools.definitions_json()
    }

    /// Update the smart conversation manager with current devices.
    /// This enables better intent analysis based on available devices.
    pub async fn update_smart_context_devices(&self, devices: Vec<crate::smart_conversation::Device>) {
        let mut smart_conv = self.smart_conversation.write().await;
        smart_conv.update_devices(devices);
    }

    /// Update the smart conversation manager with current rules.
    /// This enables better intent analysis based on available rules.
    pub async fn update_smart_context_rules(&self, rules: Vec<crate::smart_conversation::Rule>) {
        let mut smart_conv = self.smart_conversation.write().await;
        smart_conv.update_rules(rules);
    }

    // === SMART FOLLOWUP METHODS ===

    /// Update smart followup manager with current devices.
    /// This enables intelligent followup questions based on available devices.
    pub async fn update_followup_devices(&self, devices: Vec<AvailableDevice>) {
        let mut followup = self.smart_followup.write().await;
        followup.set_available_devices(devices);
    }

    /// Refresh smart followup devices from resource index.
    pub async fn refresh_followup_devices(&self) {
        let mut followup = self.smart_followup.write().await;
        followup.refresh_devices().await;
    }

    // === CONTEXT SELECTOR METHODS ===

    /// Get the context selector reference.
    pub async fn context_selector(&self) -> Arc<tokio::sync::RwLock<crate::context_selector::ContextSelector>> {
        Arc::clone(&self.context_selector)
    }

    /// Analyze query intent and get suggested context bundle.
    pub async fn analyze_intent(&self, query: &str) -> (IntentAnalysis, ContextBundle) {
        let selector = self.context_selector.read().await;
        selector.select_context(query).await
    }

    /// Update device types in the context selector.
    pub async fn update_context_device_types(&self, device_types: Vec<neomind_devices::mdl_format::DeviceTypeDefinition>) {
        let selector = self.context_selector.read().await;
        selector.set_device_types(device_types).await;
        selector.register_with_analyzer().await;
    }

    /// Update rule engine in the context selector.
    pub async fn update_context_rule_engine(&self, engine: Arc<neomind_rules::RuleEngine>) {
        let selector = self.context_selector.read().await;
        selector.set_rule_engine(engine).await;
    }

    // === P4.2: INTELLIGENT MEMORY INJECTION HELPERS ===

    /// Extract entities and topics from conversation for memory retrieval.
    ///
    /// Analyzes recent messages to identify:
    /// - Device mentions (device IDs, locations)
    /// - Topic keywords (temperature, lighting, etc.)
    /// - Rule/automation mentions
    fn extract_conversation_entities_topics(&self, messages: &[AgentMessage]) -> (Vec<String>, Vec<String>) {
        use std::collections::HashSet;

        let mut entities = HashSet::new();
        let mut topics = HashSet::new();

        // Analyze last 10 messages
        let recent_count = messages.len().min(10);
        let recent = &messages[messages.len().saturating_sub(recent_count)..];

        for msg in recent {
            let content = msg.content.to_lowercase();

            // Extract device entities
            for word in content.split_whitespace() {
                if word.starts_with("device_") || word.starts_with("设备") {
                    entities.insert(word.to_string());
                }
                // Location mentions
                if word.contains("客厅") || word.contains("卧室") || word.contains("厨房") {
                    entities.insert(word.to_string());
                }
                if word.contains("living") || word.contains("bedroom") || word.contains("kitchen") {
                    entities.insert(word.to_string());
                }
            }

            // Extract topics
            if content.contains("温度") || content.contains("temperature") {
                topics.insert("temperature".to_string());
            }
            if content.contains("灯") || content.contains("light") {
                topics.insert("lighting".to_string());
            }
            if content.contains("湿度") || content.contains("humidity") {
                topics.insert("humidity".to_string());
            }
            if content.contains("规则") || content.contains("自动化") || content.contains("rule") {
                topics.insert("automation".to_string());
            }
        }

        (
            entities.into_iter().collect(),
            topics.into_iter().collect(),
        )
    }

    /// Build memory injection hint based on entities, topics, and health status.
    ///
    /// Creates a system prompt that guides the LLM to recall relevant context
    /// from long-term memory. This is a framework that can be extended with
    /// actual memory queries when a persistent memory service is integrated.
    fn build_memory_injection_hint(
        &self,
        entities: &[String],
        topics: &[String],
        health: &crate::context::ContextHealth,
    ) -> String {
        if entities.is_empty() && topics.is_empty() {
            return String::new();
        }

        let mut hint = String::from("[Memory Recall] ");

        // Add health-based guidance
        if let Some(recommendation) = health.recommendation() {
            hint.push_str(&format!("Context hint: {}. ", recommendation));
        }

        // Add entity recall hints
        if !entities.is_empty() {
            hint.push_str(&format!("Related entities: {}. ", entities.join(", ")));
        }

        // Add topic recall hints
        if !topics.is_empty() {
            hint.push_str(&format!("Previous topics discussed: {}. ", topics.join(", ")));
        }

        // Add guidance for memory retrieval
        hint.push_str("Consider any relevant information from earlier in the conversation that might help with the current request.");

        hint
    }

    // === SEMANTIC MAPPING METHODS ===

    /// Register a device in the semantic mapper for natural language resolution.
    /// This allows LLM to reference devices by name instead of technical ID.
    pub async fn register_semantic_device(&self, device: crate::context::Resource) {
        let _ = self.semantic_mapper.register_device(device).await;
    }

    /// Update devices in the semantic mapper from smart conversation devices.
    pub async fn update_semantic_devices(&self, devices: Vec<crate::smart_conversation::Device>) {
        for device in devices {
            let resource = crate::context::Resource::device(
                &device.id,
                &device.name,
                &device.device_type
            )
                .with_location(&device.location);

            let _ = self.semantic_mapper.register_device(resource).await;
        }
    }

    /// Register rules in the semantic mapper.
    pub async fn register_semantic_rules(&self, rules: Vec<(String, String, bool)>) {
        self.semantic_mapper.register_rules(rules).await;
    }

    /// Update rules in the semantic mapper from smart conversation rules.
    pub async fn update_semantic_rules(&self, rules: Vec<crate::smart_conversation::Rule>) {
        let rules_data: Vec<(String, String, bool)> = rules.into_iter()
            .map(|r| (r.id, r.name, r.enabled))
            .collect();
        self.semantic_mapper.register_rules(rules_data).await;
    }

    /// Register workflows in the semantic mapper.
    pub async fn register_semantic_workflows(&self, workflows: Vec<(String, String, bool)>) {
        self.semantic_mapper.register_workflows(workflows).await;
    }

    /// Get semantic context for inclusion in LLM prompt.
    /// This provides LLM with available resource names without technical IDs.
    pub async fn get_semantic_context(&self) -> String {
        self.semantic_mapper.get_semantic_context().await
    }

    /// Get semantic mapper statistics.
    pub async fn get_semantic_mapping_stats(&self) -> semantic_mapper::MappingStats {
        self.semantic_mapper.get_stats().await
    }

    /// === FAST PATH: Check for simple responses BEFORE acquiring lock ===
    /// This improves latency for common queries like greetings and confirmations.
    fn try_fast_path(&self, user_message: &str) -> Option<AgentResponse> {
        let trimmed = user_message.trim().to_lowercase();
        let start = std::time::Instant::now();

        // Greeting patterns
        let greeting_responses: &[(&str, &str)] = &[
            ("你好", "你好！我是 NeoMind 智能助手，有什么可以帮您？"),
            ("您好", "您好！我是 NeoMind 智能助手，有什么可以帮您？"),
            ("hi", "Hello! I'm NeoMind, your smart assistant. How can I help you?"),
            ("hello", "Hello! I'm NeoMind, your smart assistant."),
            ("早上好", "早上好！今天有什么可以帮您的？"),
            ("下午好", "下午好！有什么可以帮您的？"),
            ("晚上好", "晚上好！有什么可以帮您的？"),
        ];

        // Confirmation patterns
        let confirmation_responses: &[(&str, &str)] = &[
            ("好的", "好的，我明白了。"),
            ("好的，", "好的。"),
            ("明白", "好的，我明白了。"),
            ("明白了", "好的，我明白了。"),
            ("知道了", "好的，我知道了。"),
            ("收到", "好的，收到了。"),
            ("嗯", "好的，我明白了。"),
            ("行", "好的，没问题。"),
            ("是", "是的，我明白了。"),
            ("对", "是的，正确。"),
            ("ok", "OK!"),
            ("好的ok", "好的！"),
            ("谢谢", "不客气！还有其他需要帮助的吗？"),
            ("thanks", "You're welcome! Is there anything else I can help with?"),
        ];

        // Check greetings
        for (pattern, response) in greeting_responses.iter() {
            if trimmed == *pattern || trimmed.starts_with(*pattern) {
                return Some(AgentResponse {
                    message: AgentMessage::assistant(*response),
                    tool_calls: vec![],
                    memory_context_used: false,
                    tools_used: vec![],
                    processing_time_ms: start.elapsed().as_millis() as u64,
                });
            }
        }

        // Check confirmations
        for (pattern, response) in confirmation_responses.iter() {
            if trimmed == *pattern || trimmed.starts_with(*pattern) {
                return Some(AgentResponse {
                    message: AgentMessage::assistant(*response),
                    tool_calls: vec![],
                    memory_context_used: false,
                    tools_used: vec![],
                    processing_time_ms: start.elapsed().as_millis() as u64,
                });
            }
        }

        None
    }

    /// Process a user message with real LLM.
    /// Uses session-level lock to prevent concurrent requests on the same session.
    pub async fn process(&self, user_message: &str) -> Result<AgentResponse> {
        tracing::debug!(message = %user_message, "Agent::process starting");

        // === FAST PATH: Try simple responses WITHOUT acquiring lock ===
        if let Some(response) = self.try_fast_path(user_message) {
            // Save to history for context continuity
            let user_msg = AgentMessage::user(user_message);
            self.internal_state
                .write()
                .await
                .push_message(user_msg);
            self.internal_state
                .write()
                .await
                .push_message(response.message.clone());

            return Ok(response);
        }

        // === NORMAL PATH: Acquire lock for complex processing ===
        let _lock = self.process_lock.lock().await;

        let start = std::time::Instant::now();

        // === SMART FOLLOWUP INTERCEPTION (Context-Aware) ===
        // More advanced interception with conversation context awareness
        let followup_analysis = {
            let ctx = self.conversation_context.read().await;
            let mut followup = self.smart_followup.write().await;
            followup.analyze_input(user_message, &ctx)
        };

        // Handle smart followup cases
        if !followup_analysis.can_proceed {
            let response_content = if let Some(first_followup) = followup_analysis.followups.first() {
                // Use the highest priority followup
                let mut content = first_followup.question.clone();

                // Add suggestions if available
                if !first_followup.suggestions.is_empty() {
                    content.push_str("\n\n建议选项：");
                    for (i, suggestion) in first_followup.suggestions.iter().enumerate() {
                        content.push_str(&format!("\n{}. {}", i + 1, suggestion));
                    }
                }

                content
            } else {
                // Should not reach here, but fallback
                "我明白您的请求，但需要更多信息。".to_string()
            };

            // Save user message and our response to history
            let user_msg = AgentMessage::user(user_message);
            let response_msg = AgentMessage::assistant(&response_content);

            self.internal_state
                .write()
                .await
                .push_message(user_msg);
            self.internal_state
                .write()
                .await
                .push_message(response_msg.clone());

            return Ok(AgentResponse {
                message: response_msg,
                tool_calls: vec![],
                memory_context_used: true,
                tools_used: vec![],
                processing_time_ms: start.elapsed().as_millis() as u64,
            });
        }

        // === SMART CONVERSATION INTERCEPTION (Simple, Fallback) ===
        // Simple pattern-based interception for backward compatibility
        let smart_analysis = {
            let smart_conv = self.smart_conversation.read().await;
            smart_conv.analyze_input(user_message)
        };

        // Handle cases where we should intercept
        if !smart_analysis.can_proceed {
            let response_content = if let Some(question) = smart_analysis.missing_info {
                // Information missing - ask user
                format!("❓ {}", question)
            } else if let Some(confirm) = smart_analysis.requires_confirmation {
                // Dangerous operation - require confirmation
                format!("⚠️ {}", confirm)
            } else if let Some(clarify) = smart_analysis.ambiguous {
                // Intent unclear - ask for clarification
                format!("❓ {}", clarify)
            } else {
                // Should not reach here, but fallback
                "我明白您的请求，但需要更多信息。".to_string()
            };

            // Save user message and our response to history
            let user_msg = AgentMessage::user(user_message);
            let response_msg = AgentMessage::assistant(&response_content);

            self.internal_state
                .write()
                .await
                .push_message(user_msg);
            self.internal_state
                .write()
                .await
                .push_message(response_msg.clone());

            return Ok(AgentResponse {
                message: response_msg,
                tool_calls: vec![],
                memory_context_used: true,
                tools_used: vec![],
                processing_time_ms: start.elapsed().as_millis() as u64,
            });
        }

        // === INTENT ANALYSIS: Use ContextSelector to analyze user intent ===
        let (intent_analysis, _context_bundle) = self.analyze_intent(user_message).await;
        tracing::info!(
            intent = ?intent_analysis.intent_type,
            confidence = intent_analysis.confidence,
            entities = intent_analysis.entities.len(),
            scope = ?intent_analysis.context_scope,
            "Intent analysis completed"
        );

        // === PROCEED WITH NORMAL PROCESSING ===
        // === CONVERSATION CONTEXT: Enhance input with context ===
        // Try to resolve ambiguous commands and enhance with previous context
        let enhanced_input = {
            let ctx = self.conversation_context.read().await;
            // First, try to resolve ambiguous commands like "打开" -> "打开客厅的灯"
            if let Some(resolved) = ctx.resolve_ambiguous_command(user_message) {
                resolved
            } else {
                // Then enhance pronouns and add context
                ctx.enhance_input(user_message)
            }
        };

        // Add user message to history (use enhanced version for processing, but save original)
        let user_msg = AgentMessage::user(user_message);
        self.internal_state
            .write()
            .await
            .push_message(user_msg.clone());

        // Check if LLM is configured
        if !self.llm_interface.is_ready().await {
            // Fall back to simple keyword-based responses
            let (message, tool_calls, tools_used) =
                process_fallback(&self.tools, &self.fallback_rules, user_message).await;
            let processing_time = start.elapsed().as_millis() as u64;

            self.internal_state
                .write()
                .await
                .push_message(message.clone());

            return Ok(AgentResponse {
                message,
                tool_calls,
                memory_context_used: true,
                tools_used,
                processing_time_ms: processing_time,
            });
        }

        // === LLM PATH: Process with real LLM ===
        // Note: Fast path responses (greetings, confirmations) are handled in try_fast_path()
        // before acquiring the lock to improve latency.
        match self.process_with_llm(&enhanced_input).await {
            Ok(response) => {
                // === CONVERSATION CONTEXT: Update context after successful response ===
                {
                    let tool_results: Vec<(String, String)> = response.tool_calls
                        .iter()
                        .filter_map(|tc| {
                            tc.result.as_ref().map(|r| {
                                (tc.name.clone(), serde_json::to_string(r).unwrap_or_else(|_| "无结果".to_string()))
                            })
                        })
                        .collect();
                    let mut ctx = self.conversation_context.write().await;
                    ctx.update(user_message, &tool_results);
                }

                let processing_time = start.elapsed().as_millis() as u64;
                self.internal_state
                    .write()
                    .await
                    .session
                    .increment_messages();
                Ok(AgentResponse {
                    processing_time_ms: processing_time,
                    ..response
                })
            }
            Err(e) => {
                // On error, fall back to simple response
                tracing::error!(error = %e, "LLM error, using fallback");
                let (message, tool_calls, tools_used) =
                    process_fallback(&self.tools, &self.fallback_rules, user_message).await;
                let processing_time = start.elapsed().as_millis() as u64;

                self.internal_state
                    .write()
                    .await
                    .push_message(message.clone());

                Ok(AgentResponse {
                    message,
                    tool_calls,
                    memory_context_used: true,
                    tools_used,
                    processing_time_ms: processing_time,
                })
            }
        }
    }

    /// Process a user message with images (multimodal input).
    ///
    /// This method is used when the user sends images along with their text message.
    /// The images should be base64-encoded data URLs (e.g., "data:image/png;base64,...").
    pub async fn process_multimodal(
        &self,
        user_message: &str,
        images: Vec<String>, // Base64 data URLs
    ) -> Result<AgentResponse> {
        tracing::debug!(
            message = %user_message,
            image_count = images.len(),
            "Agent::process_multimodal starting"
        );

        // Create multimodal message content AND prepare images for storage
        let mut parts = vec![neomind_core::ContentPart::text(user_message)];
        let mut user_images = Vec::new();

        // Process images for both ContentPart and storage
        for image_data in &images {
            // Extract mime type from data URL
            let (mime_type_str, base64_part) = if image_data.starts_with("data:image/") {
                if let Some(pos) = image_data.find(',') {
                    let mime = if image_data.contains("data:image/png") {
                        "image/png"
                    } else if image_data.contains("data:image/jpeg") || image_data.contains("data:image/jpg") {
                        "image/jpeg"
                    } else if image_data.contains("data:image/webp") {
                        "image/webp"
                    } else if image_data.contains("data:image/gif") {
                        "image/gif"
                    } else {
                        "image/png"
                    };
                    (mime, &image_data[pos + 1..])
                } else {
                    ("image/png", image_data.as_str())
                }
            } else {
                ("image/png", image_data.as_str())
            };

            // Add to ContentPart for LLM
            parts.push(neomind_core::ContentPart::image_base64(
                base64_part,
                mime_type_str,
            ));

            // Add to storage as AgentMessageImage
            user_images.push(crate::agent::types::AgentMessageImage {
                data: image_data.clone(),
                mime_type: Some(mime_type_str.to_string()),
            });
        }

        let user_msg = neomind_core::Message::new(
            neomind_core::MessageRole::User,
            neomind_core::Content::Parts(parts),
        );

        // === Skip fast path for multimodal messages (always use LLM) ===
        let _lock = self.process_lock.lock().await;
        let start = std::time::Instant::now();

        // Add user message to history WITH images (for multimodal context in follow-up requests)
        let agent_user_msg = AgentMessage::user_with_images(user_message, user_images);
        self.internal_state
            .write()
            .await
            .push_message(agent_user_msg);

        // Check if LLM is configured (required for multimodal)
        if !self.llm_interface.is_ready().await {
            return Err(NeoMindError::Llm(
                "Multimodal input requires LLM support".to_string(),
            ));
        }

        // === Get conversation history ===
        // Optimize: Clone only needed messages in one pass
        let history_without_last: Vec<AgentMessage> = {
            let state = self.internal_state.read().await;
            let memory = &state.memory;
            if memory.len() > 1 {
                memory.iter().take(memory.len() - 1).cloned().collect()
            } else {
                Vec::new()
            }
        };

        // Convert AgentMessage history to Message history
        let core_history: Vec<neomind_core::Message> =
            history_without_last.iter().map(|msg| msg.to_core()).collect();

        // === Process with LLM using multimodal message ===
        match self.llm_interface.chat_multimodal_with_history(user_msg, &core_history).await {
            Ok(llm_response) => {
                let response_msg = AgentMessage::assistant(&llm_response.text);

                self.internal_state
                    .write()
                    .await
                    .push_message(response_msg.clone());

                self.internal_state
                    .write()
                    .await
                    .session.increment_messages();

                let processing_time = start.elapsed().as_millis() as u64;

                Ok(AgentResponse {
                    message: response_msg,
                    tool_calls: vec![],
                    memory_context_used: false,
                    tools_used: vec![],
                    processing_time_ms: processing_time,
                })
            }
            Err(e) => {
                Err(NeoMindError::Llm(format!("LLM processing failed: {}", e)))
            }
        }
    }

    /// Process a multimodal user message (text + images) with streaming response (returns AgentEvent stream).
    pub async fn process_multimodal_stream_events(
        &self,
        user_message: &str,
        images: Vec<String>, // Base64 data URLs
    ) -> Result<Pin<Box<dyn Stream<Item = AgentEvent> + Send>>> {
        tracing::debug!(
            message = %user_message,
            image_count = images.len(),
            "Agent::process_multimodal_stream_events starting"
        );

        let _lock = self.process_lock.lock().await;

        // Check if LLM is configured (required for multimodal)
        if !self.llm_interface.is_ready().await {
            // Fall back to simple response without LLM
            let (message, _, _) = process_fallback(&self.tools, &self.fallback_rules, user_message).await;
            self.internal_state
                .write()
                .await
                .push_message(message.clone());

            return Ok(Box::pin(async_stream::stream! {
                yield AgentEvent::content(message.content);
                yield AgentEvent::end();
            }));
        }

        match process_multimodal_stream_events(
            self.llm_interface.clone(),
            self.internal_state.clone(),
            self.tools.clone(),
            user_message,
            images,
        )
        .await
        {
            Ok(stream) => Ok(stream),
            Err(e) => {
                // On error, fall back to simple response
                tracing::error!(error = %e, "LLM multimodal stream error, using fallback");
                let (message, _, _) =
                    process_fallback(&self.tools, &self.fallback_rules, user_message).await;
                self.internal_state
                    .write()
                    .await
                    .push_message(message.clone());

                Ok(Box::pin(async_stream::stream! {
                    yield AgentEvent::content(message.content);
                    yield AgentEvent::end();
                }))
            }
        }
    }

    /// Process with real LLM.
    ///
    /// ## Safeguards:
    /// - Maximum tool calls per request limited to MAX_TOOL_CALLS_PER_REQUEST
    /// - Tool result clearing for old messages (Anthropic-style)
    /// - Token limit configured in ChatConfig
    async fn process_with_llm(&self, user_message: &str) -> Result<AgentResponse> {
        tracing::debug!(message = %user_message, "process_with_llm starting");
        use tool_parser::parse_tool_calls;

        // Get existing history (user message already added by caller in `process`)
        // Optimize: Clone only needed messages in one pass, avoiding double-clone
        let history_without_last: Vec<AgentMessage> = {
            let state = self.internal_state.read().await;
            let memory = &state.memory;
            if memory.len() > 1 {
                // Clone only what we need (skip last message)
                memory.iter().take(memory.len() - 1).cloned().collect()
            } else {
                Vec::new()
            }
        };

        // === DYNAMIC CONTEXT WINDOW: Get model's actual capacity ===
        // Query the LLM backend for the actual context window size.
        // Reserve space for: system prompt, user message, and generation
        let max_context = self.llm_interface.max_context_length().await;

        // Calculate space needed for non-history components
        // System prompt (~500 tokens) + user message (~200 tokens) + context injection (~200 tokens) + generation reserve (~1000 tokens)
        const NON_HISTORY_TOKENS: usize = 1900;
        let safe_max_context = max_context.saturating_sub(NON_HISTORY_TOKENS);

        // === P3.2: ADAPTIVE CONTEXT SIZING ===
        // Adjust context size based on conversation complexity:
        // - High entity diversity: +10%
        // - Multiple active topics: +10%
        // - Recent errors: +15%
        // - Simple greetings: -10%
        let adaptive_adjustment = calculate_adaptive_context_adjustment(&history_without_last);

        // Calculate effective max with adaptive adjustment, ensuring we don't exceed safe limit
        let effective_max = ((safe_max_context as f64) * adaptive_adjustment) as usize;

        // Enforce reasonable bounds (minimum 1024 tokens for history, maximum safe limit)
        let effective_max = effective_max.clamp(1024, safe_max_context);

        tracing::debug!(
            "Context window: model_capacity={}, safe_max={}, adjustment={:.2}, effective_max={}",
            max_context,
            safe_max_context,
            adaptive_adjustment,
            effective_max
        );

        // === ANTHROPIC-STYLE IMPROVEMENT: Apply context window with tool result clearing ===
        // This prevents context bloat from old tool calls while maintaining conversation continuity
        let compacted_history = build_context_window(&history_without_last, effective_max);

        tracing::debug!(
            "Context: {} messages -> {} messages (after compaction)",
            history_without_last.len(),
            compacted_history.len()
        );

        // Build history for LLM (convert AgentMessage to Message)
        let mut core_history: Vec<Message> =
            compacted_history.iter().map(|msg| msg.to_core()).collect();

        // === CONVERSATION CONTEXT: Inject context summary ONLY if it changed ===
        // This prevents repeatedly injecting the same context which can cause
        // the LLM to generate repetitive responses
        let context_summary = {
            let ctx = self.conversation_context.read().await;
            let summary = ctx.get_context_summary();
            if !summary.is_empty() {
                Some(format!("当前对话上下文：\n{}", summary))
            } else {
                None
            }
        };

        // Only inject context if it has changed since last time
        // Use a simple hash to detect changes
        if let Some(summary) = context_summary {
            use std::collections::hash_map::DefaultHasher;
            use std::hash::{Hash, Hasher};

            let summary_hash = {
                let mut h = DefaultHasher::new();
                summary.hash(&mut h);
                h.finish()
            };

            let mut last_hash = self.last_injected_context_hash.write().await;
            if *last_hash != summary_hash {
                // Context has changed, inject it
                *last_hash = summary_hash;
                drop(last_hash); // Release lock before proceeding

                use neomind_core::message::{Content, MessageRole};
                core_history.push(Message::new(MessageRole::System, Content::text(&summary)));
                tracing::debug!("Injected conversation context into LLM history (changed from previous)");
            } else {
                drop(last_hash); // Release lock
                tracing::debug!("Skipping context injection - unchanged from previous");
            }
        }

        // === P4.2: INTELLIGENT MEMORY INJECTION ===
        // Detect stale context and inject relevant long-term memory summaries
        // This improves context continuity in long conversations
        use crate::context::{calculate_health, ContextHealth};
        let health = calculate_health(&history_without_last);

        // Inject memory if context is degraded
        if health.needs_refresh() {
            tracing::debug!(
                "Context health degraded (score: {:.2}), injecting memory hints",
                health.overall_score
            );

            // Extract entities and topics from recent conversation
            let (entities, topics) = self.extract_conversation_entities_topics(&history_without_last);

            if !entities.is_empty() || !topics.is_empty() {
                // Build memory hint prompt
                let memory_hint = self.build_memory_injection_hint(&entities, &topics, &health);

                if !memory_hint.is_empty() {
                    use neomind_core::message::{Content, MessageRole};
                    core_history.push(Message::new(MessageRole::System, Content::text(&memory_hint)));
                    tracing::debug!("Injected memory hint into context");
                }
            }
        }

        // Call LLM with conversation history (user message will be added by LLM interface)
        let chat_response = self
            .llm_interface
            .chat_with_history(user_message, &core_history)
            .await
            .map_err(|e| super::error::NeoMindError::Llm(e.to_string()))?;

        // Parse response for tool calls
        tracing::debug!(response_text = %chat_response.text, "LLM response received");
        let (content, mut tool_calls) = parse_tool_calls(&chat_response.text)?;
        tracing::debug!(count = tool_calls.len(), "Parsed tool calls");
        for tc in &tool_calls {
            tracing::debug!(name = %tc.name, args = %tc.arguments, "  tool call");
        }

        // Extract thinking content if present
        let thinking = chat_response.thinking;

        // If no tool calls in response content, try parsing from thinking field
        // Some models (like qwen3 with thinking enabled) may put tool calls in thinking
        if tool_calls.is_empty()
            && let Some(ref thinking_content) = thinking
                && let Ok((_, thinking_tool_calls)) = parse_tool_calls(thinking_content)
                    && !thinking_tool_calls.is_empty() {
                        tracing::debug!("Found tool calls in thinking field, using them");
                        tool_calls = thinking_tool_calls;
                    }

        // If no tool calls, return the direct response
        if tool_calls.is_empty() {
            // Save assistant response with or without thinking
            let assistant_msg = if let Some(thinking_content) = thinking {
                // Apply cleanup to thinking if it's too long
                let cleaned_thinking = if thinking_content.len() > 200 {
                    crate::agent::streaming::cleanup_thinking_content(&thinking_content)
                } else {
                    thinking_content
                };
                AgentMessage::assistant_with_thinking(&content, &cleaned_thinking)
            } else {
                AgentMessage::assistant(&content)
            };

            // === SAFEGUARD: Register response for cross-turn repetition detection ===
            {
                let mut state = self.internal_state.write().await;
                state.register_response(&content);
                state.push_message(assistant_msg.clone());
            }

            return Ok(AgentResponse {
                message: assistant_msg,
                tool_calls: vec![],
                memory_context_used: true,
                tools_used: vec![],
                processing_time_ms: 0,
            });
        }

        // === SAFEGUARD: Limit number of tool calls to prevent infinite loops ===
        let max_calls = self.config.max_tool_calls;
        if tool_calls.len() > max_calls {
            tracing::warn!(
                "Too many tool calls ({}) in single request, limiting to {}",
                tool_calls.len(),
                max_calls
            );
            tool_calls.truncate(max_calls);
        }

        // === DEDUPLICATE: Remove duplicate tool calls to avoid redundant execution ===
        // Models sometimes output the same tool call multiple times
        // We keep the first occurrence of each unique (name, arguments) pair
        let original_count = tool_calls.len();
        let mut seen = std::collections::HashSet::new();
        tool_calls.retain(|tool_call| {
            // Create a unique key based on tool name and arguments
            let key = (
                tool_call.name.clone(),
                tool_call.arguments.to_string().chars().take(100).collect::<String>()
            );
            seen.insert(key)
        });
        let dedup_count = tool_calls.len();
        if original_count > dedup_count {
            tracing::info!(
                "Deduplicated tool calls: {} -> {} (removed {} duplicates)",
                original_count,
                dedup_count,
                original_count - dedup_count
            );
        }

        // Tool calls detected - DON'T save the initial assistant message yet
        // We'll save a complete message (with tool_calls and final response) after tool execution

        // Execute tools in PARALLEL for better performance
        // Independent tools can run simultaneously, dependent tools wait for results
        let mut tool_results = Vec::new();
        let mut tools_used = Vec::new();
        let mut tool_calls_with_results = Vec::new();

        // Clone tool_calls for parallel execution
        let tool_calls_clone = tool_calls.clone();

        // Use futures for parallel execution
        let futures: Vec<_> = tool_calls_clone
            .into_iter()
            .map(|tool_call| {
                let name = tool_call.name.clone();
                let arguments = tool_call.arguments.clone();
                let id = tool_call.id.clone();

                // Spawn each tool execution as a separate task
                async move {
                    let result = self.execute_tool(&name, &arguments).await;
                    (name, id, arguments, result)
                }
            })
            .collect();

        // Execute all tools in parallel and wait for completion
        let results = futures::future::join_all(futures).await;

        // Process results in original order
        for (name, id, arguments, result) in results {
            tracing::debug!(name = %name, result = ?result, "Tool execution result");
            match result {
                Ok(ok_result) => {
                    tools_used.push(name.clone());
                    tracing::debug!(name = %name, count = tools_used.len(), "Added to tools_used");
                    tool_results.push((name.clone(), ok_result.clone()));
                    tool_calls_with_results.push(ToolCall {
                        name,
                        id,
                        arguments,
                        result: Some(serde_json::json!(ok_result)),
                    });
                }
                Err(e) => {
                    let error_msg = format!("Error: {}", e);
                    tool_results.push((name.clone(), error_msg.clone()));
                    tool_calls_with_results.push(ToolCall {
                        name,
                        id,
                        arguments,
                        result: Some(serde_json::json!({ "error": error_msg })),
                    });
                }
            }
        }

        tracing::debug!(tools_used = ?tools_used, "Before format_tool_results");

        // Format tool results directly (without calling LLM again)
        // This prevents excessive thinking and model looping
        let final_text = crate::agent::streaming::format_tool_results(&tool_results);

        // Save a complete message with tool_calls, results, and optionally thinking
        let final_message = if let Some(thinking_content) = thinking {
            // Clean up thinking if it's too long
            let cleaned_thinking = if thinking_content.len() > 200 {
                crate::agent::streaming::cleanup_thinking_content(&thinking_content)
            } else {
                thinking_content
            };
            AgentMessage::assistant_with_tools_and_thinking(
                &final_text,
                tool_calls_with_results,
                &cleaned_thinking,
            )
        } else {
            AgentMessage::assistant_with_tools(&final_text, tool_calls_with_results)
        };
        self.internal_state
            .write()
            .await
            .push_message(final_message.clone());

        Ok(AgentResponse {
            message: final_message,
            tool_calls,
            memory_context_used: true,
            tools_used,
            processing_time_ms: 0,
        })
    }

    /// Map simplified parameter names to actual tool parameter names.
    ///
    /// This bridges the gap between the user-friendly simplified interface
    /// and the actual tool implementation parameters.
    fn map_simplified_parameters(&self, tool_name: &str, arguments: &Value) -> Value {
        if let Some(args_obj) = arguments.as_object() {
            // Special handling for create_rule: convert simplified (name, condition, action) to DSL
            if tool_name == "create_rule" || tool_name == "rule.from_context" {
                // Check if we have simplified parameters (condition + action) but no dsl
                let has_condition = args_obj.contains_key("condition");
                let has_action = args_obj.contains_key("action");
                let has_description = args_obj.contains_key("description");
                let has_dsl = args_obj.contains_key("dsl");

                if (has_condition || has_description) && !has_dsl {
                    // Convert simplified parameters to DSL
                    let name = args_obj.get("name")
                        .and_then(|v| v.as_str())
                        .unwrap_or("未命名规则");

                    let dsl = if has_description {
                        // rule.from_context: extract structured rule from description
                        let description = args_obj.get("description")
                            .and_then(|v| v.as_str())
                            .unwrap_or("");

                        // Try to parse the description to extract condition/action
                        // For now, generate a simple DSL with the description as context
                        format!(r#"RULE "{name}"
WHEN sensor.temperature > 30
DO
  NOTIFY "{description}"
END"#)
                    } else if has_condition && has_action {
                        // create_rule with simplified condition/action
                        let condition = args_obj.get("condition")
                            .and_then(|v| v.as_str())
                            .unwrap_or("sensor.temperature > 30");

                        let action = args_obj.get("action")
                            .and_then(|v| v.as_str())
                            .unwrap_or("通知管理员");

                        format!(r#"RULE "{name}"
WHEN {condition}
DO
  NOTIFY "{action}"
END"#)
                    } else {
                        // Fallback: just use the name
                        format!(r#"RULE "{name}"
WHEN sensor.temperature > 30
DO
  NOTIFY "规则触发"
END"#)
                    };

                    let mut mapped = serde_json::Map::new();
                    mapped.insert("name".to_string(), serde_json::json!(name));
                    mapped.insert("dsl".to_string(), serde_json::json!(dsl));

                    // Include description if present
                    if let Some(desc) = args_obj.get("description") {
                        mapped.insert("description".to_string(), desc.clone());
                    }

                    return serde_json::Value::Object(mapped);
                }
            }

            // Standard parameter mapping for other tools
            let mut mapped = serde_json::Map::new();

            for (key, value) in args_obj {
                // Map simplified names to actual parameter names based on tool
                let actual_key = match (tool_name, key.as_str()) {
                    // query_data mappings
                    ("query_data", "device") => "device_id",
                    ("query_data", "hours") => {
                        // Convert hours to start_time timestamp
                        if let Some(hours) = value.as_i64() {
                            let end_time = chrono::Utc::now().timestamp();
                            let start_time = end_time - (hours * 3600);
                            mapped.insert("end_time".to_string(), serde_json::json!(end_time));
                            mapped.insert("start_time".to_string(), serde_json::json!(start_time));
                            continue;
                        }
                        "start_time"
                    }
                    ("query_data", other) => other,

                    // control_device mappings - FIXED: action maps to command for real tool
                    ("control_device", "device") => "device_id",
                    ("control_device", "action") => "command",
                    ("control_device", "value") => "parameters",
                    ("control_device", other) => other,

                    // device.control mappings (simplified name, same as control_device)
                    ("device.control", "device") => "device_id",
                    ("device.control", "action") => "command",
                    ("device.control", "value") => "parameters",
                    ("device.control", other) => other,

                    // create_rule mappings (only used if DSL is already provided)
                    ("create_rule", "name") => "name",
                    ("create_rule", "dsl") => "dsl",
                    ("create_rule", "description") => "description",
                    ("create_rule", other) => other,

                    // disable_rule / enable_rule: rule -> rule_id
                    ("disable_rule", "rule") | ("enable_rule", "rule") => "rule_id",
                    ("disable_rule", other) | ("enable_rule", other) => other,

                    // list_devices mappings
                    ("list_devices", "type") => "device_type",
                    ("list_devices", "status") => "status",
                    ("list_devices", other) => other,

                    // list_rules mappings
                    ("list_rules", _) => key,

                    // Default: keep original key
                    (_, other) => other,
                };

                mapped.insert(actual_key.to_string(), value.clone());
            }

            serde_json::Value::Object(mapped)
        } else {
            arguments.clone()
        }
    }

    /// Map simplified tool names to real tool names.
    ///
    /// Simplified names are used in LLM prompts (e.g., "device.discover")
    /// while real names are used in ToolRegistry (e.g., "list_devices").
    ///
    /// This now uses the unified `ToolNameMapper` to ensure consistency
    /// across the codebase.
    fn resolve_tool_name(&self, simplified_name: &str) -> String {
        // Delegate to the unified mapper
        crate::tools::resolve_tool_name(simplified_name)
    }

    /// Sanitize tool output for LLM consumption.
    ///
    /// If the output is too large (e.g., base64 images, large files),
    /// intelligently truncates while preserving useful structure.
    ///
    /// ## Strategy
    /// - **Small outputs** (<10KB): Return as-is
    /// - **Base64 strings**: Replace with metadata only (LLM can't use them)
    /// - **Large strings**: Truncate with preview
    /// - **Arrays**: Keep first N items + count of remaining
    /// - **Objects**: Keep small fields, truncate large fields
    ///
    /// This preserves data structure while avoiding token waste on unusable data.
    fn sanitize_tool_output_for_llm(
        &self,
        tool_name: &str,
        data: &serde_json::Value,
    ) -> String {
        const MAX_SIZE: usize = 10_240; // 10KB
        const MAX_PREVIEW_ITEMS: usize = 5; // Keep first 5 array items
        const MAX_STRING_PREVIEW: usize = 500; // 500 chars for string preview

        // Recursively truncate value to fit within MAX_SIZE
        let truncated = self.truncate_value(data, MAX_SIZE, MAX_PREVIEW_ITEMS, MAX_STRING_PREVIEW);

        let result = serde_json::to_string(&truncated).unwrap_or_else(|_| "null".to_string());
        let original_size = serde_json::to_string(data).unwrap_or_default().len();

        // Log if truncation occurred
        if result.len() != original_size {
            tracing::warn!(
                session_id = %self.session_id,
                tool = %tool_name,
                original_bytes = original_size,
                truncated_bytes = result.len(),
                "Tool output truncated for LLM"
            );
        }

        result
    }

    /// Recursively truncate a JSON value to fit within size constraints.
    ///
    /// This preserves data structure while trimming large content.
    fn truncate_value(
        &self,
        value: &serde_json::Value,
        max_size: usize,
        max_items: usize,
        max_string: usize,
    ) -> serde_json::Value {
        match value {
            // Base64 detection: long string with only base64 chars
            serde_json::Value::String(s) if s.len() > 500 && self.is_likely_base64(s) => {
                // Replace base64 with metadata
                serde_json::json!({
                    "_truncated": true,
                    "_type": "base64_data",
                    "size_bytes": s.len()
                })
            }

            // Regular long string - truncate with preview
            serde_json::Value::String(s) if s.len() > max_string => {
                let preview: String = s.chars().take(max_string).collect();
                serde_json::json!({
                    "_truncated": true,
                    "_original_length": s.len(),
                    "preview": format!("{}...", preview)
                })
            }

            // Array - keep first N items + count
            serde_json::Value::Array(arr) => {
                if arr.len() <= max_items {
                    // Check each element recursively
                    let truncated: Vec<serde_json::Value> = arr.iter()
                        .map(|v| self.truncate_value(v, max_size / arr.len().max(1), max_items, max_string))
                        .collect();
                    serde_json::Value::Array(truncated)
                } else {
                    let kept: Vec<serde_json::Value> = arr.iter()
                        .take(max_items)
                        .map(|v| self.truncate_value(v, max_size / max_items, max_items, max_string))
                        .collect();

                    serde_json::json!({
                        "_truncated": true,
                        "_total_count": arr.len(),
                        "_showing_first": max_items,
                        "items": kept
                    })
                }
            }

            // Object - process each field, truncate large ones
            serde_json::Value::Object(obj) => {
                let mut result = serde_json::Map::new();

                for (key, val) in obj.iter() {
                    // Skip known large fields (like full data content)
                    if matches!(key.as_str(), "data" | "content" | "base64" | "image")
                        && serde_json::to_string(val).unwrap_or_default().len() > 1000
                    {
                        result.insert(
                            format!("_{}_truncated", key),
                            serde_json::json!({
                                "_size_bytes": serde_json::to_string(val).unwrap_or_default().len(),
                                "_note": "Large data omitted"
                            })
                        );
                        continue;
                    }

                    let truncated = self.truncate_value(val, max_size / obj.len().max(1), max_items, max_string);
                    result.insert(key.clone(), truncated);

                    // Early exit if we're getting too big
                    if serde_json::to_string(&result).unwrap_or_default().len() > max_size {
                        break;
                    }
                }

                if !result.is_empty() {
                    serde_json::Value::Object(result)
                } else {
                    serde_json::json!({"_truncated": true, "_note": "All fields were too large"})
                }
            }

            // Other types pass through
            _ => value.clone(),
        }
    }

    /// Check if a string is likely base64 encoded data.
    fn is_likely_base64(&self, s: &str) -> bool {
        // Base64 contains only these chars
        let base64_chars = s.chars().take(1000).all(|c| {
            c.is_alphanumeric() || c == '+' || c == '/' || c == '=' || c == '\n' || c == '\r'
        });
        // Must be reasonably long and look like base64
        base64_chars && s.len() > 100
    }

    /// Execute a tool with retry logic.
    ///
    /// Retries up to 2 times for transient errors (network issues, timeouts).
    /// Returns a user-friendly error message if all retries fail.
    ///
    /// ## Production-ready error context:
    /// - Includes tool name, arguments, session ID for traceability
    /// - Categorizes errors (transient, validation, execution, timeout)
    /// - Logs detailed error information for debugging
    async fn execute_tool(&self, name: &str, arguments: &Value) -> Result<String> {
        const MAX_RETRIES: u32 = 2;
        let start = std::time::Instant::now();

        // Map simplified tool name to real tool name
        let real_tool_name = self.resolve_tool_name(name);

        // Convert simplified parameter names to actual tool parameters
        let mapped_arguments = self.map_simplified_parameters(name, arguments);

        // === SEMANTIC MAPPING: Convert natural language to technical IDs ===
        // This maps "客厅灯" -> "light_living_main" for device_id parameters
        let semantically_mapped = self.semantic_mapper
            .map_tool_parameters(&real_tool_name, mapped_arguments.clone())
            .await
            .unwrap_or(mapped_arguments);

        // Sanitize arguments for logging (limit size to avoid log spam)
        let args_preview = if semantically_mapped.to_string().len() > 200 {
            format!("{}...", &semantically_mapped.to_string().chars().take(200).collect::<String>())
        } else {
            semantically_mapped.to_string()
        };

        tracing::debug!(
            session_id = %self.session_id,
            tool = %real_tool_name,
            arguments = %args_preview,
            "Executing tool"
        );

        for attempt in 0..=MAX_RETRIES {
            match self.tools.execute(&real_tool_name, semantically_mapped.clone()).await {
                Ok(output) => {
                    let elapsed = start.elapsed();

                    // Check if tool execution itself failed
                    if !output.success {
                        let error_msg = output.error.unwrap_or_else(|| "Unknown error".to_string());

                        // Log detailed error with context
                        tracing::error!(
                            session_id = %self.session_id,
                            tool = %real_tool_name,
                            arguments = %args_preview,
                            error = %error_msg,
                            attempt = attempt,
                            elapsed_ms = elapsed.as_millis(),
                            error_category = "tool_execution_failed",
                            "Tool execution returned failure"
                        );

                        // Don't retry on logical errors (like invalid input)
                        return Ok(format!(
                            "工具 {} 执行失败: {}",
                            real_tool_name, error_msg
                        ));
                    }

                    tracing::debug!(
                        session_id = %self.session_id,
                        tool = %real_tool_name,
                        elapsed_ms = elapsed.as_millis(),
                        "Tool executed successfully"
                    );

                    // Sanitize output to avoid sending large data (base64, files, etc.) to LLM
                    return Ok(self.sanitize_tool_output_for_llm(&real_tool_name, &output.data));
                }
                Err(e) => {
                    let last_error = e.to_string();
                    let elapsed = start.elapsed();

                    // Categorize the error for better debugging
                    let error_category = if last_error.contains("not_found")
                        || last_error.contains("unknown") {
                        "tool_not_found"
                    } else if last_error.contains("timeout") {
                        "timeout"
                    } else if last_error.contains("network")
                        || last_error.contains("connection") {
                        "network"
                    } else if last_error.contains("parse")
                        || last_error.contains("invalid") {
                        "validation"
                    } else {
                        "unknown"
                    };

                    // Check if error is transient (worth retrying)
                    let is_transient = matches!(error_category, "timeout" | "network");

                    tracing::warn!(
                        session_id = %self.session_id,
                        tool = %real_tool_name,
                        arguments = %args_preview,
                        error = %last_error,
                        attempt = attempt,
                        elapsed_ms = elapsed.as_millis(),
                        error_category = %error_category,
                        is_transient = is_transient,
                        "Tool execution error"
                    );

                    if is_transient && attempt < MAX_RETRIES {
                        // Exponential backoff: 100ms, 200ms
                        let delay_ms = 100 * (2_u64.pow(attempt));
                        tokio::time::sleep(std::time::Duration::from_millis(delay_ms)).await;
                        continue;
                    }
                }
            }
        }

        // All retries failed - return detailed error with context
        let elapsed = start.elapsed();
        tracing::error!(
            session_id = %self.session_id,
            tool = %real_tool_name,
            arguments = %args_preview,
            elapsed_ms = elapsed.as_millis(),
            max_retries = MAX_RETRIES,
            error_category = "all_retries_failed",
            "Tool execution failed after all retries"
        );

        Err(super::error::NeoMindError::Tool(format!(
            "工具 {} 执行失败 (session: {}, 尝试: {}次, 耗时: {}ms)",
            real_tool_name,
            self.session_id,
            MAX_RETRIES + 1,
            elapsed.as_millis()
        )))
    }

    /// Process a tool call result.
    pub async fn process_tool_result(
        &self,
        tool_call_id: &str,
        result: &str,
    ) -> Result<AgentResponse> {
        // Add tool result to history
        let tool_msg = AgentMessage::tool_result(tool_call_id, result);
        self.internal_state.write().await.push_message(tool_msg);

        // Get LLM response based on tool result
        let response_content = format!("工具执行完成。结果: {}", result);

        let response = AgentMessage::assistant(response_content);
        self.internal_state
            .write()
            .await
            .push_message(response.clone());

        Ok(AgentResponse {
            message: response,
            tool_calls: Vec::new(),
            memory_context_used: true,
            tools_used: Vec::new(),
            processing_time_ms: 0,
        })
    }

    /// Process a user message with streaming response (returns AgentEvent stream).
    pub async fn process_stream_events(
        &self,
        user_message: &str,
    ) -> Result<Pin<Box<dyn Stream<Item = AgentEvent> + Send>>> {
        // Add user message to history
        let user_msg = AgentMessage::user(user_message);
        self.internal_state.write().await.push_message(user_msg);

        // Check if LLM is configured
        if !self.llm_interface.is_ready().await {
            // Fall back to simple response
            let (message, _, _) =
                process_fallback(&self.tools, &self.fallback_rules, user_message).await;
            self.internal_state
                .write()
                .await
                .push_message(message.clone());

            // Return a single-item stream with the fallback response
            let content = message.content;
            return Ok(Box::pin(async_stream::stream! {
                yield AgentEvent::content(content);
                yield AgentEvent::end();
            }));
        }

        match process_stream_events(
            self.llm_interface.clone(),
            self.internal_state.clone(),
            self.tools.clone(),
            user_message,
        )
        .await
        {
            Ok(stream) => Ok(stream),
            Err(e) => {
                // On error, fall back to simple response
                tracing::error!(error = %e, "LLM stream error, using fallback");
                let (message, _, _) =
                    process_fallback(&self.tools, &self.fallback_rules, user_message).await;
                self.internal_state
                    .write()
                    .await
                    .push_message(message.clone());

                Ok(Box::pin(async_stream::stream! {
                    yield AgentEvent::content(message.content);
                    yield AgentEvent::end();
                }))
            }
        }
    }

    /// Process a user message with streaming response (legacy, returns String stream).
    pub async fn process_stream(
        &self,
        user_message: &str,
    ) -> Result<Pin<Box<dyn Stream<Item = String> + Send>>> {
        let event_stream = self.process_stream_events(user_message).await?;
        Ok(events_to_string_stream(event_stream))
    }
}

/// Drop implementation for Agent to log session lifecycle for observability.
///
/// This helps with production debugging by tracking:
/// - When sessions are destroyed
/// - Session duration and message count
/// - Resource cleanup verification
impl Drop for Agent {
    fn drop(&mut self) {
        // Note: This is a synchronous drop, so we can't access the async internal_state
        // However, we can log basic information about the session being destroyed

        tracing::info!(
            session_id = %self.session_id,
            agent_name = %self.config.name,
            model = %self.config.model,
            tools_count = self.tools.list().len(),
            "Agent instance dropped (session destroyed)"
        );
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use neomind_tools::{Tool, Result, ToolOutput, ToolError};
    use serde_json::json;

    /// Simple mock list_devices tool for testing
    struct MockListDevicesTool;
    #[async_trait::async_trait]
    impl Tool for MockListDevicesTool {
        fn name(&self) -> &str { "list_devices" }
        fn description(&self) -> &str { "List all devices (mock for testing)" }
        fn parameters(&self) -> serde_json::Value { json!({}) }
        async fn execute(&self, _args: serde_json::Value) -> Result<ToolOutput> {
            let data = json!({"devices": [{"id": "mock_device_1", "name": "Mock Device"}]});
            Ok(ToolOutput::success(data))
        }
    }

    /// Simple mock list_rules tool for testing
    struct MockListRulesTool;
    #[async_trait::async_trait]
    impl Tool for MockListRulesTool {
        fn name(&self) -> &str { "list_rules" }
        fn description(&self) -> &str { "List all rules (mock for testing)" }
        fn parameters(&self) -> serde_json::Value { json!({}) }
        async fn execute(&self, _args: serde_json::Value) -> Result<ToolOutput> {
            let data = json!({"rules": [{"id": "mock_rule_1", "name": "Mock Rule"}]});
            Ok(ToolOutput::success(data))
        }
    }

    /// Simple mock query_data tool for testing
    struct MockQueryDataTool;
    #[async_trait::async_trait]
    impl Tool for MockQueryDataTool {
        fn name(&self) -> &str { "query_data" }
        fn description(&self) -> &str { "Query metric data (mock for testing)" }
        fn parameters(&self) -> serde_json::Value { json!({"type": "object"}) }
        async fn execute(&self, _args: serde_json::Value) -> Result<ToolOutput> {
            let data = json!({"data": [{"metric": "temperature", "value": 25.5}]});
            Ok(ToolOutput::success(data))
        }
    }

    /// Simple mock greet tool for testing
    struct MockGreetTool;
    #[async_trait::async_trait]
    impl Tool for MockGreetTool {
        fn name(&self) -> &str { "greet" }
        fn description(&self) -> &str { "Greet the user (mock for testing)" }
        fn parameters(&self) -> serde_json::Value { json!({}) }
        async fn execute(&self, _args: serde_json::Value) -> Result<ToolOutput> {
            let data = json!({"message": "Hello there!"});
            Ok(ToolOutput::success(data))
        }
    }

    /// Create a test agent with mock tools registered
    fn create_test_agent_with_mocks(session_id: String) -> Agent {
        use neomind_tools::ToolRegistryBuilder;

        let mut registry = ToolRegistryBuilder::new().build();

        // Register mock tools
        registry.register(std::sync::Arc::new(MockListDevicesTool));
        registry.register(std::sync::Arc::new(MockListRulesTool));
        registry.register(std::sync::Arc::new(MockQueryDataTool));
        registry.register(std::sync::Arc::new(MockGreetTool));

        // Add default agent tools
        use crate::tools::{ThinkTool, ToolSearchTool};
        use crate::tools::{AskUserTool, ConfirmActionTool, ClarifyIntentTool};

        let tool_search = ToolSearchTool::from_definitions(&[]);
        registry.register(std::sync::Arc::new(tool_search));

        let think_tool = ThinkTool::new();
        registry.register(std::sync::Arc::new(think_tool));

        let ask_user_tool = AskUserTool::new();
        registry.register(std::sync::Arc::new(ask_user_tool));

        let confirm_tool = ConfirmActionTool::new();
        registry.register(std::sync::Arc::new(confirm_tool));

        let clarify_tool = ClarifyIntentTool::new();
        registry.register(std::sync::Arc::new(clarify_tool));

        Agent::with_tools(AgentConfig::default(), session_id, std::sync::Arc::new(registry))
    }

    #[tokio::test]
    async fn test_agent_creation() {
        let agent = Agent::with_session("test_session".to_string());
        assert_eq!(agent.session_id(), "test_session");

        let state = agent.state().await;
        assert_eq!(state.id, "test_session");
    }

    #[tokio::test]
    async fn test_agent_history() {
        let agent = Agent::with_session("test_session".to_string());

        // Initially empty
        assert!(agent.history().await.is_empty());

        // Clear should work
        agent.clear_history().await;
        assert!(agent.history().await.is_empty());
    }

    #[tokio::test]
    async fn test_available_tools() {
        let agent = create_test_agent_with_mocks("test_session".to_string());
        let tools = agent.available_tools();

        assert!(!tools.is_empty());
        assert!(tools.contains(&"list_devices".to_string()));
        assert!(tools.contains(&"list_rules".to_string()));
    }

    #[tokio::test]
    async fn test_process_fallback() {
        let agent = create_test_agent_with_mocks("test_session".to_string());
        let response = agent.process("列出所有设备").await.unwrap();

        assert!(response.message.content.contains("设备"));
        assert!(response.tools_used.contains(&"list_devices".to_string()));
    }

    #[tokio::test]
    async fn test_process_list_rules() {
        let agent = create_test_agent_with_mocks("test_session".to_string());
        let response = agent.process("列出规则").await.unwrap();

        assert!(response.message.content.contains("规则"));
        assert!(response.tools_used.contains(&"list_rules".to_string()));
    }

    #[tokio::test]
    async fn test_process_query_data() {
        let agent = create_test_agent_with_mocks("test_session".to_string());
        let response = agent.process("查询温度数据").await.unwrap();

        assert!(response.message.content.contains("数据"));
        assert!(response.tools_used.contains(&"query_data".to_string()));
    }

    #[tokio::test]
    async fn test_process_default() {
        let agent = Agent::with_session("test_session".to_string());
        let response = agent.process("你好").await.unwrap();

        // Should get a helpful response
        assert!(!response.message.content.is_empty());
    }

    #[tokio::test]
    async fn test_history_persistence() {
        let agent = Agent::with_session("test_session".to_string());

        // Send a message
        agent.process("列出设备").await.unwrap();

        // Check history
        let history = agent.history().await;
        assert!(history.len() >= 2); // user + assistant
    }

    #[tokio::test]
    async fn test_custom_fallback_rules() {
        // Test custom rules with mock greet tool
        // Note: Use a keyword that doesn't match fast-path greetings
        let custom_rules = vec![
            FallbackRule::new(vec!["greet", "greeting"], "greet")
                .with_response_template("Greeting from fallback!"),
        ];
        let agent = create_test_agent_with_mocks("test_session".to_string())
            .with_fallback_rules(custom_rules);

        // Use "greet me" which won't match fast-path patterns
        let response = agent.process("greet me").await.unwrap();
        // The greet tool should be used
        assert!(response.tools_used.contains(&"greet".to_string()),
            "Expected 'greet' in tools_used, got: {:?}", response.tools_used);
    }
}
