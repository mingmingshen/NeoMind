# NeoTalk æ”¹è¿›ä¼˜å…ˆçº§å·¥ä½œè®¡åˆ’

## æ–‡æ¡£ä¿¡æ¯

| é¡¹ç›® | å€¼ |
|------|-----|
| åˆ›å»ºæ—¥æœŸ | 2026-01-30 |
| åŸºå‡†é¡¹ç›® | moltbot/moltbot |
| é¢„è®¡å·¥æœŸ | 8-9 å‘¨ |
| å½“å‰ç‰ˆæœ¬ | NeoTalk v0.x |

---

## ä¸€ã€é—®é¢˜åˆ†ææ€»ç»“

### 1.1 å½“å‰è¶…æ—¶å’Œä¸­æ–­é—®é¢˜

**é—®é¢˜æ—¥å¿—**ï¼š
```
2026-01-30T01:59:30.532226Z [ollama.rs] Max thinking chars reached (10001). Skipping remaining thinking chunks, waiting for content.
2026-01-30T02:00:44.223375Z Stream timeout after 120s
```

**é—®é¢˜æ ¹æº**ï¼š

| é…ç½®é¡¹ | å½“å‰å€¼ | ä½ç½® | é—®é¢˜ |
|--------|--------|------|------|
| `MAX_THINKING_CHARS` | 10,000 | `crates/llm/src/backends/ollama.rs:30` | ç¡¬ç¼–ç ï¼Œè¶…è¿‡åè·³è¿‡æ€è€ƒå†…å®¹ |
| `MAX_THINKING_TIME_SECS` | 60 | `crates/llm/src/backends/ollama.rs:37` | ç¡¬ç¼–ç ï¼Œæ€è€ƒè¶…æ—¶ |
| `max_stream_duration` | 120s | `crates/agent/src/agent/streaming.rs:50` | æµå¼ä¼ è¾“æ€»è¶…æ—¶ |
| `stream_timeout` | 120s | `crates/api/src/handlers/sessions.rs:38` | APIä¼šè¯è¶…æ—¶ |

**é—®é¢˜é“¾**ï¼š
1. qwen3-vl:2b ç”Ÿæˆæ€è€ƒå†…å®¹è¶…è¿‡ 10,000 å­—ç¬¦
2. è¶…è¿‡åè·³è¿‡å‰©ä½™æ€è€ƒï¼Œç­‰å¾…å†…å®¹ç”Ÿæˆ
3. æ€è€ƒ + å†…å®¹ç”Ÿæˆæ€»æ—¶é•¿è¶…è¿‡ 120 ç§’
4. æ— çŠ¶æ€ä¿å­˜æœºåˆ¶ï¼Œä»»åŠ¡ç›´æ¥ä¸­æ–­ä¸¢å¤±

### 1.2 ä¸ Moltbot å¯¹æ¯”

| ç‰¹æ€§ | Moltbot | NeoTalk | å·®è· |
|------|---------|---------|------|
| **ä¸Šä¸‹æ–‡å‹ç¼©** | `reserveTokensFloor`, `maxHistoryShare`, `softThresholdTokens` | æ— é…ç½® | âŒ æ— ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ |
| **è®°å¿†åˆ·æ–°** | `memoryFlush.prompt` è§¦å‘æŒä¹…åŒ– | æ— è‡ªåŠ¨åˆ·æ–° | âŒ ä¸­æ–­åçŠ¶æ€ä¸¢å¤± |
| **æ€è€ƒé™åˆ¶** | å¯é…ç½® per-agent | ç¡¬ç¼–ç  10,000 | âŒ æ— æ³•è°ƒæ•´ |
| **è¶…æ—¶å¤„ç†** | åˆ†é˜¶æ®µè­¦å‘Š + çŠ¶æ€ä¿å­˜ | ç¡¬è¶…æ—¶ä¸­æ–­ | âŒ æ— æ¢å¤æœºåˆ¶ |
| **åµŒå…¥æ¨¡å‹** | OpenAI/Gemini æ‰¹å¤„ç† | SimpleHash å‡åµŒå…¥ | âŒ æ— è¯­ä¹‰æœç´¢ |
| **å…¨æ–‡æœç´¢** | FTS5 + BM25 | æ—  | âŒ æ— å…³é”®è¯æœç´¢ |
| **æ··åˆæœç´¢** | Vector + BM25 èåˆ | æ—  | âŒ æœç´¢ç²¾åº¦ä½ |
| **åµŒå…¥ç¼“å­˜** | LRU + æŒä¹…åŒ– | æ—  | âŒ é‡å¤è®¡ç®— |

---

## äºŒã€ä¼˜å…ˆçº§å·¥ä½œè®¡åˆ’

### ğŸ”´ P0 - ç´§æ€¥ä¿®å¤ï¼ˆè¶…æ—¶ä¸ä¸­æ–­é—®é¢˜ï¼‰

> **é¢„è®¡å·¥æœŸ**: 1-3 å‘¨
> **é¢„æœŸæ•ˆæœ**: è¶…æ—¶ç‡ä» 30% é™è‡³ <5%ï¼Œä¸­æ–­åå¯æ¢å¤

#### P0.1 é…ç½®åŒ–æ€è€ƒé™åˆ¶å’Œè¶…æ—¶

**ç›®æ ‡**ï¼šå°†ç¡¬ç¼–ç çš„é™åˆ¶æ”¹ä¸ºå¯é…ç½®å‚æ•°

**æ•ˆæœé¢„æœŸ**ï¼š
- æ€è€ƒå­—ç¬¦é™åˆ¶ä» 10K å¢åŠ åˆ° 50Kï¼Œè¦†ç›– 99% çš„å¤æ‚æ¨ç†åœºæ™¯
- è¶…æ—¶æ—¶é—´ä» 120s å¢åŠ åˆ° 300sï¼Œç»™å›¾åƒæ¨ç†ç­‰è€—æ—¶ä»»åŠ¡è¶³å¤Ÿæ—¶é—´
- é…ç½®å¯æŒ‰æ¨¡å‹è°ƒæ•´ï¼Œé€‚é…ä¸åŒèƒ½åŠ›çš„æ¨¡å‹

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/core/src/llm/backend.rs æ–°å¢é…ç½®ç»“æ„
pub struct StreamConfig {
    /// Maximum thinking characters before cutoff
    pub max_thinking_chars: usize,

    /// Maximum thinking time in seconds
    pub max_thinking_time_secs: u64,

    /// Total stream timeout in seconds
    pub max_stream_duration_secs: u64,

    /// Progressive warning thresholds (in seconds)
    pub warning_thresholds: Vec<u64>,
}

impl Default for StreamConfig {
    fn default() -> Self {
        Self {
            max_thinking_chars: 50000,      // å¢åŠ åˆ° 50K
            max_thinking_time_secs: 120,    // å¢åŠ åˆ° 120s
            max_stream_duration_secs: 300,  // å¢åŠ åˆ° 300s (5åˆ†é’Ÿ)
            warning_thresholds: vec![60, 120, 180, 240],
        }
    }
}
```

**å‰ç«¯æ”¹åŠ¨**ï¼š

```typescript
// web/src/types/index.ts æ–°å¢é…ç½®ç±»å‹
export interface StreamConfig {
  maxThinkingChars: number
  maxThinkingTimeSecs: number
  maxStreamDurationSecs: number
  warningThresholds: number[]
}

// ServerMessage æ–°å¢äº‹ä»¶ç±»å‹
export interface ServerMessage {
  type: 'Thinking' | 'Content' | 'ToolCallStart' | 'ToolCallEnd' | 'StreamProgress' | 'Error' | 'end'

  // æ–°å¢ StreamProgress äº‹ä»¶æ•°æ®
  elapsed?: number
  warning?: string
  remainingTime?: number
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/llm/src/backends/ollama.rs:25-40` | ç§»é™¤ç¡¬ç¼–ç å¸¸é‡ |
| `crates/agent/src/agent/streaming.rs:45-55` | ä½¿ç”¨é…ç½®æ›¿ä»£é»˜è®¤å€¼ |
| `crates/api/src/handlers/sessions.rs:38` | ä½¿ç”¨é…ç½® |
| `web/src/types/index.ts:339-359` | æ–°å¢ç±»å‹å®šä¹‰ |
| `web/src/components/chat/ChatContainer.tsx:83-143` | å¤„ç†è¿›åº¦äº‹ä»¶ |
| `config.toml` | æ–°å¢ stream é…ç½®èŠ‚ |

**å·¥ä½œé‡**: 2-3 å¤©

---

#### P0.2 åˆ†é˜¶æ®µè¶…æ—¶è­¦å‘Šæœºåˆ¶

**ç›®æ ‡**ï¼šåœ¨è¶…æ—¶å‰å‘ç”¨æˆ·æ˜¾ç¤ºè¿›åº¦å’Œè­¦å‘Š

**æ•ˆæœé¢„æœŸ**ï¼š
- ç”¨æˆ·å¯å®æ—¶çœ‹åˆ°ä»»åŠ¡æ‰§è¡Œè¿›åº¦
- æå‰çŸ¥æ™“å¯èƒ½çš„è¶…æ—¶ï¼Œå‡å°‘ç„¦è™‘
- æ˜ç¡®æ˜¾ç¤ºå½“å‰é˜¶æ®µï¼ˆæ€è€ƒ/ç”Ÿæˆ/å·¥å…·æ‰§è¡Œï¼‰

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/agent/src/agent/streaming.rs æ–°å¢è¿›åº¦æŠ¥å‘Š
async fn report_stream_progress(
    safeguards: &StreamSafeguards,
    config: &StreamConfig,
    tx: &Sender,
) -> Result<()> {
    let start = Instant::now();
    let mut last_warning_idx = 0usize;

    loop {
        let elapsed = start.elapsed().as_secs();

        // æ£€æŸ¥è­¦å‘Šé˜ˆå€¼
        for (i, threshold) in config.warning_thresholds.iter().enumerate() {
            if elapsed >= *threshold && i == last_warning_idx {
                send_event(&tx, ServerEvent::Progress {
                    elapsed,
                    message: format!("æ‰§è¡Œä¸­... å·²è€—æ—¶ {} ç§’", threshold),
                    stage: StreamStage::from_elapsed(elapsed),
                }).await;
                last_warning_idx = i + 1;
            }
        }

        // è®¡ç®—å‰©ä½™æ—¶é—´
        let remaining = safeguards.max_stream_duration.saturating_sub(elapsed);
        if remaining <= 30 && remaining % 10 == 0 {
            send_event(&tx, ServerEvent::Warning {
                message: format!("å‰©ä½™æ—¶é—´çº¦ {} ç§’", remaining),
            }).await;
        }

        tokio::time::sleep(Duration::from_secs(1)).await;
    }
}

#[derive(Debug, Clone)]
pub enum StreamStage {
    Thinking,
    Generating,
    ToolExecution,
}

#[derive(Debug, Clone)]
pub enum ServerEvent {
    Progress { elapsed: u64, message: String, stage: StreamStage },
    Warning { message: String },
    // ... existing events
}
```

**å‰ç«¯æ”¹åŠ¨**ï¼š

```tsx
// web/src/components/chat/StreamProgress.tsx æ–°ç»„ä»¶
import { Progress } from "@/components/ui/progress"
import { Activity, AlertTriangle } from "lucide-react"

interface StreamProgressProps {
  elapsed: number
  totalDuration: number
  stage: 'thinking' | 'generating' | 'tool_execution'
  warning?: string
}

export function StreamProgress({
  elapsed,
  totalDuration,
  stage,
  warning
}: StreamProgressProps) {
  const progress = Math.min((elapsed / totalDuration) * 100, 100)

  const stageLabels = {
    thinking: 'æ€è€ƒä¸­',
    generating: 'ç”Ÿæˆä¸­',
    tool_execution: 'å·¥å…·æ‰§è¡Œ'
  }

  return (
    <div className="flex items-center gap-3 text-sm text-muted-foreground px-4 py-2 bg-muted/30 rounded-lg">
      <Activity className="h-4 w-4 animate-pulse" />
      <div className="flex-1">
        <div className="flex items-center justify-between mb-1">
          <span>{stageLabels[stage]}</span>
          <span className="text-xs">{elapsed}s / {totalDuration}s</span>
        </div>
        <div className="h-1.5 bg-muted rounded-full overflow-hidden">
          <div
            className={`h-full transition-all duration-300 ${
              progress > 80 ? 'bg-yellow-500' : 'bg-blue-500'
            }`}
            style={{ width: `${progress}%` }}
          />
        </div>
      </div>
      {warning && (
        <span className="text-yellow-600 text-xs flex items-center gap-1">
          <AlertTriangle className="h-3 w-3" />
          {warning}
        </span>
      )}
    </div>
  )
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/agent/src/agent/streaming.rs:1106-1150` | è¿›åº¦æŠ¥å‘Šé€»è¾‘ |
| `web/src/components/chat/StreamProgress.tsx` | æ–°æ–‡ä»¶ |
| `web/src/components/chat/ChatContainer.tsx:322-328` | é›†æˆè¿›åº¦æ¡ |
| `web/src/i18n/locales/en/common.json` | æ–°å¢ç¿»è¯‘é”® |
| `web/src/i18n/locales/zh/common.json` | æ–°å¢ç¿»è¯‘é”® |

**å·¥ä½œé‡**: 1-2 å¤©

---

#### P0.3 ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–ä¸æ¢å¤

**ç›®æ ‡**ï¼šä¸­æ–­åå¯æ¢å¤ä»»åŠ¡çŠ¶æ€

**æ•ˆæœé¢„æœŸ**ï¼š
- ä»»åŠ¡ä¸­æ–­åä¸ä¸¢å¤±è¿›åº¦
- ç”¨æˆ·å¯é€‰æ‹©æ¢å¤æˆ–ä¸¢å¼ƒ
- ä¸­æ–­æ¢å¤ç‡ > 80%

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/storage/src/task_state.rs æ–°å¢æ–‡ä»¶
use redb::{Database, ReadableTable, WritableTable};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskState {
    pub id: String,
    pub session_id: String,
    pub user_message: String,
    pub stage: TaskStage,
    pub thinking_content: String,
    pub partial_response: String,
    pub tool_calls: Vec<ToolCallState>,
    pub elapsed_seconds: u64,
    pub created_at: i64,
    pub updated_at: i64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TaskStage {
    Thinking,
    Generating,
    ToolExecuting,
    Interrupted,
    Completed,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCallState {
    pub name: String,
    pub arguments: serde_json::Value,
    pub result: Option<serde_json::Value>,
    pub completed: bool,
}

const TASK_STATES_TABLE: &str = "task_states";

pub struct TaskStateManager {
    db: Database,
}

impl TaskStateManager {
    pub fn new(db_path: &str) -> Result<Self> {
        let db = Database::create(db_path)?;
        Ok(Self { db })
    }

    pub fn save(&self, state: TaskState) -> Result<()> {
        let write_txn = self.db.begin_write()?;
        {
            let mut table = write_txn.open_table(TASK_STATES_TABLE)?;
            let key = state.id.as_str();
            let value = serde_json::to_vec(&state)?;
            table.insert(key, value)?;
        }
        write_txn.commit()?;
        Ok(())
    }

    pub fn load(&self, task_id: &str) -> Result<Option<TaskState>> {
        let read_txn = self.db.begin_read()?;
        let table = read_txn.open_table(TASK_STATES_TABLE)?;
        Ok(table.get(task_id)?
            .map(|value| serde_json::from_slice(&value.value()).ok())
            .flatten())
    }

    pub fn list_interrupted(&self, session_id: &str) -> Result<Vec<TaskState>> {
        let read_txn = self.db.begin_read()?;
        let table = read_txn.open_table(TASK_STATES_TABLE)?;

        let mut results = Vec::new();
        for item in table.iter()? {
            let (_, value) = item?;
            if let Ok(state) = serde_json::from_slice::<TaskState>(&value) {
                if state.session_id == session_id
                    && matches!(state.stage, TaskStage::Interrupted) {
                    results.push(state);
                }
            }
        }
        results.sort_by(|a, b| b.updated_at.cmp(&a.updated_at));
        Ok(results)
    }

    pub fn delete(&self, task_id: &str) -> Result<()> {
        let write_txn = self.db.begin_write()?;
        {
            let mut table = write_txn.open_table(TASK_STATES_TABLE)?;
            table.remove(task_id)?;
        }
        write_txn.commit()?;
        Ok(())
    }
}
```

**API ç«¯ç‚¹**ï¼š

```rust
// crates/api/src/handlers/tasks.rs æ–°å¢æ–‡ä»¶
use axum::{Json, extract::State};
use serde::{Deserialize, Serialize};

#[derive(Deserialize)]
pub struct ResumeTaskRequest {
    pub task_id: String,
}

#[derive(Serialize)]
pub struct ResumeTaskResponse {
    pub task_id: String,
    pub resumed: bool,
    pub message: String,
}

/// GET /api/tasks/interrupted?session_id=xxx
pub async fn list_interrupted_tasks(
    State(manager): State<Arc<TaskStateManager>>,
    Query(params): Query<TaskQueryParams>,
) -> Result<Json<Vec<TaskState>>, AppError> {
    let tasks = manager.list_interrupted(&params.session_id)?;
    Ok(Json(tasks))
}

/// POST /api/tasks/resume
pub async fn resume_task(
    State(agent): State<Arc<Agent>>,
    State(manager): State<Arc<TaskStateManager>>,
    Json(req): Json<ResumeTaskRequest>,
) -> Result<Json<ResumeTaskResponse>, AppError> {
    let task = manager.load(&req.task_id)?
        .ok_or_else(|| AppError::NotFound("Task not found".to_string()))?;

    // ä»ä¸­æ–­ç‚¹æ¢å¤æ‰§è¡Œ
    agent.resume_from_state(task).await?;

    Ok(Json(ResumeTaskResponse {
        task_id: req.task_id,
        resumed: true,
        message: "Task resumed successfully".to_string(),
    }))
}

/// DELETE /api/tasks/:id
pub async fn discard_task(
    State(manager): State<Arc<TaskStateManager>>,
    Path(id): Path<String>,
) -> Result<StatusCode, AppError> {
    manager.delete(&id)?;
    Ok(StatusCode::NO_CONTENT)
}
```

**å‰ç«¯æ”¹åŠ¨**ï¼š

```tsx
// web/src/components/chat/InterruptedTaskDialog.tsx æ–°ç»„ä»¶
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog"
import { Button } from "@/components/ui/button"
import { Progress } from "@/components/ui/progress"
import { AlertTriangle, Clock } from "lucide-react"
import type { TaskState } from "@/types"

interface InterruptedTaskDialogProps {
  taskState: TaskState | null
  onResume: (task: TaskState) => void
  onDiscard: () => void
}

export function InterruptedTaskDialog({
  taskState,
  onResume,
  onDiscard,
}: InterruptedTaskDialogProps) {
  if (!taskState) return null

  const progress = (taskState.elapsed_seconds / 300) * 100

  return (
    <Dialog open={!!taskState}>
      <DialogContent className="sm:max-w-md">
        <DialogHeader>
          <DialogTitle className="flex items-center gap-2">
            <AlertTriangle className="h-5 w-5 text-yellow-500" />
            ä»»åŠ¡è¢«ä¸­æ–­
          </DialogTitle>
          <DialogDescription>
            æ‚¨çš„ä»»åŠ¡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¢«ä¸­æ–­ï¼Œä½†å·²ä¿å­˜éƒ¨åˆ†è¿›åº¦ã€‚
          </DialogDescription>
        </DialogHeader>

        <div className="py-4 space-y-4">
          {/* è¿›åº¦ä¿¡æ¯ */}
          <div className="space-y-2">
            <div className="flex items-center justify-between text-sm">
              <span className="flex items-center gap-1 text-muted-foreground">
                <Clock className="h-3 w-3" />
                æ‰§è¡Œæ—¶é—´
              </span>
              <span>{taskState.elapsed_seconds}s / 300s</span>
            </div>
            <Progress value={progress} className="h-2" />
          </div>

          {/* ç”¨æˆ·æ¶ˆæ¯ */}
          <div className="text-sm">
            <span className="text-muted-foreground">åŸå§‹è¯·æ±‚ï¼š</span>
            <p className="mt-1 p-2 bg-muted rounded text-xs">
              {taskState.user_message}
            </p>
          </div>

          {/* æ€è€ƒå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰ */}
          {taskState.thinking_content && (
            <details className="text-sm">
              <summary className="cursor-pointer text-muted-foreground hover:text-foreground">
                æ€è€ƒå†…å®¹ ({taskState.thinking_content.length} å­—ç¬¦)
              </summary>
              <pre className="mt-2 p-2 bg-muted rounded text-xs overflow-auto max-h-32">
                {taskState.thinking_content}
              </pre>
            </details>
          )}

          {/* éƒ¨åˆ†å“åº”ï¼ˆå¦‚æœæœ‰ï¼‰ */}
          {taskState.partial_response && (
            <div className="text-sm">
              <span className="text-muted-foreground">å·²ç”Ÿæˆå†…å®¹ï¼š</span>
              <p className="mt-1 p-2 bg-muted rounded text-xs">
                {taskState.partial_response.slice(0, 200)}
                {taskState.partial_response.length > 200 ? '...' : ''}
              </p>
            </div>
          )}
        </div>

        <DialogFooter>
          <Button variant="outline" onClick={onDiscard}>
            ä¸¢å¼ƒ
          </Button>
          <Button onClick={() => onResume(taskState)}>
            æ¢å¤ä»»åŠ¡
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  )
}
```

**ç±»å‹å®šä¹‰**ï¼š

```typescript
// web/src/types/index.ts æ–°å¢
export interface TaskState {
  id: string
  session_id: string
  user_message: string
  stage: 'thinking' | 'generating' | 'tool_executing' | 'interrupted' | 'completed'
  thinking_content: string
  partial_response: string
  tool_calls: ToolCallState[]
  elapsed_seconds: number
  created_at: number
  updated_at: number
}

export interface ToolCallState {
  name: string
  arguments: unknown
  result?: unknown
  completed: boolean
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/storage/src/task_state.rs` | æ–°æ–‡ä»¶ |
| `crates/storage/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/agent/src/agent/streaming.rs:121-143` | ä¿å­˜çŠ¶æ€ |
| `crates/agent/src/agent/mod.rs` | æ–°å¢ resume_from_state æ–¹æ³• |
| `crates/api/src/handlers/tasks.rs` | æ–°æ–‡ä»¶ |
| `crates/api/src/lib.rs` | æ³¨å†Œæ–°è·¯ç”± |
| `web/src/components/chat/InterruptedTaskDialog.tsx` | æ–°æ–‡ä»¶ |
| `web/src/lib/api.ts` | æ–°å¢ API è°ƒç”¨ |
| `web/src/types/index.ts` | æ–°å¢ç±»å‹å®šä¹‰ |
| `web/src/components/chat/ChatContainer.tsx` | é›†æˆå¯¹è¯æ¡† |

**å·¥ä½œé‡**: 3-4 å¤©

---

### ğŸŸ  P1 - ä¸Šä¸‹æ–‡ç®¡ç†æ”¹è¿›

> **é¢„è®¡å·¥æœŸ**: 1-2 å‘¨
> **é¢„æœŸæ•ˆæœ**: é•¿å¯¹è¯ç¨³å®šæ€§æå‡ï¼Œé¿å…ä¸Šä¸‹æ–‡æº¢å‡º

#### P1.1 ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥

**ç›®æ ‡**ï¼šå®ç°ç±»ä¼¼ Moltbot çš„ä¸Šä¸‹æ–‡ç®¡ç†

**æ•ˆæœé¢„æœŸ**ï¼š
- è‡ªåŠ¨æ£€æµ‹ä¸Šä¸‹æ–‡æ¥è¿‘é™åˆ¶
- è§¦å‘è®°å¿†åˆ·æ–°ï¼ŒæŒä¹…åŒ–é‡è¦ä¿¡æ¯
- å‹ç¼©å†å²æ¶ˆæ¯ï¼Œä¿æŒå¯¹è¯è¿è´¯æ€§
- é•¿å¯¹è¯ä¸ä¼šå› ä¸Šä¸‹æ–‡è¿‡é•¿è€Œå¤±è´¥

**å‚è€ƒé…ç½®ï¼ˆMoltbotï¼‰**ï¼š
```json
{
  "compaction": {
    "mode": "default",
    "reserveTokensFloor": 20000,
    "maxHistoryShare": 0.5,
    "memoryFlush": {
      "enabled": true,
      "softThresholdTokens": 4000,
      "prompt": "Write any lasting notes to memory/YYYY-MM-DD.md; reply with NO_REPLY if nothing to store."
    }
  }
}
```

**è§¦å‘æ—¶æœº**ï¼š
```
contextWindow - reserveTokensFloor - softThresholdTokens
```

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/core/src/llm/compaction.rs æ–°å¢æ–‡ä»¶
use serde::{Deserialize, Serialize};
use crate::message::Message;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompactionConfig {
    /// Minimum tokens to reserve for system prompt and response
    pub reserve_tokens_floor: usize,

    /// Maximum share of context for history (0.0-1.0)
    pub max_history_share: f64,

    /// Threshold for triggering memory flush
    pub soft_threshold_tokens: usize,

    /// Memory flush prompt
    pub memory_flush_prompt: String,

    /// Whether compaction is enabled
    pub enabled: bool,
}

impl Default for CompactionConfig {
    fn default() -> Self {
        Self {
            reserve_tokens_floor: 20000,
            max_history_share: 0.5,
            soft_threshold_tokens: 4000,
            memory_flush_prompt: "Please store any important information from this conversation into memory. Reply with NO_REPLY if there's nothing to store.".to_string(),
            enabled: true,
        }
    }
}

pub struct ContextCompactor {
    config: CompactionConfig,
    context_window: usize,
    token_counter: Arc<TokenCounter>,
}

impl ContextCompactor {
    pub fn new(config: CompactionConfig, context_window: usize, token_counter: Arc<TokenCounter>) -> Self {
        Self { config, context_window, token_counter }
    }

    /// Calculate the threshold at which compaction should be triggered
    pub fn compaction_threshold(&self) -> usize {
        self.context_window
            .saturating_sub(self.config.reserve_tokens_floor)
            .saturating_sub(self.config.soft_threshold_tokens)
    }

    /// Check if compaction is needed
    pub fn should_compact(&self, current_tokens: usize) -> bool {
        if !self.config.enabled {
            return false;
        }
        current_tokens >= self.compaction_threshold()
    }

    /// Estimate token count for messages
    pub fn estimate_tokens(&self, messages: &[Message]) -> usize {
        messages.iter()
            .map(|msg| self.token_counter.count_message_tokens(msg))
            .sum()
    }

    /// Calculate max history tokens allowed
    pub fn max_history_tokens(&self) -> usize {
        (self.context_window as f64 * self.config.max_history_share) as usize
    }

    /// Compact and flush memory
    pub async fn compact_and_flush(
        &self,
        session_id: &str,
        messages: Vec<Message>,
        memory_store: Arc<dyn MemoryStore>,
    ) -> Result<Vec<Message>> {
        // 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è®°å¿†åˆ·æ–°
        let should_flush = self.should_compact(self.estimate_tokens(&messages));

        if !should_flush {
            return Ok(messages);
        }

        tracing::info!("Compaction triggered for session {}, current tokens: {}",
            session_id, self.estimate_tokens(&messages));

        // 2. æ„å»ºè®°å¿†åˆ·æ–°è¯·æ±‚
        let flush_messages = vec![
            Message::system(&self.config.memory_flush_prompt),
            // æ·»åŠ æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡
        ];

        // 3. è°ƒç”¨ LLM ç”Ÿæˆè®°å¿†æ‘˜è¦
        // memory_store.store(session_id, summary).await?;

        // 4. å‹ç¼©å†å²æ¶ˆæ¯
        let max_history = self.max_history_tokens();
        let mut compacted = Vec::new();
        let mut current_tokens = 0;

        // ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
        for msg in messages.iter().rev() {
            let msg_tokens = self.token_counter.count_message_tokens(msg);
            if current_tokens + msg_tokens > max_history {
                break;
            }
            compacted.insert(0, msg.clone());
            current_tokens += msg_tokens;
        }

        // åœ¨å¼€å¤´æ·»åŠ å‹ç¼©æ‘˜è¦
        let summary = memory_store.get_latest_summary(session_id).await?;
        if let Some(summary) = summary {
            compacted.insert(0, Message::system(&format!(
                "[Previous conversation summary]\n{}",
                summary
            )));
        }

        tracing::info!("Compaction complete: {} messages -> {} messages, {} -> {} tokens",
            messages.len(), compacted.len(),
            self.estimate_tokens(&messages), self.estimate_tokens(&compacted));

        Ok(compacted)
    }
}
```

**å‰ç«¯æ”¹åŠ¨**ï¼š

```tsx
// web/src/components/chat/MemoryFlushIndicator.tsx æ–°ç»„ä»¶
import { Activity, Database, AlertTriangle } from "lucide-react"

interface MemoryFlushIndicatorProps {
  currentTokens: number
  threshold: number
  contextWindow: number
  isCompacting: boolean
}

export function MemoryFlushIndicator({
  currentTokens,
  threshold,
  contextWindow,
  isCompacting,
}: MemoryFlushIndicatorProps) {
  const percentage = (currentTokens / contextWindow) * 100
  const thresholdPercentage = (threshold / contextWindow) * 100

  const status = isCompacting ? 'compacting' :
                 percentage > thresholdPercentage ? 'warning' :
                 'normal'

  const statusConfig = {
    normal: { color: 'text-green-500', text: 'ä¸Šä¸‹æ–‡æ­£å¸¸' },
    warning: { color: 'text-yellow-500', text: 'æ¥è¿‘ä¸Šä¸‹æ–‡é™åˆ¶' },
    compacting: { color: 'text-blue-500', text: 'æ­£åœ¨å‹ç¼©ä¸Šä¸‹æ–‡...' },
  }

  const config = statusConfig[status]

  return (
    <div className="flex items-center gap-2 text-xs text-muted-foreground px-2 py-1">
      <Activity className={`h-3 w-3 ${config.color} ${isCompacting ? 'animate-pulse' : ''}`} />
      <span>ä¸Šä¸‹æ–‡: {currentTokens}/{contextWindow} tokens</span>

      {/* é˜ˆå€¼æ ‡è®° */}
      <div className="flex-1 h-1.5 bg-muted rounded-full overflow-hidden relative">
        <div
          className="h-full bg-blue-500 transition-all duration-300"
          style={{ width: `${percentage}%` }}
        />
        <div
          className="absolute top-0 h-full w-0.5 bg-yellow-500"
          style={{ left: `${thresholdPercentage}%` }}
        />
      </div>

      {status !== 'normal' && (
        <span className={`${config.color} flex items-center gap-1`}>
          {status === 'warning' && <AlertTriangle className="h-3 w-3" />}
          {status === 'compacting' && <Database className="h-3 w-3 animate-spin" />}
          {config.text}
        </span>
      )}
    </div>
  )
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/core/src/llm/compaction.rs` | æ–°æ–‡ä»¶ |
| `crates/core/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/agent/src/agent/session.rs` | é›†æˆå‹ç¼© |
| `crates/memory/src/lib.rs` | è®°å¿†åˆ·æ–°è§¦å‘ |
| `crates/api/src/handlers/sessions.rs` | æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ API |
| `web/src/components/chat/MemoryFlushIndicator.tsx` | æ–°æ–‡ä»¶ |
| `web/src/components/chat/ChatContainer.tsx` | æ˜¾ç¤ºæŒ‡æ ‡ |
| `web/src/lib/api.ts` | æ–°å¢ API è°ƒç”¨ |
| `config.toml` | æ–°å¢ compaction é…ç½®èŠ‚ |

**å·¥ä½œé‡**: 4-5 å¤©

---

#### P1.2 Token è®¡æ•°å™¨

**ç›®æ ‡**ï¼šå‡†ç¡®ä¼°ç®—æ¶ˆæ¯ token æ•°é‡

**æ•ˆæœé¢„æœŸ**ï¼š
- å‡†ç¡®é¢„ä¼°ä½•æ—¶è§¦å‘å‹ç¼©
- é¿å…å›  token ä¼°ç®—é”™è¯¯å¯¼è‡´çš„æº¢å‡º
- æ”¯æŒä¸åŒæ¨¡å‹çš„ token è®¡ç®—

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/core/src/llm/token_counter.rs æ–°å¢æ–‡ä»¶
use tiktoken_rs::tiktoken;
use crate::message::{Message, MessageRole};

pub struct TokenCounter {
    bpe: tiktoken_rs::CoreBPE,
}

impl TokenCounter {
    pub fn new(model: &str) -> Result<Self> {
        // æ ¹æ® model é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
        let encoding = match model {
            m if m.starts_with("gpt-4") => "cl100k_base",
            m if m.starts_with("gpt-3.5") => "cl100k_base",
            m if m.contains("qwen") => "cl100k_base", // è¿‘ä¼¼
            _ => "cl100k_base", // é»˜è®¤
        };

        let bpe = tiktoken(encoding)?;
        Ok(Self { bpe })
    }

    pub fn count_tokens(&self, text: &str) -> usize {
        self.bpe.encode_with_special_tokens(text).len()
    }

    pub fn count_message_tokens(&self, msg: &Message) -> usize {
        // å‚è€ƒ OpenAI çš„ token è®¡æ•°è§„åˆ™
        // https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb

        let base = 4; // æ¯æ¡æ¶ˆæ¯çš„åŸºç¡€å¼€é”€ (<im_start>{role/name}\n{content}<im_end>\n)

        // è§’è‰²æ ‡è®°
        let role = match msg.role() {
            MessageRole::User => "user",
            MessageRole::Assistant => "assistant",
            MessageRole::System => "system",
        };
        let role_tokens = self.count_tokens(role);

        // åç§°æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰
        let name_tokens = if let Some(name) = msg.name() {
            1 + self.count_tokens(name) // name å­—æ®µ
        } else {
            0
        };

        // å†…å®¹ token
        let content_tokens = self.count_tokens(msg.content());

        // æ¯ä¸ªå­—æ®µåçš„åˆ†éš”ç¬¦
        let separators = 2;

        base + role_tokens + name_tokens + content_tokens + separators
    }

    pub fn count_messages_tokens(&self, messages: &[Message]) -> usize {
        // æ¶ˆæ¯æ€»æ•°
        let messages_len = messages.len();

        // æ¯æ¡æ¶ˆæ¯çš„å¼€é”€
        let per_message: usize = messages.iter()
            .map(|msg| self.count_message_tokens(msg))
            .sum();

        // å›å¤çš„å¼€é”€
        let reply = 3; // <im_start>assistant\n<im_end>\n

        messages_len + per_message + reply
    }

    /// ä¼°ç®—ä¸Šä¸‹æ–‡çª—å£ä½¿ç”¨é‡
    pub fn estimate_context_usage(
        &self,
        messages: &[Message],
        system_prompt: &str,
        reserve: usize,
    ) -> (usize, usize) {
        let system_tokens = self.count_tokens(system_prompt);
        let messages_tokens = self.count_messages_tokens(messages);
        let total = system_tokens + messages_tokens + reserve;
        (total, messages_tokens)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_count_simple_text() {
        let counter = TokenCounter::new("gpt-4").unwrap();
        let tokens = counter.count_tokens("Hello, world!");
        assert!(tokens > 0);
    }

    #[test]
    fn test_count_message() {
        let counter = TokenCounter::new("gpt-4").unwrap();
        let msg = Message::user("What is the capital of France?");
        let tokens = counter.count_message_tokens(&msg);
        assert!(tokens > 4); // åŸºç¡€å¼€é”€ + å†…å®¹
    }
}
```

**é…ç½®æ›´æ–°**ï¼š

```toml
# config.toml æ–°å¢é…ç½®
[llm]
# ä½¿ç”¨çš„ token è®¡æ•°å™¨ç¼–ç å™¨
# å¯é€‰: cl100k_base (GPT-4/GPT-3.5), p50k_base (GPT-3), r50k_base (GPT-2)
token_encoding = "cl100k_base"

# ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆæ ¹æ®æ¨¡å‹è°ƒæ•´ï¼‰
context_window = 128000  # qwen3-vl:2b çº¦ä¸º 32Kï¼ŒGPT-4 ä¸º 128K
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/core/src/llm/token_counter.rs` | æ–°æ–‡ä»¶ |
| `crates/core/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `Cargo.toml` | æ·»åŠ  `tiktoken-rs = "0.5"` ä¾èµ– |
| `crates/agent/src/agent/session.rs` | ä½¿ç”¨ TokenCounter |
| `config.toml` | æ–°å¢ token_encoding é…ç½® |

**å·¥ä½œé‡**: 2 å¤©

---

### ğŸŸ¡ P2 - è®°å¿†ç³»ç»Ÿæ”¹è¿›

> **é¢„è®¡å·¥æœŸ**: 2-3 å‘¨
> **é¢„æœŸæ•ˆæœ**: æœç´¢ç²¾åº¦æå‡ 50%+ï¼Œæ”¯æŒçœŸå®è¯­ä¹‰æœç´¢

#### P2.1 çœŸå®åµŒå…¥æ¨¡å‹æ”¯æŒ

**ç›®æ ‡**ï¼šæ›¿æ¢ SimpleEmbedding ä¸ºçœŸå®åµŒå…¥

**æ•ˆæœé¢„æœŸ**ï¼š
- å®ç°çœŸæ­£çš„è¯­ä¹‰æœç´¢
- æœç´¢ç²¾åº¦å¤§å¹…æå‡
- æ”¯æŒ Ollama å’Œ OpenAI åµŒå…¥æ¨¡å‹

**å½“å‰é—®é¢˜**ï¼š
```rust
// crates/memory/src/mid_term.rs çš„ SimpleEmbedding æ˜¯å‡çš„ï¼
pub fn embed(&self, text: &str) -> Vec<f32> {
    // è¿™åªæ˜¯ hashï¼Œä¸æ˜¯çœŸå®çš„è¯­ä¹‰åµŒå…¥
    for (i, byte) in text.bytes().enumerate() {
        let pos = i % self.dim;
        embedding[pos] = embedding[pos] * 31.0 + (byte as f32) * 0.1;
    }
}
```

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/memory/src/embeddings.rs æ–°å¢æ–‡ä»¶
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};

/// åµŒå…¥æ¨¡å‹ trait
#[async_trait]
pub trait EmbeddingModel: Send + Sync {
    async fn embed(&self, text: &str) -> Result<Vec<f32>, EmbeddingError>;
    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, EmbeddingError>;
    fn dimension(&self) -> usize;
}

#[derive(Debug, thiserror::Error)]
pub enum EmbeddingError {
    #[error("HTTP error: {0}")]
    Http(#[from] reqwest::Error),

    #[error("JSON error: {0}")]
    Json(#[from] serde_json::Error),

    #[error("Invalid response: {0}")]
    InvalidResponse(String),

    #[error("API error: {0}")]
    Api(String),
}

/// Ollama åµŒå…¥æ¨¡å‹
pub struct OllamaEmbedding {
    client: Client,
    model: String,
    endpoint: String,
    dimension: usize,
}

#[derive(Debug, Serialize)]
struct OllamaEmbedRequest<'a> {
    model: &'a str,
    input: &'a str,
}

#[derive(Debug, Deserialize)]
struct OllamaEmbedResponse {
    embedding: Vec<f32>,
}

impl OllamaEmbedding {
    pub fn new(model: &str, endpoint: &str) -> Self {
        Self {
            client: Client::new(),
            model: model.to_string(),
            endpoint: endpoint.to_string(),
            dimension: 768, // nomic-embed-text é»˜è®¤ç»´åº¦
        }
    }

    pub fn with_dimension(mut self, dimension: usize) -> Self {
        self.dimension = dimension;
        self
    }
}

#[async_trait]
impl EmbeddingModel for OllamaEmbedding {
    async fn embed(&self, text: &str) -> Result<Vec<f32>, EmbeddingError> {
        let url = format!("{}/api/embed", self.endpoint);
        let req = OllamaEmbedRequest {
            model: &self.model,
            input: text,
        };

        let resp = self.client
            .post(&url)
            .json(&req)
            .send()
            .await?;

        if !resp.status().is_success() {
            let status = resp.status().as_u16();
            let text = resp.text().await.unwrap_or_default();
            return Err(EmbeddingError::Api(format!("{}: {}", status, text)));
        }

        let data: OllamaEmbedResponse = resp.json().await?;
        Ok(data.embedding)
    }

    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, EmbeddingError> {
        // Ollama ä¸æ”¯æŒåŸç”Ÿæ‰¹å¤„ç†ï¼Œé¡ºåºæ‰§è¡Œ
        let mut results = Vec::with_capacity(texts.len());
        for text in texts {
            results.push(self.embed(text).await?);
        }
        Ok(results)
    }

    fn dimension(&self) -> usize {
        self.dimension
    }
}

/// OpenAI åµŒå…¥æ¨¡å‹
pub struct OpenAIEmbedding {
    client: Client,
    model: String,
    api_key: String,
}

#[derive(Debug, Serialize)]
struct OpenAIEmbedRequest<'a> {
    model: &'a str,
    input: Vec<&'a str>,
}

#[derive(Debug, Deserialize)]
struct OpenAIEmbedResponse {
    data: Vec<OpenAIEmbedData>,
}

#[derive(Debug, Deserialize)]
struct OpenAIEmbedData {
    embedding: Vec<f32>,
}

impl OpenAIEmbedding {
    pub fn new(model: &str, api_key: &str) -> Self {
        Self {
            client: Client::new(),
            model: model.to_string(),
            api_key: api_key.to_string(),
        }
    }
}

#[async_trait]
impl EmbeddingModel for OpenAIEmbedding {
    async fn embed(&self, text: &str) -> Result<Vec<f32>, EmbeddingError> {
        let results = self.embed_batch(&[text.to_string()]).await?;
        Ok(results.into_iter().next().unwrap())
    }

    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, EmbeddingError> {
        let url = "https://api.openai.com/v1/embeddings";
        let inputs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();

        let resp = self.client
            .post(url)
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&OpenAIEmbedRequest {
                model: &self.model,
                input: inputs,
            })
            .send()
            .await?;

        if !resp.status().is_success() {
            let status = resp.status().as_u16();
            let text = resp.text().await.unwrap_or_default();
            return Err(EmbeddingError::Api(format!("{}: {}", status, text)));
        }

        let data: OpenAIEmbedResponse = resp.json().await?;
        Ok(data.data.into_iter().map(|d| d.embedding).collect())
    }

    fn dimension(&self) -> usize {
        match self.model.as_str() {
            "text-embedding-3-small" => 1536,
            "text-embedding-3-large" => 3072,
            "text-embedding-ada-002" => 1536,
            _ => 1536,
        }
    }
}

/// åµŒå…¥æä¾›è€…é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingProviderConfig {
    pub provider: String,
    pub model: String,
    pub endpoint: Option<String>,
    pub api_key: Option<String>,
}

/// åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
pub fn create_embedding_model(config: EmbeddingProviderConfig) -> Result<Box<dyn EmbeddingModel>> {
    match config.provider.as_str() {
        "ollama" => {
            let endpoint = config.endpoint.unwrap_or_else(|| "http://localhost:11434".to_string());
            Ok(Box::new(OllamaEmbedding::new(&config.model, &endpoint)))
        }
        "openai" => {
            let api_key = config.api_key.ok_or_else(|| {
                EmbeddingError::InvalidResponse("OpenAI API key is required".to_string())
            })?;
            Ok(Box::new(OpenAIEmbedding::new(&config.model, &api_key)))
        }
        _ => Err(EmbeddingError::InvalidResponse(format!("Unknown provider: {}", config.provider))),
    }
}
```

**æ›´æ–° mid_term.rs**ï¼š

```rust
// crates/memory/src/mid_term.rs ä¿®æ”¹
use super::embeddings::{EmbeddingModel, create_embedding_model, EmbeddingProviderConfig};

pub struct MidTermMemory {
    // æ›¿æ¢ SimpleEmbedding
    embedding: Box<dyn EmbeddingModel>,
    // ... å…¶ä»–å­—æ®µ
}

impl MidTermMemory {
    pub fn new(config: &MemoryConfig) -> Result<Self> {
        let embedding_config = EmbeddingProviderConfig {
            provider: config.embedding_provider.clone(),
            model: config.embedding_model.clone(),
            endpoint: config.embedding_endpoint.clone(),
            api_key: config.embedding_api_key.clone(),
        };

        let embedding = create_embedding_model(embedding_config)?;

        Ok(Self {
            embedding,
            // ...
        })
    }

    pub async fn search(&self, query: &str, limit: usize) -> Result<Vec<MemoryEntry>> {
        // ä½¿ç”¨çœŸå®åµŒå…¥è¿›è¡Œæœç´¢
        let query_embedding = self.embedding.embed(query).await?;

        // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        let mut results: Vec<_> = self.entries.iter()
            .map(|entry| {
                let similarity = cosine_similarity(&query_embedding, &entry.embedding);
                (entry.clone(), similarity)
            })
            .filter(|(_, sim)| *sim > 0.5) // ç›¸ä¼¼åº¦é˜ˆå€¼
            .collect();

        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        results.truncate(limit);

        Ok(results.into_iter().map(|(entry, _)| entry).collect())
    }
}

fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    dot / (norm_a * norm_a)
}
```

**é…ç½®æ›´æ–°**ï¼š

```toml
# config.toml æ–°å¢é…ç½®
[memory.embedding]
provider = "ollama"  # æˆ– "openai"
model = "nomic-embed-text"
endpoint = "http://localhost:11434"  # å¯é€‰
api_key = ""  # OpenAI éœ€è¦
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/memory/src/embeddings.rs` | æ–°æ–‡ä»¶ |
| `crates/memory/src/mid_term.rs` | æ›¿æ¢ SimpleEmbedding |
| `crates/memory/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/memory/src/long_term.rs` | ä½¿ç”¨çœŸå®åµŒå…¥ |
| `Cargo.toml` | æ·»åŠ  `async-trait`, `thiserror` ä¾èµ– |
| `config.toml` | æ–°å¢ memory.embedding é…ç½®èŠ‚ |

**å·¥ä½œé‡**: 3-4 å¤©

---

#### P2.2 BM25 å…¨æ–‡æœç´¢

**ç›®æ ‡**ï¼šæ·»åŠ å…³é”®è¯æœç´¢èƒ½åŠ›

**æ•ˆæœé¢„æœŸ**ï¼š
- æ”¯æŒç²¾ç¡®å…³é”®è¯æœç´¢
- ä¸è¯­ä¹‰æœç´¢äº’è¡¥
- æå‡æœç´¢å¬å›ç‡

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/memory/src/bm25.rs æ–°å¢æ–‡ä»¶
use tantivy::{
    schema::*,
    index::{Index, IndexWriter, IndexReader, SegmentReader},
    query::QueryParser,
    collector::TopDocs,
    DocAddress,
    Score,
};
use std::path::Path;
use serde::{Deserialize, Serialize};

/// BM25 æœç´¢ç´¢å¼•
pub struct BM25Index {
    index: Index,
    reader: IndexReader,
    schema: Schema,
}

/// æœç´¢ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BM25Result {
    pub id: String,
    pub content: String,
    pub session_id: String,
    pub score: f32,
    pub timestamp: i64,
}

impl BM25Index {
    /// åˆ›å»ºæ–°çš„ BM25 ç´¢å¼•
    pub fn new<P: AsRef<Path>>(path: P) -> Result<Self> {
        let schema = Schema::builder()
            .add_text_field("id", STRING | STORED)
            .add_text_field("content", TEXT | STORED)
            .add_text_field("session_id", STRING | STORED)
            .add_u64_field("timestamp", INDEXED | STORED)
            .build();

        let index = Index::create_in_dir(path, schema.clone())?;
        let reader = index.reader()?;

        Ok(Self {
            index,
            reader,
            schema,
        })
    }

    /// è·å– schema å­—æ®µ
    fn fields(&self) -> SchemaFields {
        self.schema.fields()
    }

    /// æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
    pub fn add_document(
        &self,
        id: &str,
        content: &str,
        session_id: &str,
        timestamp: i64,
    ) -> Result<()> {
        let mut writer = self.index.writer(50_000_000)?;

        let id_field = self.schema.get_field("id").unwrap();
        let content_field = self.schema.get_field("content").unwrap();
        let session_id_field = self.schema.get_field("session_id").unwrap();
        let timestamp_field = self.schema.get_field("timestamp").unwrap();

        let doc = doc!(
            id_field => id,
            content_field => content,
            session_id_field => session_id,
            timestamp_field => timestamp
        );

        writer.add_document(doc)?;
        writer.commit()?;

        Ok(())
    }

    /// æœç´¢æ–‡æ¡£
    pub fn search(&self, query: &str, limit: usize) -> Result<Vec<BM25Result>> {
        let content_field = self.schema.get_field("content").unwrap();
        let id_field = self.schema.get_field("id").unwrap();
        let session_id_field = self.schema.get_field("session_id").unwrap();
        let timestamp_field = self.schema.get_field("timestamp").unwrap();

        let query_parser = QueryParser::for_index(&self.index, vec![content_field]);
        let query = query_parser.parse_query(query)?;

        let searcher = self.reader.searcher();
        let top_docs = searcher.search(&query, &TopDocs::with_limit(limit))?;

        let mut results = Vec::new();
        for (score, doc_address) in top_docs {
            let doc = searcher.doc(doc_address)?;

            results.push(BM25Result {
                id: doc.get_first(id_field).unwrap().to_string(),
                content: doc.get_first(content_field).unwrap().to_string(),
                session_id: doc.get_first(session_id_field).unwrap().to_string(),
                score: score as f32,
                timestamp: doc.get_first(timestamp_field).unwrap().into_u64().unwrap() as i64,
            });
        }

        Ok(results)
    }

    /// åˆ é™¤æ–‡æ¡£
    pub fn delete_document(&self, id: &str) -> Result<()> {
        let mut writer = self.index.writer(50_000_000)?;
        let id_field = self.schema.get_field("id").unwrap();
        let term = Term::from_field_text(id_field, id);
        writer.delete_term(term)?;
        writer.commit()?;
        Ok(())
    }

    /// æ›´æ–°æ–‡æ¡£
    pub fn update_document(
        &self,
        id: &str,
        content: &str,
        session_id: &str,
        timestamp: i64,
    ) -> Result<()> {
        self.delete_document(id)?;
        self.add_document(id, content, session_id, timestamp)?;
        Ok(())
    }
}

/// å°† BM25 æ’åè½¬æ¢ä¸ºåˆ†æ•° (å€’æ•°æ’å)
pub fn bm25_rank_to_score(rank: usize) -> f32 {
    let normalized = if rank < 999 { rank } else { 999 };
    1.0 / (1.0 + normalized as f32)
}
```

**é›†æˆåˆ°è®°å¿†ç³»ç»Ÿ**ï¼š

```rust
// crates/memory/src/mid_term.rs æ–°å¢æ–¹æ³•
impl MidTermMemory {
    pub async fn search_bm25(&self, query: &str, limit: usize) -> Result<Vec<MemoryEntry>> {
        if let Some(ref bm25) = self.bm25_index {
            let results = bm25.search(query, limit)?;
            // è½¬æ¢ä¸º MemoryEntry
            Ok(results.into_iter().map(|r| MemoryEntry {
                id: r.id,
                content: r.content,
                timestamp: r.timestamp,
                // ...
            }).collect())
        } else {
            Ok(Vec::new())
        }
    }
}
```

**API ç«¯ç‚¹**ï¼š

```rust
// crates/api/src/handlers/memory.rs æ–°å¢
/// GET /api/memory/search?query=xxx&method=bm25
pub async fn search_memory(
    Query(params): Query<SearchParams>,
) -> Result<Json<Vec<MemoryEntry>>, AppError> {
    let results = match params.method.as_deref() {
        Some("bm25") => memory.search_bm25(&params.query, params.limit).await?,
        Some("vector") | None => memory.search(&params.query, params.limit).await?,
        Some(_) => return Err(AppError::BadRequest("Invalid search method".to_string())),
    };
    Ok(Json(results))
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/memory/src/bm25.rs` | æ–°æ–‡ä»¶ |
| `crates/memory/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/memory/src/mid_term.rs` | é›†æˆ BM25 æœç´¢ |
| `crates/api/src/handlers/memory.rs` | æ–°å¢æœç´¢ API |
| `Cargo.toml` | æ·»åŠ  `tantivy = "0.22"` ä¾èµ– |

**å·¥ä½œé‡**: 3 å¤©

---

#### P2.3 æ··åˆæœç´¢ï¼ˆå‘é‡ + BM25ï¼‰

**ç›®æ ‡**ï¼šèåˆè¯­ä¹‰å’Œå…³é”®è¯æœç´¢

**æ•ˆæœé¢„æœŸ**ï¼š
- ç»“åˆè¯­ä¹‰å’Œå…³é”®è¯æœç´¢çš„ä¼˜åŠ¿
- æœç´¢ç²¾åº¦æå‡ 50%+
- æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/memory/src/hybrid.rs æ–°å¢æ–‡ä»¶
use std::collections::HashMap;
use super::bm25::BM25Result;
use super::embeddings::EmbeddingModel;

/// æ··åˆæœç´¢ç»“æœ
#[derive(Debug, Clone)]
pub struct HybridResult {
    pub id: String,
    pub vector_score: f32,
    pub bm25_score: f32,
    pub combined: f32,
}

/// æ··åˆæœç´¢å™¨
pub struct HybridSearcher {
    vector_weight: f32,
    bm25_weight: f32,
}

impl HybridSearcher {
    pub fn new(vector_weight: f32, bm25_weight: f32) -> Self {
        Self {
            vector_weight: vector_weight / (vector_weight + bm25_weight),
            bm25_weight: bm25_weight / (vector_weight + bm25_weight),
        }
    }

    /// èåˆå‘é‡æœç´¢å’Œ BM25 æœç´¢ç»“æœ
    pub fn merge_results(
        &self,
        vector_results: Vec<(String, f32)>,  // (id, score)
        bm25_results: Vec<BM25Result>,
        limit: usize,
    ) -> Vec<HybridResult> {
        let mut fused: HashMap<String, HybridResult> = HashMap::new();

        // å¤„ç†å‘é‡æœç´¢ç»“æœ (å€’æ•°æ’åè½¬åˆ†æ•°)
        for (i, (id, raw_score)) in vector_results.into_iter().enumerate() {
            let normalized_score = if raw_score <= 1.0 {
                raw_score
            } else {
                1.0 / (1.0 + i as f32)
            };

            fused.entry(id.clone()).or_insert_with(|| HybridResult {
                id: id.clone(),
                vector_score: 0.0,
                bm25_score: 0.0,
                combined: 0.0,
            }).vector_score = normalized_score;
        }

        // å¤„ç† BM25 æœç´¢ç»“æœ
        for (i, result) in bm25_results.into_iter().enumerate() {
            let normalized_score = if result.score <= 1.0 {
                result.score
            } else {
                bm25_rank_to_score(i)
            };

            fused.entry(result.id.clone()).or_insert_with(|| HybridResult {
                id: result.id.clone(),
                vector_score: 0.0,
                bm25_score: 0.0,
                combined: 0.0,
            }).bm25_score = normalized_score;
        }

        // è®¡ç®—èåˆåˆ†æ•°
        for result in fused.values_mut() {
            result.combined =
                self.vector_weight * result.vector_score +
                self.bm25_weight * result.bm25_score;
        }

        // æ’åºå¹¶è¿”å›
        let mut results: Vec<_> = fused.into_values().collect();
        results.sort_by(|a, b| b.combined.partial_cmp(&a.combined).unwrap());
        results.truncate(limit);

        results
    }
}

/// å°† BM25 æ’åè½¬æ¢ä¸ºåˆ†æ•°
fn bm25_rank_to_score(rank: usize) -> f32 {
    let normalized = if rank < 999 { rank } else { 999 };
    1.0 / (1.0 + normalized as f32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_merge_results() {
        let searcher = HybridSearcher::new(0.7, 0.3);

        let vector_results = vec![
            ("doc1".to_string(), 0.9),
            ("doc2".to_string(), 0.8),
            ("doc3".to_string(), 0.7),
        ];

        let bm25_results = vec![
            BM25Result {
                id: "doc2".to_string(),
                content: "test".to_string(),
                session_id: "s1".to_string(),
                score: 0.9,
                timestamp: 0,
            },
            BM25Result {
                id: "doc4".to_string(),
                content: "test2".to_string(),
                session_id: "s1".to_string(),
                score: 0.8,
                timestamp: 0,
            },
        ];

        let merged = searcher.merge_results(vector_results, bm25_results, 10);

        // doc2 åº”è¯¥æ’ç¬¬ä¸€ (åŒæ—¶å‡ºç°åœ¨ä¸¤ä¸ªæœç´¢ä¸­)
        assert_eq!(merged[0].id, "doc2");
        // doc2 çš„ combined score åº”è¯¥æ˜¯èåˆåçš„å€¼
        assert!(merged[0].combined > merged[0].vector_score);
        assert!(merged[0].combined > merged[0].bm25_score);
    }
}
```

**API ç«¯ç‚¹**ï¼š

```rust
// crates/api/src/handlers/memory.rs æ›´æ–°
/// GET /api/memory/search?query=xxx&method=hybrid&vector_weight=0.7&bm25_weight=0.3
pub async fn search_memory_hybrid(
    Query(params): Query<HybridSearchParams>,
    State(memory): State<Arc<MidTermMemory>>,
) -> Result<Json<Vec<MemoryEntry>>, AppError> {
    let vector_weight = params.vector_weight.unwrap_or(0.7);
    let bm25_weight = params.bm25_weight.unwrap_or(0.3);

    let results = memory
        .search_hybrid(&params.query, params.limit, vector_weight, bm25_weight)
        .await?;

    Ok(Json(results))
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/memory/src/hybrid.rs` | æ–°æ–‡ä»¶ |
| `crates/memory/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/memory/src/mid_term.rs` | é›†æˆæ··åˆæœç´¢ |
| `crates/api/src/handlers/memory.rs` | æ–°å¢æ··åˆæœç´¢ API |

**å·¥ä½œé‡**: 2-3 å¤©

---

#### P2.4 åµŒå…¥ç¼“å­˜

**ç›®æ ‡**ï¼šé¿å…é‡å¤è®¡ç®—åµŒå…¥

**æ•ˆæœé¢„æœŸ**ï¼š
- å‡å°‘ API è°ƒç”¨
- æå‡å“åº”é€Ÿåº¦
- é™ä½æˆæœ¬ï¼ˆä½¿ç”¨ OpenAI æ—¶ï¼‰

**åç«¯æ”¹åŠ¨**ï¼š

```rust
// crates/memory/src/cache.rs æ–°å¢æ–‡ä»¶
use lru::LruCache;
use std::hash::{Hash, Hasher};
use std::collections::hash_map::DefaultHasher;
use std::sync::{Arc, Mutex};
use std::num::NonZeroUsize;

/// åµŒå…¥ç¼“å­˜
pub struct EmbeddingCache {
    cache: Arc<Mutex<LruCache<u64, Vec<f32>>>>,
    max_size: usize,
}

impl EmbeddingCache {
    /// åˆ›å»ºæ–°çš„åµŒå…¥ç¼“å­˜
    pub fn new(max_size: usize) -> Self {
        let capacity = NonZeroUsize::new(max_size).unwrap();
        Self {
            cache: Arc::new(Mutex::new(LruCache::new(capacity))),
            max_size,
        }
    }

    /// è®¡ç®—æ–‡æœ¬çš„å“ˆå¸Œå€¼
    fn hash_text(text: &str) -> u64 {
        let mut hasher = DefaultHasher::new();
        text.hash(&mut hasher);
        hasher.finish()
    }

    /// è·å–ç¼“å­˜çš„åµŒå…¥
    pub fn get(&self, text: &str) -> Option<Vec<f32>> {
        let key = Self::hash_text(text);
        let mut cache = self.cache.lock().unwrap();
        cache.get(&key).cloned()
    }

    /// å­˜å‚¨åµŒå…¥åˆ°ç¼“å­˜
    pub fn put(&self, text: &str, embedding: Vec<f32>) {
        let key = Self::hash_text(text);
        let mut cache = self.cache.lock().unwrap();
        cache.put(key, embedding);
    }

    /// æ‰¹é‡è·å–
    pub fn get_batch(&self, texts: &[String]) -> Vec<Option<Vec<f32>>> {
        texts.iter()
            .map(|text| self.get(text))
            .collect()
    }

    /// æ‰¹é‡å­˜å‚¨
    pub fn put_batch(&self, texts: &[String], embeddings: &[Vec<f32>]) {
        for (text, embedding) in texts.iter().zip(embeddings.iter()) {
            self.put(text, embedding.clone());
        }
    }

    /// æ¸…ç©ºç¼“å­˜
    pub fn clear(&self) {
        let mut cache = self.cache.lock().unwrap();
        cache.clear();
    }

    /// è·å–ç¼“å­˜å¤§å°
    pub fn len(&self) -> usize {
        let cache = self.cache.lock().unwrap();
        cache.len()
    }

    /// è·å–ç¼“å­˜å®¹é‡
    pub fn capacity(&self) -> usize {
        self.max_size
    }
}

/// å¸¦ç¼“å­˜çš„åµŒå…¥æ¨¡å‹
pub struct CachedEmbeddingModel {
    inner: Box<dyn super::embeddings::EmbeddingModel>,
    cache: EmbeddingCache,
}

#[async_trait::async_trait]
impl super::embeddings::EmbeddingModel for CachedEmbeddingModel {
    async fn embed(&self, text: &str) -> Result<Vec<f32>, super::embeddings::EmbeddingError> {
        // å°è¯•ä»ç¼“å­˜è·å–
        if let Some(cached) = self.cache.get(text) {
            return Ok(cached);
        }

        // è®¡ç®—åµŒå…¥
        let embedding = self.inner.embed(text).await?;

        // å­˜å…¥ç¼“å­˜
        self.cache.put(text, embedding.clone());

        Ok(embedding)
    }

    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, super::embeddings::EmbeddingError> {
        let mut results = Vec::with_capacity(texts.len());
        let mut uncached_indices = Vec::new();
        let mut uncached_texts = Vec::new();

        // æ£€æŸ¥ç¼“å­˜
        for (i, text) in texts.iter().enumerate() {
            if let Some(cached) = self.cache.get(text) {
                results.push(Some(cached));
            } else {
                results.push(None);
                uncached_indices.push(i);
                uncached_texts.push(text.clone());
            }
        }

        // æ‰¹é‡è®¡ç®—æœªç¼“å­˜çš„
        if !uncached_texts.is_empty() {
            let uncached_embeddings = self.inner.embed_batch(&uncached_texts).await?;

            for (idx, embedding) in uncached_indices.into_iter().zip(uncached_embeddings.into_iter()) {
                results[idx] = Some(embedding.clone());
                self.cache.put(&texts[idx], embedding);
            }
        }

        Ok(results.into_iter().map(|r| r.unwrap()).collect())
    }

    fn dimension(&self) -> usize {
        self.inner.dimension()
    }
}
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```rust
// crates/memory/src/mid_term.rs
use super::cache::{EmbeddingCache, CachedEmbeddingModel};

impl MidTermMemory {
    pub fn new(config: &MemoryConfig) -> Result<Self> {
        let embedding = create_embedding_model(config.embedding_config)?;
        let cache = EmbeddingCache::new(config.cache_size);
        let cached_embedding = CachedEmbeddingModel::new(embedding, cache);

        Ok(Self {
            embedding: cached_embedding,
            // ...
        })
    }
}
```

**å½±å“æ–‡ä»¶æ¸…å•**ï¼š
| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|----------|
| `crates/memory/src/cache.rs` | æ–°æ–‡ä»¶ |
| `crates/memory/src/lib.rs` | å¯¼å‡ºæ–°æ¨¡å— |
| `crates/memory/src/mid_term.rs` | ä½¿ç”¨ç¼“å­˜åµŒå…¥ |
| `Cargo.toml` | æ·»åŠ  `lru = "0.12"` ä¾èµ– |
| `config.toml` | æ–°å¢ memory.cache_size é…ç½® |

**å·¥ä½œé‡**: 1 å¤©

---

### ğŸ”µ P3 - å¯é€‰å¢å¼ºï¼ˆä½ä¼˜å…ˆçº§ï¼‰

ä»¥ä¸‹åŠŸèƒ½æ¥è‡ª Moltbot çš„å­¦ä¹ ï¼Œä½†ä¼˜å…ˆçº§è¾ƒä½ï¼Œå¯åœ¨ P0-P2 å®Œæˆåè€ƒè™‘ï¼š

#### P3.1 åè®®ç‰ˆæœ¬æ§åˆ¶
- WebSocket åè®®ç‰ˆæœ¬åå•†
- å‘åå…¼å®¹æ€§æ”¯æŒ

#### P3.2 åºåˆ—å·æœºåˆ¶
- æ¶ˆæ¯é¡ºåºä¿è¯
- é—´éš™æ£€æµ‹

#### P3.3 å¤šç²’åº¦è·¯ç”±
- peer/guild/team/account/channel çº§åˆ«è·¯ç”±

#### P3.4 Tool æ‰§è¡Œé’©å­
- beforeToolExecute, afterToolExecute, onToolError

#### P3.5 å¿ƒè·³å¯è§æ€§æ§åˆ¶
- é…ç½®å¿ƒè·³äº‹ä»¶å‘é€é¢‘ç‡

---

## ä¸‰ã€å®æ–½æ—¶é—´è¡¨

| å‘¨æ¬¡ | ä»»åŠ¡ | å·¥æœŸ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **ç¬¬1å‘¨** | P0.1 é…ç½®åŒ–æ€è€ƒé™åˆ¶å’Œè¶…æ—¶ | 2-3å¤© | ğŸ”´ P0 |
| **ç¬¬1-2å‘¨** | P0.2 åˆ†é˜¶æ®µè¶…æ—¶è­¦å‘Šæœºåˆ¶ | 1-2å¤© | ğŸ”´ P0 |
| **ç¬¬2-3å‘¨** | P0.3 ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–ä¸æ¢å¤ | 3-4å¤© | ğŸ”´ P0 |
| **ç¬¬4å‘¨** | P1.2 Token è®¡æ•°å™¨ | 2å¤© | ğŸŸ  P1 |
| **ç¬¬5-6å‘¨** | P1.1 ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥ | 4-5å¤© | ğŸŸ  P1 |
| **ç¬¬7å‘¨** | P2.1 çœŸå®åµŒå…¥æ¨¡å‹æ”¯æŒ | 3-4å¤© | ğŸŸ¡ P2 |
| **ç¬¬8å‘¨** | P2.2 BM25 å…¨æ–‡æœç´¢ | 3å¤© | ğŸŸ¡ P2 |
| **ç¬¬9å‘¨** | P2.3 æ··åˆæœç´¢ | 2-3å¤© | ğŸŸ¡ P2 |
| **ç¬¬9å‘¨** | P2.4 åµŒå…¥ç¼“å­˜ | 1å¤© | ğŸŸ¡ P2 |

**æ€»è®¡ï¼šçº¦ 8-9 å‘¨**

---

## å››ã€å…³é”®æŒ‡æ ‡

### ä¿®å¤å‰

| æŒ‡æ ‡ | å½“å‰å€¼ |
|------|--------|
| å¹³å‡è¶…æ—¶ç‡ | ~30% (å¤æ‚æŸ¥è¯¢) |
| ä¸­æ–­åæ¢å¤ç‡ | 0% |
| æ€è€ƒå†…å®¹å®Œæ•´ä¿ç•™ | å¦ (>10K å­—ç¬¦ä¸¢å¤±) |
| ä¸Šä¸‹æ–‡ç®¡ç† | æ—  |
| è¯­ä¹‰æœç´¢è´¨é‡ | ä½ (å‡åµŒå…¥) |
| å…³é”®è¯æœç´¢ | æ—  |

### ä¿®å¤åç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| å¹³å‡è¶…æ—¶ç‡ | <5% |
| ä¸­æ–­åæ¢å¤ç‡ | >80% |
| æ€è€ƒå†…å®¹å®Œæ•´ä¿ç•™ | æ˜¯ (æœ€å¤š 50K å­—ç¬¦) |
| ä¸Šä¸‹æ–‡è‡ªåŠ¨å‹ç¼© | æ˜¯ |
| è¯­ä¹‰æœç´¢ç²¾åº¦ | æå‡ 50%+ |
| æ··åˆæœç´¢ | æ”¯æŒ |

---

## äº”ã€é£é™©è¯„ä¼°

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| ä¾èµ–åº“å…¼å®¹æ€§ | ä¸­ | å……åˆ†æµ‹è¯•ï¼Œä½¿ç”¨æˆç†Ÿç‰ˆæœ¬ |
| é…ç½®è¿ç§» | ä½ | æä¾›é»˜è®¤å€¼ï¼Œå‘åå…¼å®¹ |
| æ€§èƒ½å½±å“ | ä½ | ç¼“å­˜ã€æ‰¹å¤„ç†ä¼˜åŒ– |
| å‰ç«¯å¤æ‚åº¦ | ä½ | ç»„ä»¶åŒ–ï¼Œå¤ç”¨ç°æœ‰ UI |

---

## å…­ã€ä¾èµ–æ¸…å•

### æ–°å¢ Rust ä¾èµ–

```toml
[dependencies]
# Token è®¡æ•°
tiktoken-rs = "0.5"

# å…¨æ–‡æœç´¢
tantivy = "0.22"

# LRU ç¼“å­˜
lru = "0.12"

# é”™è¯¯å¤„ç†
thiserror = "1.0"
async-trait = "0.1"
```

### æ–°å¢å‰ç«¯ä¾èµ–

```bash
npm install
# æ— éœ€æ–°å¢ï¼Œä½¿ç”¨ç°æœ‰ç»„ä»¶
```

---

## ä¸ƒã€å˜æ›´æ—¥å¿—

| æ—¥æœŸ | ç‰ˆæœ¬ | å˜æ›´å†…å®¹ |
|------|------|----------|
| 2026-01-30 | v0.1 | åˆå§‹è®¡åˆ’æ–‡æ¡£ |
